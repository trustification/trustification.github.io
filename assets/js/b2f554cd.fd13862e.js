"use strict";(self.webpackChunktrustification_io=self.webpackChunktrustification_io||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/2023/03/20/cyclonedx-maven-aggregate-bom-why-not-to-trust","metadata":{"permalink":"/blog/2023/03/20/cyclonedx-maven-aggregate-bom-why-not-to-trust","editUrl":"https://github.com/trustification/trustification.github.io/tree/main/blog/2023-03-20-cyclonedx-maven-aggregate-bom-why-not-to-trust.md","source":"@site/blog/2023-03-20-cyclonedx-maven-aggregate-bom-why-not-to-trust.md","title":"The CycloneDX Maven Aggregate SBOM and why you shouldn\'t trust it (yet)","description":"Over the last few months I\'ve spent a lot of time with the CycloneDX Maven Plugin, trying to prove it is suitable for us to use as part of securing the Software Supply Chain. I\'ve discovered and fixed a number of issues, related to the generation of an SBOM for each project using the makeBom goal, and have now turned my focus to aggregates and the makeAggregateBom goal.","date":"2023-03-20T00:00:00.000Z","formattedDate":"March 20, 2023","tags":[{"label":"cyclonedx","permalink":"/blog/tags/cyclonedx"}],"readingTime":12.945,"hasTruncateMarker":true,"authors":[{"name":"Kevin Conner","title":"Maintainer","url":"https://github.com/kevinconner","imageURL":"https://github.com/kevinconner.png","key":"kevinconner"}],"frontMatter":{"title":"The CycloneDX Maven Aggregate SBOM and why you shouldn\'t trust it (yet)","authors":"kevinconner","tags":["cyclonedx"]},"nextItem":{"title":"in-toto attestations","permalink":"/blog/2023/03/13/in-toto-attestations"}},"content":"Over the last few months I\'ve spent a lot of time with the [CycloneDX Maven Plugin](https://github.com/CycloneDX/cyclonedx-maven-plugin \\"The CycloneDX Maven Plugin GitHub repository\\"), trying to prove it is suitable for us to use as part of securing the Software Supply Chain. I\'ve discovered and fixed a number of issues, related to the generation of an SBOM for each project using the `makeBom` goal, and have now turned my focus to aggregates and the `makeAggregateBom` goal.\\n\\n\x3c!--truncate--\x3e\\n\\nI\'ll start this post with a description of what I believe an aggregate SBOM should contain. This may be different to what you are looking for, if so I would like to hear about your expectations and tailor my fix to address your needs. In any case I believe the issues I am about to describe will likely impact yourselves and create enough doubt in the accuracy of the aggregate SBOM for you to decide not to trust it.\\n\\n# What do I expect from an Aggregate SBOM?\\n\\nGenerating an SBOM for an individual project is fairly straight forward, you run the `makeBom` goal and create an SBOM representing the `build time` dependency graph for your component. You can choose to filter certain artifacts and/or scopes from the SBOM, however the dependency resolution will still be impacted by all the dependencies within the project.\\n\\nWhen generating SBOMs for a multi-module project you have two choices\\n\\n- generate an SBOM for each individual project\\n- generate an aggregate SBOM which represents all projects within your multi-module project\\n\\nMy expectation is the aggregate SBOM would be an aggregate of all the individual SBOMs, in other words each component present in the individual SBOMs should be present in the aggregate SBOM and each dependency tree represented in the individual SBOMs should also be present in the aggregate SBOM.\\n\\n# What do we get from the current aggregate SBOM?\\n\\nBefore we describe what happens it is important to understand the CyloneDX specification requires a component to specify a `bom-ref` attribute if the component is to be referenced elsewhere in the SBOM, with the `bom-ref` being unique across the set of components. The CycloneDX maven plugin will always generate a `bom-ref` for each component, currently the same as the component purl, and will use this reference when creating the dependency hierarchy.\\n\\nNow to the details.\\n\\nWhen generating an aggregate SBOM the plugin will iterate over all the projects within the multi-module build (the reactor), generate an SBOM for each project and combining this with the previous SBOMs. It will continue until each project within the reactor has been processed, resulting in an aggregate SBOM. On the surface this appears to be the right thing to do, all components and all dependencies should be included in the SBOM and this is what we want ..... only the details are more subtle.\\n\\nWhat actually happens when we combine the components and dependencies from a project is the plugin will iterate over each component, adding it to the set of known components if it has not previously encountered its `bom-ref`, and in the case of dependencies it will add all project dependencies to the set of known dependencies within the aggregate. Now I know what you are thinking! This still appears to be correct, so where is it going wrong? To understand that we need to look at the `Dependency` class.\\n\\nThe `Dependency` class defines equality only on the content of its ref attribute, so when adding dependencies to a `Set` the following are considered to be the same.\\n\\n```\\n<dependency ref=\\"pkg:maven/com.example.dependency_trees/dependency_B@1.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.dependency_trees/dependency_C@1.0.0?type=jar\\"/>\\n</dependency>\\n```\\n\\nand\\n\\n```\\n<dependency ref=\\"pkg:maven/com.example.dependency_trees/dependency_B@1.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.dependency_trees/dependency_C@2.0.0?type=jar\\"/>\\n</dependency>\\n```\\n\\nWhat this means is the first dependency added to the set of all dependencies will be the winner, will be part of the generated SBOM, and all subsequent dependency hierarchies will be lost. But wait, shouldn\'t a component always have the same dependencies on other components? This is certainly the assumption made in the plugin, unfortunately it is not a valid assumption for many reasons and for this we need to look more at how maven handles dependency resolution. Scenarios where this assumption is invalid include\\n\\n- the component is included in a project with a different set of dependencies\\n- the component is included in a project which specifies dependencies in a different order\\n- the component includes dependencies on artifacts with `test` or `provided` scope, or includes dependencies on artifacts marked as `optional`\\n- there is a dependency management section overriding the version of an artifact\\n- there is a dependency management section excluding dependencies from an artifact\\n\\nThere are likely more scenarios, however the above are reasonably common and sufficient to demonstrate the problem. In the next sections we will go through examples for each of the top three, ignoring version management, so we can demonstrate how easy it is to fall into this trap and explore the impact on the generated aggregate SBOM.\\n\\n# Differing Sets of Dependencies\\n\\nIn this scenario we will take a look at a multi-module project where an external project is consumed within the context of differing sets of dependencies. Project `dependency_A` will consume the artifact we are interested in, `dependency_C`, alongside a second dependency, `dependency_E`, which will cause a version conflict which needs to be resolved by the maven conflict resolution mechanism. Project `dependency_B` will simply consume `dependency_C` as is, without there being any conflict needing to be resolved.\\n\\nLet us now take a look at the dependency hierarchy for both projects, as visualized through the dependency:tree plugin. For `dependency_A` we see\\n\\n```\\ncom.example.example1:dependency_A:jar:1.0.0\\n+- com.example.external:dependency_C:jar:1.0.0:compile\\n|  \\\\- com.example.external:dependency_D:jar:1.0.0:compile\\n|     \\\\- (com.example.external:dependency_E:jar:1.0.0:compile - omitted for conflict with 2.0.0)\\n\\\\- com.example.external:dependency_E:jar:2.0.0:compile (scope not updated to compile)\\n   \\\\- com.example.external:dependency_F:jar:2.0.0:compile\\n```\\n\\nand for `dependency_B` we see\\n\\n```\\ncom.example.example1:dependency_B:jar:1.0.0\\n\\\\- com.example.external:dependency_C:jar:1.0.0:compile\\n   \\\\- com.example.external:dependency_D:jar:1.0.0:compile\\n      \\\\- com.example.external:dependency_E:jar:1.0.0:compile\\n         \\\\- com.example.external:dependency_F:jar:1.0.0:compile\\n```\\n\\nIn the first project we see the maven dependency resolution mechanism has aligned the version of `dependency_E` referenced in the tree to version `2.0.0`, this was chosen because this dependency is closer to the root of the tree.\\n\\nLet\'s now take a look at what we see in the aggregate SBOM.\\n\\n```\\n<dependency ref=\\"pkg:maven/com.example.example1/dependency_A@1.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_C@1.0.0?type=jar\\"/>\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_E@2.0.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.example.external/dependency_C@1.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_D@1.0.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.example.external/dependency_D@1.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_E@2.0.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.example.external/dependency_E@2.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_F@2.0.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.example.external/dependency_F@2.0.0?type=jar\\"/>\\n<dependency ref=\\"pkg:maven/com.example.example1/dependency_B@1.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_C@1.0.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.example.external/dependency_E@1.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_F@1.0.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.example.external/dependency_F@1.0.0?type=jar\\"/>\\n```\\n\\nFrom the above we see there is a flow matching `A1->C1->D1->E2->F2` and `A1->E2->F2`, so the aggregated SBOM correctly represents the dependency hierarchy for `dependency_A`.\\n\\nIf we now look at how the dependency tree for `dependency_B` is represented we see a problem. The representation for `dependency_B` is a flow from `B1->C1->D1->E2-F2` which does not match the dependency hierarchy for `dependency_B`; it should be `B1->C1->D1->E1->F1`! Where has the flow `E1->F1` gone? Unfortunately this flow is now orphaned with `E1` being its own root in the SBOM!\\n\\n# Differing Order of Dependencies\\n\\nIn this scenario we will take a look at a multi-module project where we have two projects consuming the same sets of dependencies, but specifying those dependencies in opposite order within their respective `pom.xml` files. The first will declare a dependency on `dependency_C` followed by `deependency_D` whereas the second will declare a dependency on `dependency_D` followed by `dependency_C`. Both `dependency_C` and `dependency_D` will consume `dependency_E` but with different versions, causing a conflict which needs to be resolved.\\n\\nLet us now take a look at the dependency hierarchy for both projects, as visualized through the dependency:tree plugin. For `dependency_A` we see\\n\\n```\\ncom.example.example2:dependency_A:jar:1.0.0\\n+- com.example.external:dependency_C:jar:1.0.0:compile\\n|  \\\\- com.example.external:dependency_E:jar:1.0.0:compile\\n|     \\\\- com.example.external:dependency_F:jar:1.0.0:compile\\n\\\\- com.example.external:dependency_D:jar:1.0.0:compile\\n   \\\\- (com.example.external:dependency_E:jar:2.0.0:compile - omitted for conflict with 1.0.0)\\n```\\n\\nand for `dependency_B` we see\\n\\n```\\ncom.example.example2:dependency_B:jar:1.0.0\\n+- com.example.external:dependency_D:jar:1.0.0:compile\\n|  \\\\- com.example.external:dependency_E:jar:2.0.0:compile\\n|     \\\\- com.example.external:dependency_F:jar:2.0.0:compile\\n\\\\- com.example.external:dependency_C:jar:1.0.0:compile\\n   \\\\- (com.example.external:dependency_E:jar:1.0.0:compile - omitted for conflict with 2.0.0)\\n```\\n\\nIn the first project we see the maven dependency resolution mechanism has aligned `dependency_E` with version `1.0.0` while in the second project the resolution mechanism has aligned `dependency_E` with version `2.0.0`. Since both versions of `dependency_E` appear at the same depth it is the one which is first included in the dependency tree which will win.\\n\\nLet\'s now take a look at what we see in the aggregate SBOM.\\n\\n```\\n<dependency ref=\\"pkg:maven/com.example.example2/dependency_A@1.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_C@1.0.0?type=jar\\"/>\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_D@1.0.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.example.external/dependency_C@1.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_E@1.0.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.example.external/dependency_E@1.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_F@1.0.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.example.external/dependency_F@1.0.0?type=jar\\"/>\\n<dependency ref=\\"pkg:maven/com.example.external/dependency_D@1.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_E@1.0.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.example.example2/dependency_B@1.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_D@1.0.0?type=jar\\"/>\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_C@1.0.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.example.external/dependency_E@2.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_F@2.0.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.example.external/dependency_F@2.0.0?type=jar\\"/>\\n```\\n\\nFrom the above we see there is a flow matching `A1->C1->E1->F1` and `A1->D1->E1->F1`, so the aggregated SBOM correctly represents the dependency hierarchy for `dependency_A`.\\n\\nIf we now look at how the dependency tree for `dependency_B` is represented we can again see a problem. The representation for `dependency_B` has flows from `B1->D1->E1-F1` and `B1->C1->E1->F1` which do not match the dependency hierarchy for `dependency_B` since it does not include `E1->F1` in its hierarchy! Where has the flow `E2->F2` gone? Unfortunately this flow is now orphaned with `E2` being its own root in the SBOM!\\n\\n# Non Transitive Dependencies\\n\\nIn this scenario we will take a look at how non-Transitive dependencies can impact the normal resolution process, and therefore the derived dependency trees. The non-Transitive dependencies could be those dependencies with scopes of either `provided` or `test`, or could be dependencies which are marked as being `optional`.\\n\\nProject `dependency_A` will declare a dependency on `dependency_C` alongside an optional dependency on `dependency_E`, which causes a conflict which needs to be resolved. Project `dependency_B` will have a single dependency on `dependency_A`, consuming it as is.\\n\\nLet us now take a look at the dependency hierarchy for both projects, as visualized through the dependency:tree plugin. For `dependency_A` we see\\n\\n```\\ncom.example.example3:dependency_A:jar:1.0.0\\n+- com.example.external:dependency_C:jar:1.0.0:compile\\n|  \\\\- com.example.external:dependency_D:jar:1.0.0:compile\\n|     \\\\- (com.example.external:dependency_E:jar:1.0.0:compile - omitted for conflict with 2.0.0)\\n\\\\- com.example.external:dependency_E:jar:2.0.0:compile (scope not updated to compile)\\n   \\\\- com.example.external:dependency_F:jar:2.0.0:compile\\n```\\n\\nand for `dependency_B` we see\\n\\n```\\ncom.example.example3:dependency_B:jar:1.0.0\\n\\\\- com.example.example3:dependency_A:jar:1.0.0:compile\\n   \\\\- com.example.external:dependency_C:jar:1.0.0:compile\\n      \\\\- com.example.external:dependency_D:jar:1.0.0:compile\\n         \\\\- com.example.external:dependency_E:jar:1.0.0:compile\\n            \\\\- com.example.external:dependency_F:jar:1.0.0:compile\\n```\\n\\nIn the first project we see the maven dependency resolution mechanism has aligned `dependency_E` with version `2.0.0` through the transitive dependency inherited via the `optional` dependency. In the second project the maven dependency resolution mechanism has ignored the optional dependency leaving the original dependency on `dependency_E:1.0.0`.\\n\\n---\\n\\n**Note:** While this example is making use of an optional dependency, the same tree would be generated if this dependency was of scope `test` or `provided`.\\n\\n---\\n\\nLet\'s now take a look at what we see in the aggregate SBOM.\\n\\n```\\n<dependency ref=\\"pkg:maven/com.example.example3/dependency_A@1.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_C@1.0.0?type=jar\\"/>\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_E@2.0.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.example.external/dependency_C@1.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_D@1.0.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.example.external/dependency_D@1.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_E@2.0.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.example.external/dependency_E@2.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_F@2.0.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.example.external/dependency_F@2.0.0?type=jar\\"/>\\n<dependency ref=\\"pkg:maven/com.example.example3/dependency_B@1.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.example3/dependency_A@1.0.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.example.external/dependency_E@1.0.0?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.example.external/dependency_F@1.0.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.example.external/dependency_F@1.0.0?type=jar\\"/>\\n```\\n\\nFrom the above we see there is a flow matching `A1->C1->D1->E2->F2` and `A1->E2->F2`, so the aggregated SBOM correctly represents the dependency hierarchy for `dependency_A`.\\n\\nIf we now look at how the dependency tree for `dependency_B` is represented we see a problem. The representation for `dependency_B` includes flows from `B1->A1->C1->D1->E2->F2` and `B1->A1->E2->F2`, neither of which match the dependency hierarchy for B; the hierarchy for `dependency_B` should be a flow of `B1->A1->C1->D1->E1->F1`! Not only has the flow `E1->F1` been orphaned from the `dependency_B` dependency tree, and now its own root, but the SBOM claims `dependency_B` has multiple dependency flows when it should only be a single flow.\\n\\n## Summarising the issue\\n\\nWe see from the above scenarios it is reasonably easy to end up with a multi-module project which results in an invalid expression of the project dependency trees within the aggregated SBOM, and we have still to look at exclusions and version management through the maven `dependencyManagement` declarations. To make matters worse we have no easy way of identifying whether the aggregated SBOM contains an invalid dependency graph. While it is possible the aggregated SBOM could contain orphaned dependency trees it is also possible those dependency trees would be consumed by other components within the SBOM.\\n\\nGiven the aggregate SBOM is no longer reliable the only safe approach is to rely on the individual SBOMs for each project, at least for now.\\n\\n## How can we solve this?\\n\\nThere is a solution for this, however the solution comes with implications which _may_ break some tools which work with SBOMs. These tools will already be making invalid assumptions, and we will have to work with their authors to identify and fix any occurrences we find.\\n\\nWhat is really lacking in the CycloneDX specification is a way in which we can easily describe alternative dependency hierarchies for a component, since as we have shown this is something which happens within the java space and likely other areas.\\n\\nThe CyloneDX specification defines the component dependency hierarchies by referring to the `components` themselves, through their references, however this reference also defines the unique hierarchy within the `dependencies` section. For example\\n\\n```\\n<dependency ref=\\"pkg:maven/org.owasp.webgoat.lesson/auth-bypass@v8.0.0.M15?type=jar\\">\\n  <dependency ref=\\"pkg:maven/org.owasp.encoder/encoder@1.2?type=jar\\"/>\\n  <dependency ref=\\"pkg:maven/com.thoughtworks.xstream/xstream@1.4.7?type=jar\\"/>\\n  <dependency ref=\\"pkg:maven/org.apache.commons/commons-exec@1.3?type=jar\\"/>\\n</dependency>\\n```\\n\\nThe `ref` attribute `pkg:maven/org.owasp.webgoat.lesson/auth-bypass@v8.0.0.M15?type=jar` represents not only the specific component, tied through the component `bom-ref` attribute, but also a unique dependency hierarchy within the `dependencies` section. Given this how can we also represent the following hierarchy within the same aggregate SBOM?\\n\\n```\\n<dependency ref=\\"pkg:maven/org.owasp.webgoat.lesson/auth-bypass@v8.0.0.M15?type=jar\\">\\n  <dependency ref=\\"pkg:maven/org.owasp.webgoat/webgoat-container@v8.0.0.M15?type=jar\\"/>\\n  <dependency ref=\\"pkg:maven/org.owasp.encoder/encoder@1.2?type=jar\\"/>\\n  <dependency ref=\\"pkg:maven/com.thoughtworks.xstream/xstream@1.4.7?type=jar\\"/>\\n  <dependency ref=\\"pkg:maven/org.projectlombok/lombok@1.16.20?type=jar\\"/>\\n  <dependency ref=\\"pkg:maven/org.apache.commons/commons-exec@1.3?type=jar\\"/>\\n</dependency>\\n```\\n\\nIn order to achieve this we need to have some way of enriching the `ref` attribute value so it represents not only the specific component but also the direct dependencies in its particular location within the dependency graph, for example including a hash which would be derived from each set of dependencies. This hash would need to be calculated from the leaves back to the root, with every deviation in the dependency tree propagating its way up to the root.\\n\\nWe could then have a representation similar to the following\\n\\n```\\n<dependency\\n    ref=\\"pkg:maven/org.owasp.webgoat.lesson/auth-bypass@v8.0.0.M15?hash=<hash1>&amp;type=jar\\">\\n  <dependency ref=\\"pkg:maven/org.owasp.encoder/encoder@1.2?type=jar\\"/>\\n  <dependency ref=\\"pkg:maven/com.thoughtworks.xstream/xstream@1.4.7?type=jar\\"/>\\n  <dependency ref=\\"pkg:maven/org.apache.commons/commons-exec@1.3?type=jar\\"/>\\n</dependency>\\n\\n<dependency\\n    ref=\\"pkg:maven/org.owasp.webgoat.lesson/auth-bypass@v8.0.0.M15?hash=<hash2>&amp;type=jar\\">\\n  <dependency ref=\\"pkg:maven/org.owasp.webgoat/webgoat-container@v8.0.0.M15?type=jar\\"/>\\n  <dependency ref=\\"pkg:maven/org.owasp.encoder/encoder@1.2?type=jar\\"/>\\n  <dependency ref=\\"pkg:maven/com.thoughtworks.xstream/xstream@1.4.7?type=jar\\"/>\\n  <dependency ref=\\"pkg:maven/org.projectlombok/lombok@1.16.20?type=jar\\"/>\\n  <dependency ref=\\"pkg:maven/org.apache.commons/commons-exec@1.3?type=jar\\"/>\\n</dependency>\\n```\\n\\n---\\n\\n**Note:** The child dependencies in the example above should also have a hash, however these have been omitted for brevity.\\n\\n---\\n\\nThe above would allow us to represent different hierarchies, but now we have another problem. Each reference used in the `dependencies` section _must_ refer to an existing `component` within the `components` section and, therefore, we now need the `component` to be defined for each reference in use, e.g.\\n\\n```\\n<component type=\\"library\\"\\n    bom-ref=\\"pkg:maven/org.owasp.webgoat.lesson/auth-bypass@v8.0.0.M15?hash=<hash1>&amp;type=jar\\">\\n  <publisher>OWASP</publisher>\\n  <group>org.owasp.webgoat.lesson</group>\\n  <name>auth-bypass</name>\\n  <version>v8.0.0.M15</version>\\n  <description>Parent Pom for the WebGoat Project. A deliberately insecure Web Application</description>\\n  <purl>pkg:maven/org.owasp.webgoat.lesson/auth-bypass@v8.0.0.M15?type=jar</purl>\\n  ...\\n</component>\\n\\n<component type=\\"library\\"\\n    bom-ref=\\"pkg:maven/org.owasp.webgoat.lesson/auth-bypass@v8.0.0.M15?hash=<hash2>&amp;type=jar\\">\\n  <publisher>OWASP</publisher>\\n  <group>org.owasp.webgoat.lesson</group>\\n  <name>auth-bypass</name>\\n  <version>v8.0.0.M15</version>\\n  <description>Parent Pom for the WebGoat Project. A deliberately insecure Web Application</description>\\n  <purl>pkg:maven/org.owasp.webgoat.lesson/auth-bypass@v8.0.0.M15?type=jar</purl>\\n  ...\\n</component>\\n```\\n\\nIt is this change which, while not invalid according to the specification schema, will likely cause problems for tools which have made the assumption the component can only exist once.\\n\\n# Conclusions\\n\\nThere is a [CycloneDX maven plugin PR](https://github.com/CycloneDX/cyclonedx-maven-plugin/pull/306) open to modify the plugin\'s behaviour and generate an aggregate SBOM using the above hashing scheme, representing all the dependency hierarchies within a multi-module project. There is also an associated [CycloneDX maven plugin issue](https://github.com/CycloneDX/cyclonedx-maven-plugin/issues/310) where the reasons for this change are still being discussed, as well as a number of discussions on how the specification can evolve to address this issue in a cleaner way.\\n\\nI am hopeful we can come up with a suitable resolution to these discussions and deliver a solution to the issue, however until this is achieved the `dependencies` section of the aggregate SBOMs generated by the CycloneDX maven plugin is unreliable and should not be trusted. The only reliable source of dependency hierarchy information is to be found within the individual SBOMs of each project."},{"id":"/2023/03/13/in-toto-attestations","metadata":{"permalink":"/blog/2023/03/13/in-toto-attestations","editUrl":"https://github.com/trustification/trustification.github.io/tree/main/blog/2023-03-13-in-toto-attestations.md","source":"@site/blog/2023-03-13-in-toto-attestations.md","title":"in-toto attestations","description":"When we sign an artifact, like a blob, the signature proves that we were in","date":"2023-03-13T00:00:00.000Z","formattedDate":"March 13, 2023","tags":[],"readingTime":7.425,"hasTruncateMarker":true,"authors":[{"name":"Daniel Bevenius","title":"Maintainer","url":"https://github.com/danbev","imageURL":"https://github.com/danbev.png","key":"danbev"}],"frontMatter":{"title":"in-toto attestations","authors":"danbev","tags":[]},"prevItem":{"title":"The CycloneDX Maven Aggregate SBOM and why you shouldn\'t trust it (yet)","permalink":"/blog/2023/03/20/cyclonedx-maven-aggregate-bom-why-not-to-trust"},"nextItem":{"title":"Signing elf binaries, or not?! Lessons learned.","permalink":"/blog/2023/02/13/elfsign"}},"content":"When we sign an artifact, like a blob, the signature proves that we were in\\npossesion of the private key. When we verify, we use the signature, the public\\nkey, and the blob, and we are verifying that this was in fact the case. But it\\ndoes not say anything else about the artifact, we don\'t know `what` was actually\\nsigned.\\n\\n\x3c!--truncate--\x3e\\n\\nBy providing, and signing a document specifying `statements` about the artifact\\nwe can say things about the artifact as well. Statements could be anything which\\nwe will address later in this document. A signed Statement is called an\\nAttestation.\\n\\nAn attestation is authenticated metadata about software artifacts and follows\\nthe [Software-chain Levels for Software Artifacts attestation model](https://slsa.dev/attestation-model).\\n\\nAn attestation can be json object, and the outer-most element is the `Envelope`:\\n\\n```json\\n{\\n  \\"payloadType\\": \\"application/vnd.in-toto+json\\",\\n  \\"payload\\": \\"<Base64(Statement)>\\",\\n  \\"signatures\\": [{ \\"sig\\": \\"<Base64(Signature)>\\" }]\\n}\\n```\\n\\nNotice that the `payload` is a base64 encoded `Statement`. This format follows\\nthe [DSSE] format.\\n\\nThe `payloadType` could be JSON, CBOR, or ProtoBuf.\\n\\nThe structure of the `Statement`, in payload element above, looks something\\nlike this (before it is base64 encoded):\\n\\n```json\\n{\\n  \\"_type\\": \\"https://in-toto.io/Statement/v0.1\\",\\n  \\"subject\\": [\\n    {\\n      \\"name\\": \\"<NAME>\\",\\n      \\"digest\\": { \\"<ALGORITHM>\\": \\"<HEX_VALUE>\\" }\\n    }\\n  ],\\n  \\"predicateType\\": \\"<URI>\\",\\n  \\"predicate\\": {}\\n}\\n```\\n\\nThe subjects bind this attestation to a set of software artifacts, notice that\\nthis is an array of objects.\\n\\nEach software artifact is given a name and a digest. The digest contains the\\nname of the hashing algorithm used, and the digest (the outcome of the hash\\nfunction).\\nThe name could be a file name but it can also be left unspecified using `_`.\\n\\nThis leads us to the `predicate` fields, which like shown above has one field\\nfor the type of the predicate (predicateType), and an object as the content of\\nthe predicate.\\n\\nThe predicate can contain pretty much any metadata related to the Statement\\nobject\'s subjects. The `predicateType` provides a way of knowing how to\\ninterpret the `predicate` field.\\n\\nExamples of predicate types are\\n[SLSA Provenance](https://slsa.dev/provenance/v0.1#example),\\n[in-toto Link](https://github.com/in-toto/attestation/blob/main/spec/predicates/link.md)\\n, [SPDX](https://github.com/in-toto/attestation/blob/main/spec/predicates/spdx.md)\\n, [Software Supply Chain Attribute Integrity (SCAI)](https://github.com/in-toto/attestation/blob/main/spec/predicates/scai.md).\\n\\nNPM also uses this for it to publish [attestations](https://github.com/npm/rfcs/blob/main/accepted/0049-link-packages-to-source-and-build.md#slsa-provenance-schema):\\n\\n```json\\n{\\n  \\"_type\\": \\"https://in-toto.io/Statement/v0.1\\",\\n  \\"subject\\": [\\n    {\\n      \\"name\\": \\"pkg:npm/@scope/package-foo@1.4.3\\",\\n      \\"digest\\": { \\"sha512\\": \\"41o0P/CEffYGDqvo2pHQXRBOfFOxvYY3WkwkQTy...\\" }\\n    }\\n  ],\\n  \\"predicateType\\": \\"https://github.com/npm/attestation/tree/main/specs/publish/v0.1\\",\\n  \\"predicate\\": {\\n    \\"name\\": \\"@scope/package-foo\\",\\n    \\"version\\": \\"1.4.3\\",\\n    \\"registry\\": \\"https://registry.npmjs.org\\"\\n  }\\n}\\n```\\n\\nThe `digest` in this case is the sha512sum of the published tar file.\\n\\nSo, we mentioned that the predicate type is used by the consumer of the\\npredicate so it knows how to interpret the contents of the predicate. But who\\nis the consumer?  \\nMost often this would be a Policy Engine. The Policy engine would be passed\\nthe contents of the Statement related to the predicate, and rules written in\\nthe Policy Engine\'s language would process the predicate as input. The outcome\\nwould be a true/false result (remember that a predicate is a statement/function\\nthat returns true or false).\\n\\nTo try to make this a little more concrete lets take a look at an example\\nthat creates an attestation.\\n\\nFor this example I\'m going to use a GitHub Action named\\n[slsa-github-generator] which can generate SLSA provenance attestations for\\ngithub native projects. This generator can generate SLSA provenance for SLSA\\nlevel 3.\\n\\nThe example project I\'m going to use is [tuf-keyid](https://github.com/danbev/tuf-keyid)\\nbut the actual project is not important in this case, any Rust project could\\nhave been used.\\n\\nSo we need to set up a GitHub Action which can been seen in\\n[provenance.yaml](https://github.com/danbev/tuf-keyid/blob/main/.github/workflows/provenance.yaml).\\n\\nAfter that workflow has run it will produce an attestation and a binary which\\nwe will use to verify.\\n\\nFirst, we need to download the binary from the [workflow run] (we should really\\nbe able to be download this from the releases page too, but I\'ve not been able\\nto get that to work just yet):\\n\\n```console\\n$ unzip tuf-keyid.zip\\n$ file tuf-keyid\\ntuf-keyid: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=ceaa62d49b024798ebd7fe7d021f3ade5925b1f9, for GNU/Linux 3.2.0, with debug_info, not stripped\\n```\\n\\nAnd then we need to download the attestation file:\\n\\n```console\\n$ curl -L https://github.com/danbev/tuf-keyid/releases/download/v0.2.0/tuf-keyid.intoto.jsonl --output tuf-keyid.intoto.jsonl\\n```\\n\\nLets inspect the attestation file:\\n\\n```console\\n$ cat tuf-keyid.intoto.jsonl | jq\\n{\\n  \\"payloadType\\": \\"application/vnd.in-toto+json\\",\\n  \\"payload\\": \\"...\\",\\n  \\"signatures\\": [\\n    {\\n      \\"keyid\\": \\"\\",\\n      \\"sig\\": \\"MEUCIHwmJopmrXWqi+rKIeTlWW0r027hLL1nO7xEj0mW8czsAiEAhdc6SDlhWo3m0YOtsUSoIYSlvw3Xu7ts3S8btHzdMpw=\\",\\n      \\"cert\\": \\"-----BEGIN CERTIFICATE-----\\\\nMIIDtjCCAzygAwIBAgIUCeak2sfkfZbS0IMRSbK4+BHcUzAwCgYIKoZIzj0EAwMw\\\\nNzEVMBMGA1UEChMMc2lnc3RvcmUuZGV2MR4wHAYDVQQDExVzaWdzdG9yZS1pbnRl\\\\ncm1lZGlhdGUwHhcNMjMwMTI2MTAxMzQxWhcNMjMwMTI2MTAyMzQxWjAAMFkwEwYH\\\\nKoZIzj0CAQYIKoZIzj0DAQcDQgAEZMurC3H80wzo+Xn7uifeTDV/AAFnye8uFwEj\\\\n5VmxJb30VzuEw8gD8/Dj4V79bIW9sePcZjvREhFWak+PhUZVMqOCAlswggJXMA4G\\\\nA1UdDwEB/wQEAwIHgDATBgNVHSUEDDAKBggrBgEFBQcDAzAdBgNVHQ4EFgQUncOT\\\\nSyRyKgylBYlUHwPF+EyemfkwHwYDVR0jBBgwFoAU39Ppz1YkEZb5qNjpKFWixi4Y\\\\nZD8wgYQGA1UdEQEB/wR6MHiGdmh0dHBzOi8vZ2l0aHViLmNvbS9zbHNhLWZyYW1l\\\\nd29yay9zbHNhLWdpdGh1Yi1nZW5lcmF0b3IvLmdpdGh1Yi93b3JrZmxvd3MvZ2Vu\\\\nZXJhdG9yX2dlbmVyaWNfc2xzYTMueW1sQHJlZnMvdGFncy92MS40LjAwOQYKKwYB\\\\nBAGDvzABAQQraHR0cHM6Ly90b2tlbi5hY3Rpb25zLmdpdGh1YnVzZXJjb250ZW50\\\\nLmNvbTASBgorBgEEAYO/MAECBARwdXNoMDYGCisGAQQBg78wAQMEKDUxM2IwZTA2\\\\nMGM3NmExZGVkN2IxYTQxNzMxNjUxMDM4MzhmOGRkZTcwFQYKKwYBBAGDvzABBAQH\\\\nUHVibGlzaDAeBgorBgEEAYO/MAEFBBBkYW5iZXYvdHVmLWtleWlkMB4GCisGAQQB\\\\ng78wAQYEEHJlZnMvdGFncy92MC4yLjAwgYoGCisGAQQB1nkCBAIEfAR6AHgAdgDd\\\\nPTBqxscRMmMZHhyZZzcCokpeuN48rf+HinKALynujgAAAYXtkZ+vAAAEAwBHMEUC\\\\nIQDgO+S94sXq3wcfg344IV8FRhynvsJsVFEfHmwOHGqAVgIgArfX+7pnaLrplJ0u\\\\nXB6tlWaCxQJ7GAo9YByqXCa0b2gwCgYIKoZIzj0EAwMDaAAwZQIxAOYkXbpLbSqC\\\\njdORW6lWGWB/Ts2aOhK7VAHaQCRgRHQGiZx4Pe/LCwqkQF/1W2BAEQIwLB9Ic2jt\\\\nIiEjtw8xKFDQAfnUleNUtZ51LXgXEkdpIX9cnj4UdR6k4gu/wul16Bd8\\\\n-----END CERTIFICATE-----\\\\n\\"\\n    }\\n  ]\\n}\\n```\\n\\nIf we look back at the beginning of this document we will see that this format\\nmatches the `Envelope` of the attestation, and we have the `payloadType`,\\na `payload`, and `signatures`.\\n\\nThe certificate can be inspected using:\\n\\n```console\\n$ cat tuf-keyid.intoto.jsonl | jq -r \'.signatures[].cert\' | openssl x509 --text\\n```\\n\\nRecall that the payload is a base64 encoded `Statement`. Let\'s decode the\\n`Statement` and take a closer a look at it:\\n\\n```console\\n$ cat tuf-keyid.intoto.jsonl | jq -r \'.payload\' | base64 -d | jq\\n{\\n  \\"_type\\": \\"https://in-toto.io/Statement/v0.1\\",\\n  \\"predicateType\\": \\"https://slsa.dev/provenance/v0.2\\",\\n  \\"subject\\": [\\n    {\\n      \\"name\\": \\"tuf-keyid\\",\\n      \\"digest\\": {\\n        \\"sha256\\": \\"470c549740f98fe1b1977d48e014031ed5183785fd459df7e04605daefe8e293\\"\\n      }\\n    }\\n  ],\\n  \\"predicate\\": {\\n    \\"builder\\": {\\n      \\"id\\": \\"https://github.com/slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@refs/tags/v1.4.0\\"\\n    },\\n    \\"buildType\\": \\"https://github.com/slsa-framework/slsa-github-generator/generic@v1\\",\\n    \\"invocation\\": {\\n      \\"configSource\\": {\\n        \\"uri\\": \\"git+https://github.com/danbev/tuf-keyid@refs/tags/v0.2.0\\",\\n        \\"digest\\": {\\n          \\"sha1\\": \\"513b0e060c76a1ded7b1a4173165103838f8dde7\\"\\n        },\\n        \\"entryPoint\\": \\".github/workflows/provenance.yaml\\"\\n      },\\n      \\"parameters\\": {},\\n      \\"environment\\": {\\n        \\"github_actor\\": \\"danbev\\",\\n        \\"github_actor_id\\": \\"432351\\",\\n        \\"github_base_ref\\": \\"\\",\\n        \\"github_event_name\\": \\"push\\",\\n        \\"github_event_payload\\": {\\n          \\"after\\": \\"13c69c54cbd04d1920cc5e42441f0a693a371494\\",\\n          \\"base_ref\\": null,\\n          \\"before\\": \\"0000000000000000000000000000000000000000\\",\\n          \\"commits\\": [],\\n          \\"compare\\": \\"https://github.com/danbev/tuf-keyid/compare/v0.2.0\\",\\n          \\"created\\": true,\\n          \\"deleted\\": false,\\n          \\"forced\\": false,\\n          \\"head_commit\\": {\\n            \\"author\\": {\\n              \\"email\\": \\"daniel.bevenius@gmail.com\\",\\n              \\"name\\": \\"Daniel Bevenius\\",\\n              \\"username\\": \\"danbev\\"\\n            },\\n            \\"committer\\": {\\n              \\"email\\": \\"daniel.bevenius@gmail.com\\",\\n              \\"name\\": \\"Daniel Bevenius\\",\\n              \\"username\\": \\"danbev\\"\\n            },\\n            \\"distinct\\": true,\\n            \\"id\\": \\"513b0e060c76a1ded7b1a4173165103838f8dde7\\",\\n            \\"message\\": \\"Add content(releases) write permission\\\\n\\\\nSigned-off-by: Daniel Bevenius <daniel.bevenius@gmail.com>\\",\\n            \\"timestamp\\": \\"2023-01-26T11:09:02+01:00\\",\\n            \\"tree_id\\": \\"5030177fa47fc8b8252c26e8556083b4abc5df71\\",\\n            \\"url\\": \\"https://github.com/danbev/tuf-keyid/commit/513b0e060c76a1ded7b1a4173165103838f8dde7\\"\\n          },\\n          \\"pusher\\": {\\n            \\"email\\": \\"daniel.bevenius@gmail.com\\",\\n            \\"name\\": \\"danbev\\"\\n          },\\n          \\"ref\\": \\"refs/tags/v0.2.0\\",\\n          \\"repository\\": {\\n            \\"allow_forking\\": true,\\n            \\"archive_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/{archive_format}{/ref}\\",\\n            \\"archived\\": false,\\n            \\"assignees_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/assignees{/user}\\",\\n            \\"blobs_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/git/blobs{/sha}\\",\\n            \\"branches_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/branches{/branch}\\",\\n            \\"clone_url\\": \\"https://github.com/danbev/tuf-keyid.git\\",\\n            \\"collaborators_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/collaborators{/collaborator}\\",\\n            \\"comments_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/comments{/number}\\",\\n            \\"commits_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/commits{/sha}\\",\\n            \\"compare_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/compare/{base}...{head}\\",\\n            \\"contents_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/contents/{+path}\\",\\n            \\"contributors_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/contributors\\",\\n            \\"created_at\\": 1674117641,\\n            \\"default_branch\\": \\"main\\",\\n            \\"deployments_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/deployments\\",\\n            \\"description\\": \\"A command line tool to print the key id for a TUF public key in JSON format.\\",\\n            \\"disabled\\": false,\\n            \\"downloads_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/downloads\\",\\n            \\"events_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/events\\",\\n            \\"fork\\": false,\\n            \\"forks\\": 0,\\n            \\"forks_count\\": 0,\\n            \\"forks_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/forks\\",\\n            \\"full_name\\": \\"danbev/tuf-keyid\\",\\n            \\"git_commits_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/git/commits{/sha}\\",\\n            \\"git_refs_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/git/refs{/sha}\\",\\n            \\"git_tags_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/git/tags{/sha}\\",\\n            \\"git_url\\": \\"git://github.com/danbev/tuf-keyid.git\\",\\n            \\"has_discussions\\": false,\\n            \\"has_downloads\\": true,\\n            \\"has_issues\\": true,\\n            \\"has_pages\\": false,\\n            \\"has_projects\\": true,\\n            \\"has_wiki\\": true,\\n            \\"homepage\\": null,\\n            \\"hooks_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/hooks\\",\\n            \\"html_url\\": \\"https://github.com/danbev/tuf-keyid\\",\\n            \\"id\\": 590801502,\\n            \\"is_template\\": false,\\n            \\"issue_comment_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/issues/comments{/number}\\",\\n            \\"issue_events_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/issues/events{/number}\\",\\n            \\"issues_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/issues{/number}\\",\\n            \\"keys_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/keys{/key_id}\\",\\n            \\"labels_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/labels{/name}\\",\\n            \\"language\\": \\"Rust\\",\\n            \\"languages_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/languages\\",\\n            \\"license\\": null,\\n            \\"master_branch\\": \\"main\\",\\n            \\"merges_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/merges\\",\\n            \\"milestones_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/milestones{/number}\\",\\n            \\"mirror_url\\": null,\\n            \\"name\\": \\"tuf-keyid\\",\\n            \\"node_id\\": \\"R_kgDOIzbqXg\\",\\n            \\"notifications_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/notifications{?since,all,participating}\\",\\n            \\"open_issues\\": 0,\\n            \\"open_issues_count\\": 0,\\n            \\"owner\\": {\\n              \\"avatar_url\\": \\"https://avatars.githubusercontent.com/u/432351?v=4\\",\\n              \\"email\\": \\"daniel.bevenius@gmail.com\\",\\n              \\"events_url\\": \\"https://api.github.com/users/danbev/events{/privacy}\\",\\n              \\"followers_url\\": \\"https://api.github.com/users/danbev/followers\\",\\n              \\"following_url\\": \\"https://api.github.com/users/danbev/following{/other_user}\\",\\n              \\"gists_url\\": \\"https://api.github.com/users/danbev/gists{/gist_id}\\",\\n              \\"gravatar_id\\": \\"\\",\\n              \\"html_url\\": \\"https://github.com/danbev\\",\\n              \\"id\\": 432351,\\n              \\"login\\": \\"danbev\\",\\n              \\"name\\": \\"danbev\\",\\n              \\"node_id\\": \\"MDQ6VXNlcjQzMjM1MQ==\\",\\n              \\"organizations_url\\": \\"https://api.github.com/users/danbev/orgs\\",\\n              \\"received_events_url\\": \\"https://api.github.com/users/danbev/received_events\\",\\n              \\"repos_url\\": \\"https://api.github.com/users/danbev/repos\\",\\n              \\"site_admin\\": false,\\n              \\"starred_url\\": \\"https://api.github.com/users/danbev/starred{/owner}{/repo}\\",\\n              \\"subscriptions_url\\": \\"https://api.github.com/users/danbev/subscriptions\\",\\n              \\"type\\": \\"User\\",\\n              \\"url\\": \\"https://api.github.com/users/danbev\\"\\n            },\\n            \\"private\\": false,\\n            \\"pulls_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/pulls{/number}\\",\\n            \\"pushed_at\\": 1674727856,\\n            \\"releases_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/releases{/id}\\",\\n            \\"size\\": 21,\\n            \\"ssh_url\\": \\"git@github.com:danbev/tuf-keyid.git\\",\\n            \\"stargazers\\": 1,\\n            \\"stargazers_count\\": 1,\\n            \\"stargazers_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/stargazers\\",\\n            \\"statuses_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/statuses/{sha}\\",\\n            \\"subscribers_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/subscribers\\",\\n            \\"subscription_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/subscription\\",\\n            \\"svn_url\\": \\"https://github.com/danbev/tuf-keyid\\",\\n            \\"tags_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/tags\\",\\n            \\"teams_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/teams\\",\\n            \\"topics\\": [],\\n            \\"trees_url\\": \\"https://api.github.com/repos/danbev/tuf-keyid/git/trees{/sha}\\",\\n            \\"updated_at\\": \\"2023-01-19T10:26:19Z\\",\\n            \\"url\\": \\"https://github.com/danbev/tuf-keyid\\",\\n            \\"visibility\\": \\"public\\",\\n            \\"watchers\\": 1,\\n            \\"watchers_count\\": 1,\\n            \\"web_commit_signoff_required\\": false\\n          },\\n          \\"sender\\": {\\n            \\"avatar_url\\": \\"https://avatars.githubusercontent.com/u/432351?v=4\\",\\n            \\"events_url\\": \\"https://api.github.com/users/danbev/events{/privacy}\\",\\n            \\"followers_url\\": \\"https://api.github.com/users/danbev/followers\\",\\n            \\"following_url\\": \\"https://api.github.com/users/danbev/following{/other_user}\\",\\n            \\"gists_url\\": \\"https://api.github.com/users/danbev/gists{/gist_id}\\",\\n            \\"gravatar_id\\": \\"\\",\\n            \\"html_url\\": \\"https://github.com/danbev\\",\\n            \\"id\\": 432351,\\n            \\"login\\": \\"danbev\\",\\n            \\"node_id\\": \\"MDQ6VXNlcjQzMjM1MQ==\\",\\n            \\"organizations_url\\": \\"https://api.github.com/users/danbev/orgs\\",\\n            \\"received_events_url\\": \\"https://api.github.com/users/danbev/received_events\\",\\n            \\"repos_url\\": \\"https://api.github.com/users/danbev/repos\\",\\n            \\"site_admin\\": false,\\n            \\"starred_url\\": \\"https://api.github.com/users/danbev/starred{/owner}{/repo}\\",\\n            \\"subscriptions_url\\": \\"https://api.github.com/users/danbev/subscriptions\\",\\n            \\"type\\": \\"User\\",\\n            \\"url\\": \\"https://api.github.com/users/danbev\\"\\n          }\\n        },\\n        \\"github_head_ref\\": \\"\\",\\n        \\"github_ref\\": \\"refs/tags/v0.2.0\\",\\n        \\"github_ref_type\\": \\"tag\\",\\n        \\"github_repository_id\\": \\"590801502\\",\\n        \\"github_repository_owner\\": \\"danbev\\",\\n        \\"github_repository_owner_id\\": \\"432351\\",\\n        \\"github_run_attempt\\": \\"1\\",\\n        \\"github_run_id\\": \\"4014167952\\",\\n        \\"github_run_number\\": \\"13\\",\\n        \\"github_sha1\\": \\"513b0e060c76a1ded7b1a4173165103838f8dde7\\"\\n      }\\n    },\\n    \\"metadata\\": {\\n      \\"buildInvocationID\\": \\"4014167952-1\\",\\n      \\"completeness\\": {\\n        \\"parameters\\": true,\\n        \\"environment\\": false,\\n        \\"materials\\": false\\n      },\\n      \\"reproducible\\": false\\n    },\\n    \\"materials\\": [\\n      {\\n        \\"uri\\": \\"git+https://github.com/danbev/tuf-keyid@refs/tags/v0.2.0\\",\\n        \\"digest\\": {\\n          \\"sha1\\": \\"513b0e060c76a1ded7b1a4173165103838f8dde7\\"\\n        }\\n      }\\n    ]\\n  }\\n}\\n```\\n\\nSo that gives us a concrete example of an attestation and in this case it is\\na [SLSA Provenance].\\n\\nNotice that the `digest` in the subject array is the sha256sum of the tuf-keyid\\nbinary:\\n\\n```console\\n$ cat tuf-keyid.intoto.jsonl | jq -r \'.payload\' | base64 -d | jq -r \'.subject[].digest.sha256\'\\n470c549740f98fe1b1977d48e014031ed5183785fd459df7e04605daefe8e293\\n$ sha256sum tuf-keyid\\n470c549740f98fe1b1977d48e014031ed5183785fd459df7e04605daefe8e293  tuf-keyid\\n```\\n\\nAlright, so next step if to verify the binary that we produced, using the\\nattestation.\\n\\nThere is project named [slsa-verifier] which can be used to verify the artifact.\\nInstalling `slsa-verifier`:\\n\\n```console\\n$ go install github.com/slsa-framework/slsa-verifier/v2/cli/slsa-verifier@v2.0.1\\n```\\n\\nLet\'s try verifying the attestation using `slsa-verifier` and using a local\\nbuild of the binary, that is, a local build on my laptop:\\n\\n```console\\n$ slsa-verifier verify-artifact --provenance-path tuf-keyid.intoto.jsonl \\\\\\n  --source-uri github.com/danbev/tuf-keyid \\\\\\n   ~/work/rust/tuf-keyid/target/release/tuf-keyid\\nVerified signature against tlog entry index 11978552 at URL: https://rekor.sigstore.dev/api/v1/log/entries/24296fb24b8ad77a217b8f07bccab3dc8caa1c7badf65f104a762647e5e355db23ccc13a22e275dd\\nFAILED: SLSA verification failed: expected hash \'32dcff46ec4be5462a66aeb5d82366da3b870d36796f3d1fe6fec6245f21ce6f\' not found: artifact hash does not match provenance subject\\n```\\n\\nAnd now let\'s see what happens when we try with the binary that was produced by\\nthe GitHub action:\\n\\n```console\\n$ slsa-verifier verify-artifact --provenance-path tuf-keyid.intoto.jsonl \\\\\\n  --source-uri github.com/danbev/tuf-keyid \\\\\\n   tuf-keyid\\nVerified signature against tlog entry index 11978552 at URL: https://rekor.sigstore.dev/api/v1/log/entries/24296fb24b8ad77a217b8f07bccab3dc8caa1c7badf65f104a762647e5e355db23ccc13a22e275dd\\nVerified build using builder https://github.com/slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@refs/tags/v1.4.0 at commit 513b0e060c76a1ded7b1a4173165103838f8dde7\\nPASSED: Verified SLSA provenance\\n```\\n\\n`slsa-verifier` can also print out the predicate information after validation\\n, using `--print-provenance`, which could then be passed to a Policy Engine:\\n\\n```console\\n$ slsa-verifier verify-artifact --provenance-path tuf-keyid.intoto.jsonl \\\\\\n  --source-uri github.com/danbev/tuf-keyid \\\\\\n  --print-provenance \\\\\\n  tuf-keyid\\n```\\n\\n[dsse]: https://github.com/secure-systems-lab/dsse\\n[slsa-github-generator]: https://github.com/slsa-framework/slsa-github-generator/blob/main/internal/builders/generic/README.md\\n[workflow run]: https://github.com/danbev/tuf-keyid/actions/runs/4015220869\\n[SLSA Provenance]: https://slsa.dev/provenance/v0.1\\n[slsa-verifier]: https://github.com/slsa-framework/slsa-verifier#example"},{"id":"/2023/02/13/elfsign","metadata":{"permalink":"/blog/2023/02/13/elfsign","editUrl":"https://github.com/trustification/trustification.github.io/tree/main/blog/2023-02-13-elfsign.md","source":"@site/blog/2023-02-13-elfsign.md","title":"Signing elf binaries, or not?! Lessons learned.","description":"Trying to figure out what went into a binary can be a tricky thing. And once you figured it out, how do you","date":"2023-02-13T00:00:00.000Z","formattedDate":"February 13, 2023","tags":[],"readingTime":8.66,"hasTruncateMarker":true,"authors":[{"name":"Jens Reimann","title":"Maintainer","url":"https://github.com/ctron","imageURL":"https://github.com/ctron.png","key":"ctron"}],"frontMatter":{"title":"Signing elf binaries, or not?! Lessons learned.","authors":"ctron","tags":[]},"prevItem":{"title":"in-toto attestations","permalink":"/blog/2023/03/13/in-toto-attestations"},"nextItem":{"title":"Continuing the Adventure with the CycloneDX Maven Plugin","permalink":"/blog/2023/02/10/cyclonedx-maven-plugin-adventure-continues"}},"content":"Trying to figure out what went into a binary can be a tricky thing. And once you figured it out, how do you\\ntransport this information? True, it all starts simple: Java, NodeJS, Go, or Rust, all languages[^1] bring their\\ndependency management, which defines what the final command line tool you create is made of. Or, does it?\\n\\n\x3c!--truncate--\x3e\\n\\nBut let\'s take a step back: A typical use-case today is to download a command line application from the internet.\\nTake `helm` for example. You navigate to their GitHub releases page, download the binary, unzip it into a local\\nfolder and run it. But, what exactly is inside the binary?\\n\\n## SBOMs\\n\\n_Software Bills of Material_ (SBOMs) are a thing which already exist. Yet, someone needs to create them,\\nand creating an accurate SBOM can be tricky.\\n\\nThere are tools to create SBOMs from the source code of a project, but that doesn\'t tell the whole story in\\nmost cases. Those tools simply analyze the dependency information declared in e.g. a `package.lock` file, or a Maven `pom.xml`. Should be enough, right? Well, no. Maven projects for example have \\"profiles\\", and you\\nneed to exactly generate the SBOM for the profile that gets enabled when creating the final artifact. Also,\\nwith Maven mirrors and proxies, it\'s not always 100% clear what gets into a \\"binary\\" and where it comes from.\\n\\n## What really goes in\\n\\nThen again, JAR files aren\'t real binaries are they. They are ZIP files, which contain compiled `.class` files. So\\nit actually is pretty simple to understand what is in there. Assuming you trust your build process\\n(which is a topic of its own). Even if you create a \\"fat\\", or \\"shaded\\" JAR, it is possible to understand what\\nreally ended up in your \\"binary\\".\\n\\nAnd, others can do the same. Rust for example allows one use `cargo auditable`, to tap into the compilation process,\\nand record the actual dependencies which go into a binary. On Linux, the binary will be an ELF file, which then\\ncontains the dependency information from the compilation process. And Go can do the same.\\n\\n## Null and void\\n\\nBut, if someone can write dependency information into a binary, then someone else can also overwrite it. So\\nunless you protect the binary against modifications, this information isn\'t really trustworthy. Taking a look at\\nthe Helm release page, you will find SHA based checksums. Isn\'t that enough?\\n\\nNo, not really. Because if someone can alter the binary on the download page, the attacker can also swap out the\\nSHA checksum file. The checksum, or digest, really isn\'t more than a checksum. By its own, it doesn\'t protect much.\\n\\nIf however, you encrypt the digest with a private key that only the author knows, and you publish the public key\\npart, then this becomes a \\"signature\\". And this is good enough to prove, that only the person how knew the private\\nkey, could have signed the binary. And if we can trust this person to create the correct SBOM and binary, and keep\\nthe private key secure, we are good.\\n\\n## Usability\\n\\nOr, not! In the Maven world JARs have been signed for quite a while. Everyone uploading JARs to Maven Central needs\\nto sign their JAR with GPG. And I guess most people never validated a JAR during a build.\\n\\nOn the other side, the Eclipse IDE (as well as some other Java applications), did JAR file validation for quite a\\nwhile. Whenever you install a plugin, it cryptographically validates the JAR. And it\'s easy, as the JAR file itself\\nis signed, and the signature is part of the JAR file. As part of the build process in the Eclipse Foundation\\nbuild system, JARs which got created by the build, can automatically get signed. No additional files needed,\\nand only the actual build output is considered, no guessing of dependencies.\\n\\nFrom a user perspective, the IDE automatically checks signatures, and only bothers the user if an issue was found.\\nThe user can override, because the idea is to give the final authority to the user.\\n\\n## Back to binaries\\n\\nNow, just assume we could do the same with (ELF) binaries. In fact this was possible a while back, Solaris had some\\ntools to sign ELF binaries. That doesn\'t help on modern Linux systems. But with a bit of Rust code, it was possible\\nto create [elfsign](https://github.com/ctron/elfsign). The idea is simple:\\n\\n- Create a digest of all relevant ELF sections and headers\\n- Sign this digest\\n- Add the signature to the ELF binary as a new section\\n\\n## Solvable downsides\\n\\nThe first downside might be that people are afraid of signed binaries. Windows and macOS have been doing this for a\\nwhile, and it happens that signatures get in the way of the user running a binary. Well, actually it is the operating\\nsystem which gets into the user\'s way. To protect the user, that\'s the argument. And that might be true, but it also\\nis true that some users indeed know better than the operating system and want to have the final word in what they\\ncan run.\\n\\nThis problem can easily be addressed. Checking a signature, and making a decision if a binary can be run or not,\\nactually are two different things. Even if a system brings a default rule/policy set which would reject invalid\\nsignatures, it could still be possible to let the user customize the behavior and override, just like the\\nEclipse IDE does.\\n\\nAnother issue the handling of keys and certificates. Prices for code signing certificates can be quite high.\\nEspecially when we are talking about open source projects, this can become a truly limiting factor. It also takes a\\nbit of care, handling a private key properly.\\n\\nLuckily, we now have [sigstore](https://www.sigstore.dev/). Sigstore can help us with two things, creating\\nshort-lived private keys (Fulcio), and a tamper-resistant log (Rekor). We already talked a bit about both in\\nthe context of [gitsign]({% post_url 2022-12-02-sign-commits-with-sigstore %}).\\n\\nAdding Fulcio and Rekor to `elfsign`, we gain a bunch of cool things:\\n\\n- Short-lived, disposable private keys: You don\'t need to store them, they are only valid for a few minutes.\\n- X.509 certificates: Alongside the key, we get an X.509 certificates, with our identity, which we can use for signing.\\n- An attestation that we provided the valid certificate and signature to Rekor, at a time the key was valid\\n\\nAnd with that, we can easily implement `elfsign sign` to sign a binary, and `elfsign verify` to validate one. We\\ncould also create something like `elfsign execute` to verify and execute, but that\'s just a variant of `verify`.\\n\\nAs we can prove, using Rekor, that we did own the private key during the time the certificate was valid, and we\\nprovided the signature/digest at the same time, we now only need to decide if we want to trust the issuer and the\\nsubject the certificate was issued for. And by storing [the Rekor bundle]({% post_url 2023-01-13-sigstore-bundle-format %}), we can do this offline too.\\n\\n## Too good to be true?\\n\\nSo where\'s the catch?\\n\\nSigning elf binaries adds a bit of extra complexity. Creating a digest of an ELF binary isn\'t as trivial as\\njust running `sha256sum` on a file. Storing an additional \\"signature section\\" in the ELF binary, will actually\\nalter it. So it is necessary have a normalized view on the ELF binary, which creates a reproducible digest, one that\\ndoes include all important information, but excludes the signature information itself, and still is a valid ELF\\nbinary format.\\n\\nIt works, but is a bit complex. And more complexity might lead to more bugs, which isn\'t a good thing when\\nit comes to cryptography. But if this allows one to drop handling additional files (like SBOMs or checksum files),\\nand increase the usability, it may actually be worth it.\\n\\nThe problem is, that the tooling which creates the dependency information for the ELF binaries, isn\'t\\naccurate enough.\\n\\nIn many cases it works, but as soon as you include a C library, add some JavaScript for an embedded frontend, or\\ndeviate from \\"standard artifact repositories\\", many of those tools just fall short. And I am not even talking\\nabout all those little hacks in build systems, or the mess called \\"vendor\\" folder in Go.\\n\\nSBOM formats like [CycloneDX](https://cyclonedx.org/) allow you to compensate and fix up generated SBOMs.\\nBut, that\'s another step in the build process, and the output doesn\'t go into the ELF binary. As Go only cares\\nabout Go dependencies, and Cargo only about Cargo.\\n\\nSo adding all the complexity isn\'t good enough in the end. You still need to handle an extra file, and validate it.\\n\\n## Happy end?\\n\\nThe truth is, `cosign`, which is intended to sign containers, can actually sign any BLOB. Just the same way,\\nusing Fulcio to get a short-lived private key and certificate, and storing the signature in Rekor. So if we can make\\nour peace with handling an extra file, we can just use `cosign sign-blob` and `cosign verify-blob` to sign\\nanything we want. Using `cosign attest-blob`, we can even \\"attach\\" an SBOM to the Rekor log entry.\\n\\nYes, we need to handle the extra \\"bundle\\" and \\"signature\\" files. Or we can accept the fact that we need rely on the\\nuptime of the Rekor instance (or our ISP). But it definitely improves the situation over the status quo.\\n\\n## So what?\\n\\n`elfsign` was a nice experiment. And while it didn\'t work out, I still learned a lot. I also still believe that\\nthe idea works out in general. It just needs more work for a more specialized solution. So the approach of\\n\\"cosign blob\\" is a more generic one. However, through that, also less user-friendly one.\\n\\nBut this situation could also be improved. Just assume someone would create a more convenient version of\\ncosign, which \\"downloads and verifies\\" or \\"verifies and executes\\". That would definitely lead to a similar\\nuser experience, and help with adoption.\\n\\nAnd, having a policy engine like Seedwing, you could actually define checks like: Only run this binary if it is signed, and does not contain a dependency which has an active CVE.\\n\\nIf you are interested in things like this, maybe this blog post gave you a few insights and ideas.\\n\\n[^1]: Yes, C/C++ is missing here. Let\'s not talk about build systems and dependency management for C/C++ \ud83d\ude09"},{"id":"/2023/02/10/cyclonedx-maven-plugin-adventure-continues","metadata":{"permalink":"/blog/2023/02/10/cyclonedx-maven-plugin-adventure-continues","editUrl":"https://github.com/trustification/trustification.github.io/tree/main/blog/2023-02-10-cyclonedx-maven-plugin-adventure-continues.md","source":"@site/blog/2023-02-10-cyclonedx-maven-plugin-adventure-continues.md","title":"Continuing the Adventure with the CycloneDX Maven Plugin","description":"My investigation into the CycloneDX Maven Plugin began back in November/December 2022 with the intent of integrating the plugin into the Quarkus build process to generate Software Bill of Materials (SBOMs) for the project. I quickly discovered issues in the plugin and raised these with the maintainer early in December, writing a blog post (An Adventure with the CycloneDX Maven Plugin) to help clarify each issue. I finally opened a pull request in early January to move the conversation forward and this is where our story continues .....","date":"2023-02-10T00:00:00.000Z","formattedDate":"February 10, 2023","tags":[{"label":"cyclonedx","permalink":"/blog/tags/cyclonedx"}],"readingTime":15.685,"hasTruncateMarker":true,"authors":[{"name":"Jens Reimann","title":"Maintainer","url":"https://github.com/ctron","imageURL":"https://github.com/ctron.png","key":"ctron"}],"frontMatter":{"title":"Continuing the Adventure with the CycloneDX Maven Plugin","authors":"ctron","tags":["cyclonedx"]},"prevItem":{"title":"Signing elf binaries, or not?! Lessons learned.","permalink":"/blog/2023/02/13/elfsign"},"nextItem":{"title":"The Update Framework (TUF)","permalink":"/blog/2023/01/31/tuf"}},"content":"My investigation into the [CycloneDX Maven Plugin](https://github.com/CycloneDX/cyclonedx-maven-plugin \\"The CycloneDX Maven Plugin GitHub repository\\") began back in November/December 2022 with the intent of integrating the plugin into the [Quarkus](https://github.com/quarkusio/quarkus \\"The Quarkus GitHub repository\\") build process to generate Software Bill of Materials (SBOMs) for the project. I quickly discovered issues in the plugin and raised these with the maintainer early in December, writing a blog post ([An Adventure with the CycloneDX Maven Plugin](/blog/2022/12/09/cyclonedx-maven-plugin-adventure)) to help clarify each issue. I finally opened a pull request in early January to move the conversation forward and this is where our story continues .....\\n\\n\x3c!--truncate--\x3e\\n\\n# Let\'s Get Started\\n\\nTwo weeks ago I received some feedback on the pull request from Steve Springett, he ran my version of the CycloneDX plugin and hit some problems. Steve was running the plugin against the [WebGoat 8.0.0](https://github.com/WebGoat/WebGoat/tree/v8.0.0) codebase and noticed some dependencies were not present in the components section! This was intriguing as I had been running the plugin against a complex codebase ([Quarkus](https://github.com/quarkusio/quarkus \\"The Quarkus GitHub repository\\")) without seeing the issue, and had also included a BOM validation step in my pull request which would emit **WARNING** log messages if this situation occurred. I took a look at the WebGoat codebase and could not get this specific version to build, however a build of a different version did succeed without displaying the problem. Curiouser and curiouser .......\\n\\nWe now jump forward to this Monday (4 days ago) when I\'m trying to arrange a call with Steve to discuss the differences in our environments and help move this forward. Steve suggested we include Herv\xe9 Boutemy in the call, the new maintainer of the upstream codebase, however he offered instead to review my pull request as-is. It\'s at this point I realised the pull request now had conflicts with the base branch, so I quickly rebased and fixed the conflicts. I also decided to give the WebGoat codebase another try.\\n\\nI spent time investigating the failures I had seen with the WebGoat build and eventually realised I needed to be running on an older version of Java, I needed to install JDK8 in order to make progress. I was now able to build the same version of the code Steve had been using, although with errors, but could now see missing components. Even better, I could also see the expected **WARNING** messages were present!\\n\\n```\\n[WARNING] CycloneDX: Dependency missing component entry: pkg:maven/org.webjars/jquery@1.11.1?type=jar\\n[WARNING] CycloneDX: Dependency missing component entry: pkg:maven/commons-io/commons-io@LATEST?type=jar\\n[WARNING] CycloneDX: Dependency missing component entry: pkg:maven/com.google.guava/guava@18.0?type=jar\\n[WARNING] CycloneDX: Dependency missing component entry: pkg:maven/org.apache.commons/commons-lang3@3.4?type=jar\\n```\\n\\nThis was great, I now had something to work with.\\n\\n# Comparing Upstream Output with my pull request\\n\\nBefore investigating I decided to first understand the differences in output between the current upstream codebase and what was being generated by my pull request. This may provide some insight into the new issue and could possibly hint at a direction to follow.\\n\\nWith regard to **components** I discovered three were missing from my version of the bom, however in each case the component was never referenced in the dependencies section. These components were\\n\\n- pkg:maven/com.google.guava/guava@20.0?type=jar\\n- pkg:maven/commons-io/commons-io@2.11.0?type=jar\\n- pkg:maven/org.apache.commons/commons-lang3@3.6?type=jar\\n\\nThese are three of the components we were warned about, but suspiciously each has a different version.\\n\\nI also found we were now including two additional **components**, these are\\n\\n- pkg:maven/junit/junit@4.12?type=jar\\n- pkg:maven/org.hamcrest/hamcrest-core@1.3?type=jar\\n\\nWith regard to **dependencies** we were expanding the dependency tree to include transitive dependencies for the following\\n\\n- pkg:maven/com.fasterxml.jackson.core/jackson-databind@2.8.11.1?type=jar\\n- pkg:maven/org.springframework.boot/spring-boot-autoconfigure@1.5.12.RELEASE?type=jar\\n- pkg:maven/org.springframework.boot/spring-boot@1.5.12.RELEASE?type=jar\\n- pkg:maven/org.springframework.security/spring-security-core@4.2.5.RELEASE?type=jar\\n- pkg:maven/org.springframework.security/spring-security-web@4.2.5.RELEASE?type=jar\\n- pkg:maven/org.springframework/spring-aop@4.3.16.RELEASE?type=jar\\n- pkg:maven/org.springframework/spring-beans@4.3.16.RELEASE?type=jar\\n- pkg:maven/org.springframework/spring-context@4.3.16.RELEASE?type=jar\\n- pkg:maven/org.springframework/spring-expression@4.3.16.RELEASE?type=jar\\n- pkg:maven/org.springframework/spring-test@4.3.16.RELEASE?type=jar\\n- pkg:maven/org.springframework/spring-web@4.3.16.RELEASE?type=jar\\n- pkg:maven/org.webjars/bootstrap@3.3.7?type=jar\\n\\nas well as adding new **dependencies** into the tree\\n\\n- pkg:maven/aopalliance/aopalliance@1.0?type=jar\\n- pkg:maven/org.hamcrest/hamcrest-core@1.3?type=jar\\n- pkg:maven/junit/junit@4.12?type=jar\\n\\nhowever, we are also seeing the following **dependencies** without any mention in the **component** section\\n\\n- pkg:maven/com.google.guava/guava@18.0?type=jar\\n- pkg:maven/commons-io/commons-io@LATEST?type=jar\\n- pkg:maven/org.apache.commons/commons-lang3@3.4?type=jar\\n- pkg:maven/org.webjars/jquery@1.11.1?type=jar\\n\\nThese match the list of **dependencies** reported as **WARNING**s in the log, confirming the issue.\\n\\nWe now know the pull request codebase is having a beneficial effect and providing a more detailed dependency graph. What is left to work out is why we are seeing these four dependencies in the tree with no associated component.\\n\\n# Identity, Does it Matter?\\n\\nBefore we take a look at each of the problematic dependencies let us quickly cover how components are identified in the upstream CycloneDX codebase and in my pull request.\\n\\nThe upstream codebase discovers its components by asking maven for those artifacts which it has resolved to be the definitive set for the build. These artifacts are then filtered based on their scope, however as we discovered in the previous post this does not follow the transitive scoping rules applied by maven, and then used to create the set of components included in the bom file. It is also important to realise that when resolving the dependency tree the upstream codebase will not include any dependencies which do not exist in the set of known components. No components will be removed, even if they do not take part in the dependency tree.\\n\\nIn my pull request we take a slightly different approach. To discover the set of possible components we still ask maven for the definitive set of artifacts, but rely instead on maven to handle the filtering when collecting the dependency graph. At the end of the process we check the set of components and remove any which do not appear in the dependency tree. No dependencies will be removed, even if they do not have an associated component, however a warning is emitted on the console. This is the warning we are now seeing.\\n\\nThese approaches are, essentially, tackling the discovery from opposite directions.\\n\\nWith the above in mind let us now take a look at the problematic artifacts and return to our trusty dependency tree graph. We can see from the **WARNING**s that we should focus on two of the projects\\n\\n- **xxe** for **guava** and **commons-lang3**\\n- **webwolf** for **jquery** and **commons-io**\\n\\n## A look at Guava\\n\\nThe parts of the **xxe** dependency tree which are of interest are\\n\\n```\\norg.owasp.webgoat.lesson:xxe:jar:v8.0.0.M15\\n+- com.github.tomakehurst:wiremock:jar:2.8.0:test\\n|  +- com.google.guava:guava:jar:20.0:provided\\n|  +- com.flipkart.zjsonpatch:zjsonpatch:jar:0.3.0:test\\n|  |  +- (com.google.guava:guava:jar:18.0:test - omitted for conflict with 20.0)\\n+- org.owasp.webgoat:webgoat-container:jar:v8.0.0.M15:provided\\n|  +- (com.google.guava:guava:jar:18.0:provided - omitted for conflict with 20.0)\\n+- org.owasp.webgoat:webgoat-container:jar:tests:v8.0.0.M15:test\\n|  +- (com.google.guava:guava:jar:18.0:test - omitted for conflict with 20.0)\\n```\\n\\nFrom this we can see **guava:20.0** has been resolved as the winner by maven, however the winning artifact is hidden beneath a **test** scoped artifact (we saw this in our previous issues). We can also see the artifact discovered through the transitive **compile** scope is being reported as **guava:18.0**, so while version **20.0** has been declared the winner we are still seeing the marker nodes report the original version of **18.0**. How does each version of the plugin handle this scenario?\\n\\nThe upstream code discovers **guava:20.0** in the set of resolved artifacts, including it in its set of known components. When creating the dependency tree it discovers **guava:18.0**, however decides not to include it as this version is not in the set of known components. This results in a bom which includes the **guava:20.0** component and a dependency graph which does not reference the **guava** dependency, losing the dependency relationship between **webgoat-container** and **guava**. The bom looks as follows\\n\\n```\\n<component type=\\"library\\" bom-ref=\\"pkg:maven/com.google.guava/guava@20.0?type=jar\\">\\n```\\n\\nIn my pull request we discover **guava:20.0** in the set of resolved artifacts, including it as a known component. When creating the dependency tree we discover the **guava:18.0** dependency and include it in the tree. At the end of the process we drop components which are not mentioned in the dependency tree, in this instance the **guava:20.0** component, but keep the dependency relationship between **webgoat-container** and **guava:18.0**, which is a missing component. The bom looks as follows\\n\\n```\\n<dependency ref=\\"pkg:maven/org.owasp.webgoat/webgoat-container@v8.0.0.M15?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.google.guava/guava@18.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.google.guava/guava@18.0?type=jar\\"/>\\n```\\n\\n## A look at commons-lang3\\n\\nThe parts of the **xxe** dependency tree which are of interest are\\n\\n```\\norg.owasp.webgoat.lesson:xxe:jar:v8.0.0.M15\\n+- com.github.tomakehurst:wiremock:jar:2.8.0:test\\n|  +- org.apache.commons:commons-lang3:jar:3.6:provided\\n|  \\\\- com.github.jknack:handlebars:jar:4.0.6:test\\n|     +- (org.apache.commons:commons-lang3:jar:3.1:test - omitted for conflict with 3.6)\\n+- org.owasp.webgoat:webgoat-container:jar:v8.0.0.M15:provided\\n|  +- (org.apache.commons:commons-lang3:jar:3.4:provided - omitted for conflict with 3.6)\\n+- org.owasp.webgoat:webgoat-container:jar:tests:v8.0.0.M15:test\\n|  +- (org.apache.commons:commons-lang3:jar:3.4:test - omitted for conflict with 3.6)\\n```\\n\\nWe can see from the above that the **commons-lang3** artifact suffers from the same problem as the **guava** artifact, with the artifact identified through the transitive **compile** scope having a version of **3.4** while the resolved winner has a version of **3.6** but is hidden beneath a **test** scoped artifact. We can also see there is a third version being referenced beneath the **test** scoped artifact, **commons-lang3:3.1**.\\n\\nThe upstream bom looks as follows\\n\\n```\\n<component type=\\"library\\" bom-ref=\\"pkg:maven/org.apache.commons/commons-lang3@3.6?type=jar\\">\\n```\\n\\nThe bom from my pull request looks as follows\\n\\n```\\n<dependency ref=\\"pkg:maven/org.owasp.webgoat/webgoat-container@v8.0.0.M15?type=jar\\">\\n  <dependency ref=\\"pkg:maven/org.apache.commons/commons-lang3@3.4?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/org.apache.commons/commons-lang3@3.4?type=jar\\"/>\\n```\\n\\n## A look at jquery\\n\\nThe parts of the **webwolf** dependency tree which are of interest are\\n\\n```\\norg.owasp.webgoat:webwolf:jar:v8.0.0.M15\\n+- org.webjars:bootstrap:jar:3.3.7:compile\\n|  \\\\- (org.webjars:jquery:jar:1.11.1:compile - omitted for conflict with 3.2.1)\\n+- org.webjars:jquery:jar:3.2.1:compile\\n```\\n\\nThis scenario is slightly different from the previous ones in that the resolved component is not hidden behind a **test** scoped artifact. We can see from the above that we have two artifacts being discovered within the transitive **compile** scope, **jquery:3.2.1** and **jquery:1.11.1**. Version **3.2.1** is the resolved winner and version **1.11.1** is the marker node for an artifact which lost the resolution process. How does each version of the plugin handle this scenario?\\n\\nThe upstream code discovers **jquery:3.2.1** in the set of resolved artifacts, including it in its set of known components. When creating the dependency tree it discovers both **jquery:3.2.1** and **jquery:1.11.1**, including **3.2.1** in the tree but deciding not to include **1.11.1** as this does not match a known component. This results in a bom which includes the **jquery:3.2.1** component and the dependency relationship between **webwolf** and **jquery** but loses the dependency relationship between **bootstrap** and **jquery**. The bom looks as follows\\n\\n```\\n<component type=\\"library\\" bom-ref=\\"pkg:maven/org.webjars/jquery@3.2.1?type=jar\\">\\n\\n<dependency ref=\\"pkg:maven/org.owasp.webgoat/webwolf@v8.0.0.M15?type=jar\\">\\n  <dependency ref=\\"pkg:maven/org.webjars/jquery@3.2.1?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/org.webjars/jquery@3.2.1?type=jar\\"/>\\n```\\n\\nIn my pull request we discover **jquery:3.2.1** in the set of resolved artifacts, including it as a known component. When creating the dependency tree we discover both **jquery:3.2.1** and **jquery:1.11.1**, including both in the tree. This results in a bom which includes the **jquery:3.2.1** component and the dependency relationship between **webwolf** and **jquery**. The bom also keeps the dependency relationship between **bootstrap** and **jquery:1.11.1**, which is a missing component. The bom looks as follows\\n\\n```\\n<component type=\\"library\\" bom-ref=\\"pkg:maven/org.webjars/jquery@3.2.1?type=jar\\">\\n\\n<dependency ref=\\"pkg:maven/org.owasp.webgoat/webwolf@v8.0.0.M15?type=jar\\">\\n  <dependency ref=\\"pkg:maven/org.webjars/jquery@3.2.1?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/org.webjars/bootstrap@3.3.7?type=jar\\">\\n  <dependency ref=\\"pkg:maven/org.webjars/jquery@1.11.1?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/org.webjars/jquery@1.11.1?type=jar\\"/>\\n<dependency ref=\\"pkg:maven/org.webjars/jquery@3.2.1?type=jar\\"/>\\n```\\n\\n## A look at commons-io\\n\\nThe parts of the **webwolf** dependency tree which are of interest are\\n\\n```\\norg.owasp.webgoat:webwolf:jar:v8.0.0.M15\\n+- commons-io:commons-io:jar:LATEST:compile\\n```\\n\\nNow this scenario is very different from the previous ones. In each of the previous scenarios the dependency tree included marker nodes with versions which did not match the version resolved by maven, the first two with the resolved artifact hidden behind a **test** scoped artifact and the third with both artifacts discovered through the transitive **compile** scope. So what is going on here? It\'s time for a quick dive under the covers of maven!\\n\\nMaven includes support for two _metaversions_ which can be used when specifying the version of an artifact, these are **RELEASE** and **LATEST**. These _metaversions_ have specific meanings when resolving artifacts within a pom, these are\\n\\n- **RELEASE**: represents the latest non-snapshot version of the artifact within a repository\\n- **LATEST**: represents the latest version of the artifact within a repository, which includes both released and snapshot versions\\n\\n---\\n\\n**Note:** Using either **RELEASE** or **LATEST** in a build breaks reproducibility. Thankfully maven is now issuing the following deprecation **WARNING** when encountering these _metaversions_, which means support for these versions should be removed at some point in the future.\\n\\n```\\n[WARNING] \'dependencies.dependency.version\' for commons-io:commons-io:jar is either LATEST or RELEASE (both of them are being deprecated)\\n```\\n\\n---\\n\\nWhen maven encounters either of these _metaversions_ it will resolve the artifact to a specific version based on the above meanings. In our case, at least as of today, maven will resolve **commons-io:LATEST** to **commons-io:2.11.0**. How does each version of the plugin handle this scenario?\\n\\nThe upstream code discovers **commons-io:2.11.0** in the set of resolved artifacts, including it in its set of known components. When creating the dependency tree it discovers **commons-io:LATEST**, however decides not to include it as this version is not in the set of known components. This results in a bom which includes the **commons-io:2.11.0** component and a dependency graph which does not reference the **common-io** dependency, losing the dependency relationship between **webwolf** and **commons-io**. The bom looks as follows\\n\\n```\\n<component type=\\"library\\" bom-ref=\\"pkg:maven/commons-io/commons-io@2.11.0?type=jar\\">\\n```\\n\\nIn my pull request we discover **commons-io:2.11.0** in the set of resolved artifacts, including it as a known component. When creating the dependency tree we discover the **commons-io:LATEST** dependency and include it in the tree. At the end of the process we drop components which are not mentioned in the dependency tree, in this instance the **commons-io:2.11.0** component, but keep the dependency relationship between **webwolf** and **commons-io:LATEST**, which is a missing component. The bom looks as follows\\n\\n```\\n<dependency ref=\\"pkg:maven/org.owasp.webgoat/webwolf@v8.0.0.M15?type=jar\\">\\n  <dependency ref=\\"pkg:maven/commons-io/commons-io@LATEST?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/commons-io/commons-io@LATEST?type=jar\\"/>\\n```\\n\\n## Summarising the issues\\n\\nWe have three different scenarios here, however each has the same root cause. Maven is returning a dependency graph which includes marker nodes referencing the original artifact versions and not the versions resolved within the context of the build. These marker nodes have a different identity to the resolved dependencies and are, therefore, treated separately. As we would expect, identity does matter!\\n\\nWith the upstream codebase we see the resolved components being included in the bom, but with certain dependency relationships missing from the dependency tree.\\n\\nWith my pull request we see some missing components from the bom, but with all dependency relationships included in the dependency tree. The problem is that some of these relationships reference dependencies with their original version and not the version resolved by maven.\\n\\n---\\n\\n**Note:** The issues we are seeing do not happen with dependencies which have their version managed, if we look at the node for a managed dependency we can see the version of the marker has been updated\\n\\n```\\norg.slf4j:slf4j-api:1.7.25:compile    (org.slf4j:slf4j-api:jar:1.7.25:compile - version managed from 1.6.6; omitted for duplicate)\\n```\\n\\nIn this case the version of the marker node has been updated from **1.6.6** to **1.7.25**.\\n\\nUnfortunately this additional information is not available to us other than through the **toNodeString** method on the **VerboseDependencyNode** class, that is unless we delve under the covers and work on the internal _[aether](https://wiki.eclipse.org/Aether \\"Eclipse Aether website\\")_ dependency tree which does contain a data map including this information.\\n\\n---\\n\\n## Now for the solution\\n\\nNow we have identified a root cause there is an obvious solution. We know maven is not updating the versions for some marker nodes, leaving them with their original version, so we need to handle this aspect. We need to track the versions of the resolved artifacts and, when creating the dependency graph, ensure all dependency versions reference the resolved version of the artifact. Thankfully this is a straight forward update to the codebase.\\n\\nNow that we have a working solution how does this look for each component?\\n\\n### Recap and Solution for guava\\n\\nFrom our earlier discussion we saw the upstream plugin had identified the correct guava version for the component, but had lost all dependency relationships, and my pull request had kept the dependency relationships but had lost the component as it was referring to the original version of the artifact. What do we see now in the bom file?\\n\\n```\\n<component type=\\"library\\" bom-ref=\\"pkg:maven/com.google.guava/guava@20.0?type=jar\\">\\n\\n<dependency ref=\\"pkg:maven/org.owasp.webgoat/webgoat-container@v8.0.0.M15?type=jar\\">\\n  <dependency ref=\\"pkg:maven/com.google.guava/guava@20.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/com.google.guava/guava@20.0?type=jar\\"/>\\n```\\n\\nFantastic, we now see a component with the version resolved by maven and all the dependency relationships we were expecting!\\n\\n### Recap and Solution for commons-lang3\\n\\nFrom our earlier discussion we saw a similar issue with commons-lang3. The upstream plugin had identified the correct commons-lang3 version for the component, but had lost all dependency relationships, and my pull request had kept the dependency relationships but had lost the component as it was using the original version of the artifact. What do we see now in the bom file?\\n\\n```\\n<component type=\\"library\\" bom-ref=\\"pkg:maven/org.apache.commons/commons-lang3@3.6?type=jar\\">\\n\\n<dependency ref=\\"pkg:maven/org.owasp.webgoat/webgoat-container@v8.0.0.M15?type=jar\\">\\n  <dependency ref=\\"pkg:maven/org.apache.commons/commons-lang3@3.6?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/org.apache.commons/commons-lang3@3.6?type=jar\\"/>\\n```\\n\\nWe are now two for two, we again see the component with the resolved version and also see all the dependency relationships!\\n\\n### Recap and Solution for jquery\\n\\nIn our earlier discussion we had identified a slightly different scenario with jquery, as the **compile** scoped artifacts included both the resolved version (3.2.1) and an older version (1.11.1). Both plugins had identified the component and included the dependency relationship between **webwolf** and **jquery**, however the upstream plugin had lost the dependency relationship between **bootstrap** and **jquery** whereas my pull request included the dependency but referenced the original version. What do we now see in the bom file?\\n\\n```\\n<component type=\\"library\\" bom-ref=\\"pkg:maven/org.webjars/jquery@3.2.1?type=jar\\">\\n\\n<dependency ref=\\"pkg:maven/org.owasp.webgoat/webwolf@v8.0.0.M15?type=jar\\">\\n  <dependency ref=\\"pkg:maven/org.webjars/jquery@3.2.1?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/org.webjars/bootstrap@3.3.7?type=jar\\">\\n  <dependency ref=\\"pkg:maven/org.webjars/jquery@3.2.1?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/org.webjars/jquery@3.2.1?type=jar\\"/>\\n```\\n\\nWe are on a roll, and are now three for three. We can see all the expected dependency relationships are present with all the relationships referencing the resolved version!\\n\\n### Recap and Solution for commons-io\\n\\nNow we come to our final scenario and the use of _metaversions_, can we make it four for four?\\n\\nIn our earlier discussion we covered the use and meaning of _metaversions_ within maven dependencies and saw the upstream plugin had correctly identified the resolved component, but had no dependency relationships, whereas my pull request identified the dependency relationships using the **LATEST** _metaversion_ but did not identify the component. What do we now see in the bom file?\\n\\n```\\n<component type=\\"library\\" bom-ref=\\"pkg:maven/commons-io/commons-io@2.11.0?type=jar\\">\\n\\n<dependency ref=\\"pkg:maven/org.owasp.webgoat/webwolf@v8.0.0.M15?type=jar\\">\\n  <dependency ref=\\"pkg:maven/commons-io/commons-io@2.11.0?type=jar\\"/>\\n</dependency>\\n<dependency ref=\\"pkg:maven/commons-io/commons-io@2.11.0?type=jar\\"/>\\n```\\n\\nBrilliant, the component and all expected dependency relationships are present, with each referencing the resolved version and not the _metaversion_!\\n\\nWe have done it, we are now four for four!\\n\\n# Conclusions\\n\\nWith this latest issue now resolved I feel we have a much better solution for generating SBOMs for maven projects. We know these bom files will contain all dependency relationships returned via maven, and now this version mismatch issue has been addressed we can be confident we will only include entries for resolved artifacts.\\n\\nMy original pull request has been updated to include the fix for these issues, in addition to the issues covered in the previous post ([An Adventure with the CycloneDX Maven Plugin](/blog/2022/12/09/cyclonedx-maven-plugin-adventure)), and has now been merged into the upstream codebase with help from Herv\xe9. I\'m looking forward to having this released in the next [CycloneDX Maven Plugin](https://github.com/CycloneDX/cyclonedx-maven-plugin \\"The CycloneDX Maven Plugin GitHub repository\\") release and being able to use this in earnest as part of our effort to secure our Software Supply Chain. With any luck this can also be of benefit to your efforts, at least I hope that proves to be the case."},{"id":"/2023/01/31/tuf","metadata":{"permalink":"/blog/2023/01/31/tuf","editUrl":"https://github.com/trustification/trustification.github.io/tree/main/blog/2023-01-31-tuf.md","source":"@site/blog/2023-01-31-tuf.md","title":"The Update Framework (TUF)","description":"TUF seems to pop again and again when learning about Secure Supply-Chain","date":"2023-01-31T00:00:00.000Z","formattedDate":"January 31, 2023","tags":[],"readingTime":19.76,"hasTruncateMarker":true,"authors":[{"name":"Daniel Bevenius","title":"Maintainer","url":"https://github.com/danbev","imageURL":"https://github.com/danbev.png","key":"danbev"}],"frontMatter":{"title":"The Update Framework (TUF)","authors":"danbev","tags":[]},"prevItem":{"title":"Continuing the Adventure with the CycloneDX Maven Plugin","permalink":"/blog/2023/02/10/cyclonedx-maven-plugin-adventure-continues"},"nextItem":{"title":"Is this a cryptographic key which I see before me?","permalink":"/blog/2023/01/25/keys"}},"content":"TUF seems to pop again and again when learning about Secure Supply-Chain\\nSecurity (SSCS). The goal of this post is to get some hands-on experience\\nwith TUF, showing examples that will hopefully clarify TUF concepts, and\\nthe reason for using it in projects like Sigstore.\\n\\n\x3c!--truncate--\x3e\\n\\nAs the name \\"The Update Framework\\" implies this is a framework for update\\nsystems, and doing so in a secure way. What is getting updated could be\\nanything, it could be software packages, source files, certificates, public\\nkeys, etc. And by following this \\"framework\\", updates can be performed in a\\nsecure way.\\n\\nProducers want the things they produce to be available to consumers, and they\\nwant to make sure that consumers are getting updates for these things. I\'m using\\n\\"things\\" just to make it clear that this does not have to be software packages\\nwhich might be what first comes to mind.\\n\\nLets take a look what this might look like without TUF:\\n\\n```\\n  +-----------+     +---------------------+                      +----------+\\n  | Producer  |     | Distribution server |                      | Consumer |\\n  |-----------+     |---------------------+                      |----------|\\n  | thing_v1  | --\x3e | thing_v1            | -------------------\x3e | thing_v1 |\\n  +-----------+     +---------------------+                      +----------+\\n```\\n\\nSo we have a producer that has something that it makes available to consumer\'s\\nvia a distribution server. The consumer uses this thing by downloading it from\\nthe distribution server in some manner. If the distribution server just allows\\nthe consumer to poll and download the thing/artifact, then it will be up to the\\nconsumer code to decide when it should poll to check for updates and update if\\nneeded.\\n\\nFor example, lets say that a man in the middle (MITM) attack is put in place\\nand the consumer is no longer talking to the distribution server but instead\\nto server controlled by an attacker which provides the consumer with a malicious\\nartifacts:\\n\\n```\\n  +-----------+     +---------------------+     +----------+     +----------+\\n  | Producer  |     | Distribution server |     | MITM     |     | Consumer |\\n  |-----------+     |---------------------+     |          |     |----------|\\n  | thing_v1  | --\x3e | thing_v1            |     | evil_v1  | --\x3e | thing_v1 |\\n  +-----------+     +---------------------+     +----------+     +----------+\\n```\\n\\nAn attacker may also target the distribution server itself and modify the\\nartifacts that the consumers download:\\n\\n```\\n  +-----------+     +---------------------+                      +----------+\\n  | Producer  |     | Distribution server |                      | Consumer |\\n  |-----------+     |---------------------+                      |----------|\\n  | thing_v1  | --\x3e | thing_v1 (evil_v1)  | ------------------\x3e  | thing_v1 |\\n  +-----------+     +---------------------+                      +----------+\\n```\\n\\nThis type of attack is refered to as `Arbitary software installation`.\\n\\nAnother attack against the distribution server is where the attacker prevents\\nthe consumer from getting updates, and instead provides the consumer with an\\nolder version which might contain a known vulnerabilty.\\n\\nThis would be the current state where the correct/latest version is being used:\\n\\n```\\n  +-----------+     +---------------------+                      +----------+\\n  | Producer  |     | Distribution server |                      | Consumer |\\n  |-----------+     |---------------------+                      |----------|\\n  | thing_v3  | --\x3e | thing_v3            | ------------------\x3e  | thing_v3 |\\n  +-----------+     +---------------------+                      +----------+\\n```\\n\\nThe attacker then changes the version on the distribution server to an older\\nversion, causing the consumer to downgrade, or rollback to that version:\\n\\n```\\n  +-----------+     +---------------------+                      +----------+\\n  | Producer  |     | Distribution server |                      | Consumer |\\n  |-----------+     |---------------------+                      |----------|\\n  | thing_v3  | --\x3e | thing_v2 (evil_v2)  | ------------------\x3e  | thing_v2 |\\n  +-----------+     +---------------------+                      +----------+\\n```\\n\\nThis type of attack is refered to as `Rollback attack`.\\n\\nThere are other [attacks](https://theupdateframework.io/security/) but these\\nI found helped me better understand the metadata that is provided by TUF.\\n\\nA simplified overview of this can be seen below and I\'m going into more\\ndetails later in the document.\\n\\nWhat I\'d like to convey with this is that the producer will update the TUF\\nrepository by creating `metadata` about the artifact(s) that are going to be\\nmade available. This metadata is signed by one or more keys.\\nThe motivation for signing is that we want to prevent the situation above where\\nan attacker is able to replace an artifact with an older version, or a modified\\nversion. Having the metadata signed for each version means that it would not be\\npossible for an attacker to do this as the TUF client framework will verify\\nsignatures.\\n\\nIn TUF, the \\"distribution server\\" in the above scenarios will have a TUF\\nrepository integrated into it. This repository will be updated by the producer,\\nand the consumer in the above scenarios will be replaced by a TUF client:\\n\\n```\\n  +-----------+         +---------------------+               +------------------+\\n  | Producer  |         | TUF Repository      | <-----------  | TUF Client       |\\n  |-----------+ update  |---------------------+               |------------------|\\n  | thing_v1  | ------\x3e | Metadata (signed)   | ------------\x3e | Metadata (signed)|\\n  +-----------+         +---------------------+               +------------------+\\n                                                ------------\x3e | thing_v1         |\\n                                                              +------------------+\\n```\\n\\nNotice that in addition to the metadata that exists on the TUF repository there\\nis also metadata on the TUF consumer/client side. This metadata is downloaded\\nand frequently resigned, and it has an short expiration date. This is how the\\nTUF framework enforces updates actually take place. Because the TUF client\\nframework checks the expiration and the signature of the metadata file, it\\ncan detect if the expiration date has passed. If there has not been any updates\\nand the expiration date has passed, perhaps a certain number of times, it can\\ntake action to notify the client side software about this situation. This is how\\nTUF can enforce that updates actually get applied to the consumer.\\n\\nThis metadata is also signed as we don\'t want an attacker to be able to serve\\nthe client with a metadata file they crafted themselves, which might have\\nenabled the attacker to trick the consumer into thinking that there are no\\nupdates and force the consumer to be stuck on an older version.\\n\\nHopefully this has provided an overview and some idea about the metadata and\\nthe signing in TUF. Later we will see a concrete example of the metadata files\\nto get \\"feel\\" for what they look like.\\n\\nSo, we have mentioned metadata files and signing. The following is an attempt\\nto visualize where the metadata files exist.\\n\\nInitially, we would have the following metadata files:\\n\\n```\\n  +-----------+         +---------------------+               +---------------+\\n  | Producer  |         | TUF Repository      |               | TUF Consumer  |\\n  |-----------+ update  |---------------------+               |---------------|\\n  | Keys      | ------\x3e | Metadata            |               | Metadata      |\\n  +-----------+         | 1.root.json         |               | root.json     |\\n  | Metadata  |         | 1.targets.json      |               +---------------+\\n  | root.json |         | 1.snapshot.json     |\\n  +-----------+         | timestamp.json      |\\n  | thing_v1  |         +---------------------+\\n  +-----------+         | thing_v1            |\\n                        +---------------------+\\n```\\n\\nInitially, before the client has interacted with the TUF repository, the client\\nhas a trusted root.json metadata file. This file is shipped with the client and\\nit does not matter if it\'s expire date has passed, as it will get updated once\\nthe client interacts with the TUF reporitory which is part of the client\\n[workflow](https://theupdateframework.github.io/specification/latest/index.html#detailed-client-workflow)\\nof TUF\'s specification.\\n\\nThe client starts by [loading](https://theupdateframework.github.io/specification/latest/index.html#load-trusted-root) this trusted root file.\\n\\nNext, client proceeds to the [download](https://theupdateframework.github.io/specification/latest/index.html#update-root) \\\\<version\\\\>.root.json files from the TUF repository until it has\\nreached the latest:\\n\\n```\\n  +-----------+         +---------------------+               +---------------+\\n  | Producer  |         | TUF Repository      | <-----------  | TUF Consumer  |\\n  |-----------+ update  |---------------------+               |---------------|\\n  | Keys      | ------\x3e | Metadata            | ------------\x3e | Metadata      |\\n  +-----------+         | 1.root.json         |               | root.json     |\\n  | Metadata  |         | 1.targets.json      |               +---------------+\\n  | root.json |         | 1.snapshot.json     |\\n  +-----------+         | timestamp.json      |\\n  | thing_v1  |         +---------------------+\\n  +-----------+         | thing_v1            |\\n                        +---------------------+\\n```\\n\\nNotice that the root.json is written on the client side without the version\\nprefix. This will be used to verify the files that are download later.\\n\\nNext, the client will [download](https://theupdateframework.github.io/specification/latest/index.html#update-timestamp) the timestamp metadata file:\\n\\n```\\n  +-----------+         +---------------------+               +---------------+\\n  | Producer  |         | TUF Repository      | <-----------  | TUF Consumer  |\\n  |-----------+ update  |---------------------+               |---------------|\\n  | Keys      | ------\x3e | Metadata            | ------------\x3e | Metadata      |\\n  +-----------+         | 1.root.json         |               | root.json     |\\n  | Metadata  |         | 1.targets.json      |               | timestamp.json|\\n  | root.json |         | 1.snapshot.json     |               +---------------+\\n  +-----------+         | timestamp.json      |\\n  | thing_v1  |         +---------------------+\\n  +-----------+         | thing_v1            |\\n                        +---------------------+\\n```\\n\\nAnd there will number of verifications performed on timestamp.json.\\n\\nNext, the client will [download](https://theupdateframework.github.io/specification/latest/index.html#update-snapshot) the snapshot metadata file:\\n\\n```\\n  +-----------+         +---------------------+               +---------------+\\n  | Producer  |         | TUF Repository      | <-----------  | TUF Consumer  |\\n  |-----------+ update  |---------------------+               |---------------|\\n  | Keys      | ------\x3e | Metadata            | ------------\x3e | Metadata      |\\n  +-----------+         | 1.root.json         |               | root.json     |\\n  | Metadata  |         | 1.targets.json      |               | timestamp.json|\\n  | root.json |         | 1.snapshot.json     |               | snapshot.json |\\n  +-----------+         | timestamp.json      |               +---------------+\\n  | thing_v1  |         +---------------------+\\n  +-----------+         | thing_v1            |\\n                        +---------------------+\\n```\\n\\nAnd there will number of verifications performed on snapshot.json.\\n\\nNext, the client will [download](https://theupdateframework.github.io/specification/latest/index.html#update-targets) the targets.json metadata file:\\n\\n```\\n  +-----------+         +---------------------+               +---------------+\\n  | Producer  |         | TUF Repository      | <-----------  | TUF Consumer  |\\n  |-----------+ update  |---------------------+               |---------------|\\n  | Keys      | ------\x3e | Metadata            | ------------\x3e | Metadata      |\\n  +-----------+         | 1.root.json         |               | root.json     |\\n  | Metadata  |         | 1.targets.json      |               | timestamp.json|\\n  | root.json |         | 1.snapshot.json     |               | snapshot.json |\\n  +-----------+         | timestamp.json      |               | targets.json  |\\n  | thing_v1  |         +---------------------+               +---------------+\\n  +-----------+         | thing_v1            |\\n                        +---------------------+\\n```\\n\\nAnd there will number of verifications performed on targets.json.\\n\\nFinally, the client will [fetch](https://theupdateframework.github.io/specification/latest/index.html#fetch-target) the actual target files, these are the actual artifacts.\\n\\nThe names of the metadata files are named after the four top level roles in TUF:\\n\\n- `Root` Delegates trust to specific keys for all the other top level roles\\n- `Target` Signs metadata for the target files\\n- `Snapshot` Signs metadata about the latest version of targets metadata\\n- `Timestamp` Signs metadata about the latest version of the snapshot metadata\\n\\nThe following sections will take a closer look at these metadata files.\\n\\n#### Root Metadata\\n\\nInstead of having a single root key, there will often be multiple root keys\\nwhich are stored in different offline locations, meaning that they are not\\naccessible remotely. These are often hardware keys, like Yubikeys.\\n\\nRoot keys are often used together to sign other keys. These non-root keys can\\nthen be re-signed/revoked/rotated if/when needed.\\n\\nEach role has metadata associated with it, and the specification defines a\\ncanonical json format for them. So there would be a root.json, a targets.json,\\na, timestamps.json, and a snapshot.json.\\n\\nSo what do these file look like?\\n\\nLets try this out by using a tool called\\n[tuftool](https://github.com/awslabs/tough/tree/develop/tuftool):\\n\\n```console\\n$ cargo install --force tuftool\\n```\\n\\n#### Root metadata\\n\\nNext we initiate a new `root.json` file using the following command:\\n\\n```console\\n$ tuftool root init root/root.json\\n```\\n\\nThis command will generate a file named `root/root.json`:\\n\\n```console\\n$ cat root/root.json\\n{\\n  \\"signed\\": {\\n    \\"_type\\": \\"root\\",\\n    \\"spec_version\\": \\"1.0.0\\",\\n    \\"consistent_snapshot\\": true,\\n    \\"version\\": 1,\\n    \\"expires\\": \\"2023-01-17T07:48:23Z\\",\\n    \\"keys\\": {},\\n    \\"roles\\": {\\n      \\"timestamp\\": {\\n        \\"keyids\\": [],\\n        \\"threshold\\": 1507\\n      },\\n      \\"root\\": {\\n        \\"keyids\\": [],\\n        \\"threshold\\": 1507\\n      },\\n      \\"snapshot\\": {\\n        \\"keyids\\": [],\\n        \\"threshold\\": 1507\\n      },\\n      \\"targets\\": {\\n        \\"keyids\\": [],\\n        \\"threshold\\": 1507\\n      }\\n    }\\n  },\\n  \\"signatures\\": []\\n}\\n```\\n\\nThese are just the default values, and we can see that there are no root keys,\\nthat field is just an empty object, and notice that the roles are mainly empty\\napart from the `threshold` values which is 1507. The threshold value specifies\\nthe minimum number of keys required to sign that roles metadata. 1507 is a large\\nnumber of keys and we can change this to just requiring one key:\\n\\n```console\\n$ tuftool root set-threshold root/root.json snapshot 1\\n$ tuftool root set-threshold root/root.json root 1\\n$ tuftool root set-threshold root/root.json timestamp 1\\n$ tuftool root set-threshold root/root.json targets 1\\n```\\n\\nAnd we can see that `root/root.json` has been updated:\\n\\n```\\n$ cat root/root.json\\n{\\n  \\"signed\\": {\\n    \\"_type\\": \\"root\\",\\n    \\"spec_version\\": \\"1.0.0\\",\\n    \\"consistent_snapshot\\": true,\\n    \\"version\\": 1,\\n    \\"expires\\": \\"2023-02-27T14:05:04Z\\",\\n    \\"keys\\": {},\\n    \\"roles\\": {\\n      \\"root\\": {\\n        \\"keyids\\": [],\\n        \\"threshold\\": 1\\n      },\\n      \\"targets\\": {\\n        \\"keyids\\": [],\\n        \\"threshold\\": 1\\n      },\\n      \\"snapshot\\": {\\n        \\"keyids\\": [],\\n        \\"threshold\\": 1\\n      },\\n      \\"timestamp\\": {\\n        \\"keyids\\": [],\\n        \\"threshold\\": 1\\n      }\\n    }\\n  },\\n  \\"signatures\\": []\\n}\\n```\\n\\nWe can also set the expire time for the root using the following command:\\n\\n```console\\n$ tuftool root expire root/root.json \'in 6 weeks\'\\n```\\n\\nNow, to sign anything we will need a private key to create the signatures and\\na matching public key to be used to verify signatures.\\n\\nWe can generate a root key and one can be generated using\\n`tuftool root gen-rsa-key`:\\n\\n```console\\n$ tuftool root gen-rsa-key root/root.json ./keys/root.pem --role root\\n6e99ec437323f2c7334c8b16fd7a7a197829ba89ff50d07aa4b50fc9634dad9f\\n```\\n\\nThis will generate keys/root.pem which is a private key in pkcs8 format. The\\nhex value printed above is the `key_id` which will be used later to reference\\nthis key in metadata files.\\n\\nNow, if we again inspect `root.json` we find that a key has been added:\\n\\n```console\\n$ cat root/root.json\\n{\\n  \\"signed\\": {\\n    \\"_type\\": \\"root\\",\\n    \\"spec_version\\": \\"1.0.0\\",\\n    \\"consistent_snapshot\\": true,\\n    \\"version\\": 1,\\n    \\"expires\\": \\"2023-02-28T08:30:48Z\\",\\n    \\"keys\\": {\\n      \\"6e99ec437323f2c7334c8b16fd7a7a197829ba89ff50d07aa4b50fc9634dad9f\\": {\\n        \\"keytype\\": \\"rsa\\",\\n        \\"keyval\\": {\\n          \\"public\\": \\"-----BEGIN PUBLIC KEY-----\\\\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5ZiWzak3CBJkRrCfw5GO\\\\nSUtYjIK2jLozyaZ44FePW/KYEhM8LyHcNz9lwx45tZ8gId4AsxGBj9fhsOgjpN7l\\\\nMPXpaKsV/5f37HzQLCrbldz3ei9LkMWG5La4Cwil0qPDpTxfzI7IWDKk6l4/epgi\\\\nOrAJDaQ/mKhH5OZ485JYuDIE7a0jplU/GvsNeCdZVMEQ8dko/CA4Di8lPkDRRdSw\\\\naC/8g3K6mF+87ADdGOmZ+LFodLEPvqIVljece2JlX2z44Io3N7Y5FH63Az4H3MFL\\\\nDPZJH5lFs7Lb/fHx25rWSE2/GHcUUTs4oScPp2X0hAnblOsmCFSCjf8Kb0R7dLUb\\\\nnQIDAQAB\\\\n-----END PUBLIC KEY-----\\"\\n        },\\n        \\"scheme\\": \\"rsassa-pss-sha256\\"\\n      }\\n    },\\n    \\"roles\\": {\\n      \\"timestamp\\": {\\n        \\"keyids\\": [],\\n        \\"threshold\\": 1\\n      },\\n      \\"targets\\": {\\n        \\"keyids\\": [],\\n        \\"threshold\\": 1\\n      },\\n      \\"root\\": {\\n        \\"keyids\\": [\\n          \\"6e99ec437323f2c7334c8b16fd7a7a197829ba89ff50d07aa4b50fc9634dad9f\\"\\n        ],\\n        \\"threshold\\": 1\\n      },\\n      \\"snapshot\\": {\\n        \\"keyids\\": [],\\n        \\"threshold\\": 1\\n      }\\n    }\\n  },\\n  \\"signatures\\": []\\n}\\n```\\n\\nNotice that the `keys` object has a field which is named after the `key_id` and\\nthat the `root` role has this `key_id` in its `keyids` array. This `key_id` is\\ncreated from the json value of the fields `keytype`, `keyval`, and `scheme`,\\nwhich is then canonicalized before hashed using sha256. We can inspect/verify\\nthis using a tool named [tuf-keyid](https://github.com/danbev/tuf-keyid), and\\npassing in the above json field of the public key:\\n\\n```console\\n$ tuf-keyid --json=\'{\\n            \\"keytype\\": \\"rsa\\",\\n            \\"keyval\\": {\\n              \\"public\\": \\"-----BEGIN PUBLIC KEY-----\\\\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5ZiWzak3CBJkRrCfw5GO\\\\nSUtYjIK2jLozyaZ44FePW/KYEhM8LyHcNz9lwx45tZ8gId4AsxGBj9fhsOgjpN7l\\\\nMPXpaKsV/5f37HzQLCrbldz3ei9LkMWG5La4Cwil0qPDpTxfzI7IWDKk6l4/epgi\\\\nOrAJDaQ/mKhH5OZ485JYuDIE7a0jplU/GvsNeCdZVMEQ8dko/CA4Di8lPkDRRdSw\\\\naC/8g3K6mF+87ADdGOmZ+LFodLEPvqIVljece2JlX2z44Io3N7Y5FH63Az4H3MFL\\\\nDPZJH5lFs7Lb/fHx25rWSE2/GHcUUTs4oScPp2X0hAnblOsmCFSCjf8Kb0R7dLUb\\\\nnQIDAQAB\\\\n-----END PUBLIC KEY-----\\"\\n            },\\n            \\"scheme\\": \\"rsassa-pss-sha256\\"\\n    }\'\\n\\nkey_id: SHA256:6e99ec437323f2c7334c8b16fd7a7a197829ba89ff50d07aa4b50fc9634dad9f\\n```\\n\\nAnd we can see that the `key_id`\'s produced are the same. This may be obvious\\nbut just to be clear, `keys` only includes the public key.\\n\\n#### Targets metadata\\n\\nThe Target role is a role that signs metadata files that describe the\\nproject artifacts, like software packages, source code, or whatever artifacts\\nthat are to be consumed by TUF clients/consumers.\\n\\nSo lets add a target key, and we will use the same private key as before:\\n\\n```console\\n$ tuftool root add-key root/root.json ./keys/root.pem --role targets\\n6e99ec437323f2c7334c8b16fd7a7a197829ba89ff50d07aa4b50fc9634dad9f\\n```\\n\\nThis will update the `targets` role in `root.json`:\\n\\n```console\\n$ jq \'.signed.roles.targets\' < root/root.json\\n{\\n  \\"keyids\\": [\\n    \\"6e99ec437323f2c7334c8b16fd7a7a197829ba89ff50d07aa4b50fc9634dad9f\\"\\n  ],\\n  \\"threshold\\": 1\\n}\\n```\\n\\n#### Snapshot metadata\\n\\nThe Snapshot roles signs a metadata file which contains information about the\\nlatest version of the targets metadata. This is used to identify which versions\\nof a target are in a repository at a certain time. This is used to know if there\\nis an update available (remember it\'s call The Update Framework).\\n\\nSo, lets add a key to the snapshot role:\\n\\n```console\\n$ tuftool root add-key root/root.json ./keys/root.pem --role snapshot\\n```\\n\\nAnd we can see that following change to `root.json`:\\n\\n```console\\n$ jq \'.signed.roles.snapshot\' < root/root.json\\n{\\n  \\"keyids\\": [\\n    \\"6e99ec437323f2c7334c8b16fd7a7a197829ba89ff50d07aa4b50fc9634dad9f\\"\\n  ],\\n  \\"threshold\\": 1\\n}\\n```\\n\\n#### Timestamp metadata\\n\\nFinally, we have the timestamp role which tells if there is an update.\\n\\n```console\\n$ tuftool root add-key root/root.json ./keys/root.pem --role timestamp\\n```\\n\\nAnd we can see that following change to `root.json`:\\n\\n```console\\n$ jq \'.signed.roles.timestamp\' < root/root.json\\n{\\n  \\"keyids\\": [\\n    \\"6e99ec437323f2c7334c8b16fd7a7a197829ba89ff50d07aa4b50fc9634dad9f\\"\\n  ],\\n  \\"threshold\\": 1\\n}\\n```\\n\\n#### Signing root.json\\n\\nSo we have configured which keys to be used for each of the roles but we have\\nnot signed this metadata file. We need to sign it to prevent tampering of it\\nas it will be sent to the TUF repository, usually on server but for this\\nexample everything will be on the local file system.\\n\\nWe can now sign root.json using the following command:\\n\\n```console\\n$ tuftool root sign ./root/root.json -k ./keys/root.pem\\n```\\n\\nAnd we can check `root.json` that the signatures field has been updated with\\na keyid and a signature.\\n\\n```console\\n jq \'.signatures\' < root/root.json\\n[\\n  {\\n    \\"keyid\\": \\"6e99ec437323f2c7334c8b16fd7a7a197829ba89ff50d07aa4b50fc9634dad9f\\",\\n    \\"sig\\": \\"8e7c7ca88e242ff360af30ba83e0ccfd9de2a9dee774166abe508ad2757620d6439bc7a5163c55867e069812a21ba31b7097d9ded3590f03f8bdc7106755a9ae840efbfe9b6c7d69e047230c59f3bd682e83f0b5b9c271d6db60943f7fa57d565790de58687560b50951a363725471c3a8f64c3980385eb214876bb1fe87d4aefc5cdd557bd022ddd794a52368f8502c1944185c75827ca97bba8fd5cdd5bb41b7ad76f0105072fbee980d3dbbf9889ec223ea1399228560fd747bc03a378d3ba93990560b000d02a59aab04844ec70662f8baaee33f8591f5bbe3126fb057f9b3055d498005220d1715c92166506995e89f2e8e62d1032452d51ba6579eb0e2\\"\\n  }\\n]\\n```\\n\\n#### Generate the TUF repository\\n\\nWith `root.json` and the private key we can generate a tuf repository using the\\nfollowing command:\\n\\n```console\\n$ tuftool create \\\\\\n  --root root/root.json \\\\\\n  --key keys/root.pem \\\\\\n  --add-targets artifacts \\\\\\n  --targets-expires \'in 3 weeks\' \\\\\\n  --targets-version 1 \\\\\\n  --snapshot-expires \'in 3 weeks\' \\\\\\n  --snapshot-version 1 \\\\\\n  --timestamp-expires \'in 1 week\' \\\\\\n  --timestamp-version 1 \\\\\\n  --outdir repo\\n```\\n\\nThat command will create a directory named `repo` which contains two\\ndirectories, `metadata` and `targets`.\\n\\nLet start by looking at the `targets` directory:\\n\\n```console\\n$ ls -l repo/targets/01ab0faaf41a4543df1fa218b8e9f283d07536339cf11d2afae9d116a257700c.artifact_1.txt\\nlrwxrwxrwx. 1 danielbevenius danielbevenius 79 Jan 17 12:50 repo/targets/01ab0faaf41a4543df1fa218b8e9f283d07536339cf11d2afae9d116a257700c.artifact_1.txt -> artifacts/artifact_1.txt\\n```\\n\\nNotice that the name of this link is the sha256sum of the contents of\\nartifact_1.txt file:\\n\\n```console\\n$ sha256sum  artifacts/artifact_1.txt\\n01ab0faaf41a4543df1fa218b8e9f283d07536339cf11d2afae9d116a257700c  artifacts/artifact_1.txt\\n```\\n\\nAnd we can check the size of this file using:\\n\\n```console\\n$ stat -c \\"%s\\" artifact_1.txt\\n24\\n```\\n\\nThe reason for showing this values is that they are referred to in the next\\nsection.\\n\\nNow, lets take a look at the `metadata` directory.\\n\\n```console\\n$ ls repo/metadata/\\n1.root.json  1.snapshot.json  1.targets.json  timestamp.json\\n```\\n\\nThe number prefix is the version, to 1.targets.json is for version 1 for\\nexample.\\n\\nLets start by looking at `targets.json`:\\n\\n```json\\n{\\n  \\"signed\\": {\\n    \\"_type\\": \\"targets\\",\\n    \\"spec_version\\": \\"1.0.0\\",\\n    \\"version\\": 1,\\n    \\"expires\\": \\"2023-02-07T11:50:00.608598188Z\\",\\n    \\"targets\\": {\\n      \\"artifact_1.txt\\": {\\n        \\"length\\": 24,\\n        \\"hashes\\": {\\n          \\"sha256\\": \\"01ab0faaf41a4543df1fa218b8e9f283d07536339cf11d2afae9d116a257700c\\"\\n        }\\n      }\\n    },\\n    \\"delegations\\": {\\n      \\"keys\\": {},\\n      \\"roles\\": []\\n    }\\n  },\\n  \\"signatures\\": [\\n    {\\n      \\"keyid\\": \\"6e99ec437323f2c7334c8b16fd7a7a197829ba89ff50d07aa4b50fc9634dad9f\\",\\n      \\"sig\\": \\"48b9810f275e16acb2d093c5487da4107f2312e0c9e084f6974aa836661a0e87d341be37fe84a4afd6ba82e8e54301e01a7431a0e69d3bef95ce3e34d90badff3c4a19ed7a6cea2a4ec69c6dc7392fde1f20b1246f3113ace85a223bfc54203a9254e82c8cd9686b8b973bfc41cdda657ff707a41c3db125b61dfc41c8937896f7fcc0ea17429a934b9c0fee912ca4df4a3b1dac6811968aa34bbf2d3327bbeab9cad1dadc1f8134c0add4267bf8ff285c066d24ea39b24d9bca197bf9762025133205612d41b167ee1232adf8c122320d77b70b936817ddf2cc93732228f772078b663f3fc896ec8873873414ba44fd3e28772589f69af06ee3e1297e0b2b37\\"\\n    }\\n  ]\\n}\\n```\\n\\nNotice that the `hashes` object has a single field which is the sha256sum of\\nthe file `artifacts_1.txt`, and that `length` matches the size which we showed\\nin the previous section.\\n\\nWe have information about the targets, in this case on a single file named\\nartifact_1.txt, and this is the file that a client wants to consume. This\\nmetadata file is signed and the signature in the `sig` field of the\\n`signatures` array.\\n\\nAgain, this needs to be signed to prevent an attacker from modifying the targets\\nand modifying the `length`, and `sha256` fields which would otherwise allow them\\nto replace the target artifact with a potentially malicious version.\\n\\nNext, lets take a look at the `snapshots.json` metadata file:\\n\\n```console\\n$ cat repo/metadata/1.snapshot.json\\n{\\n  \\"signed\\": {\\n    \\"_type\\": \\"snapshot\\",\\n    \\"spec_version\\": \\"1.0.0\\",\\n    \\"version\\": 1,\\n    \\"expires\\": \\"2023-02-07T11:50:00.608597347Z\\",\\n    \\"meta\\": {\\n      \\"targets.json\\": {\\n        \\"length\\": 1048,\\n        \\"hashes\\": {\\n          \\"sha256\\": \\"896781ff1260ed4ad5b05a004b034279219dc92b64068a2cc376604e8a6821c9\\"\\n        },\\n        \\"version\\": 1\\n      }\\n    }\\n  },\\n  \\"signatures\\": [\\n    {\\n      \\"keyid\\": \\"6e99ec437323f2c7334c8b16fd7a7a197829ba89ff50d07aa4b50fc9634dad9f\\",\\n      \\"sig\\": \\"5a8d9597329183c52547591d1abc8e36c1535d81c0e51ed51d95d2ddf1ec2076f2412ba8e631f039c7bf9e5a14cdd44eb7a5c7dae5dcc84e6aa2ebd51049ee791cf3c3dc486af26731fc06ba39e449ef85b102247c4254cb48784e4a95b54943df9e668470a6def79c7c3d532a68e93d18f1d59f1636455dddec0b5960afeb5a9ac38c38c6891e6f819f22aed7996a7f9964d655d634a940e1234f2015caa8f4f710570443bc0bc3ec04117c3dc97c8d564f42489cc499593f6232b7f5062646644aecafaf50dc9a4005a000f6720b0b9c455e5b92d7a1bcfb96f14a6a9da162e9b091497b0eb24283a837ba1d15ff67f12d104b1c5e1d83c36ae49400bb326e\\"\\n    }\\n  ]\\n}\\n```\\n\\nSo looking at the above `meta` field, we can see that there is a `targets.json`\\n\\"meta path\\". If we search for this file we won\'t be able to find it. The actual\\nfile is prefixed with the version from the `version` field. So the file in\\nquestion is `repo/metadata/1.targets.json`, and we can check the size of this\\nfile using:\\n\\n```console\\n$ stat -c \\"%s\\" repo/metadata/1.targets.json\\n1048\\n```\\n\\nAnd that matches the `length` field above.\\n\\nAnd we also can check the hash using:\\n\\n```console\\n$ sha256sum repo/metadata/1.targets.json\\n896781ff1260ed4ad5b05a004b034279219dc92b64068a2cc376604e8a6821c9  repo/metadata/1.targets.json\\n```\\n\\nThe snapshot metadata file tells us which versions of metadata files\\nexisted for a specific version. Even though we only have one version currently\\nin our example, a real world repositories would have multiple versions of\\nmetadata files. The purpose of the snapshot.json file is to prevent an attacker\\nfrom including metadata files from another version in some version:\\n\\n```\\n       TUF Repository              TUF Client\\n     +----------------+           +----------------+\\n     | timestamp.json | --------\x3e | timestamp.json |\\n     | 1.targets.json | version 3 | - version 3    |\\n     | 2.targets.json | <-------  |                |\\n     | 3.targets.json | version 2 |                |\\n     |                | --------\x3e |                |\\n     +----------------+           +----------------+kk\\n```\\n\\nThe above is trying to show that the client has received a timestamp.json,\\nwhich we will discuss shortly, with a version number of 3. The client proceeds\\nto retrieve this version update from the TUF server. Now if an attacker was able\\nto include other metadata files, which are available in the TUF repository only\\nthey are not part of the request version, it would be possible for them those\\ntargets to be sent to the client.Having the `snapshot.json` prevents this as\\nit specifies which metadata files are included in a specific version and no\\nother additional metadata files that may exist in the TUF repository are\\nincluded.\\n\\nSo that leaves us with `timestamp.json`. This is a file that is downloaded by\\nthe client and usually has a short expiration date. As mentioned before this is\\nthe part that allows the system to enforce that updates are actually reaching\\nconsumers, and if they are not they allow the consumer to take action.\\n\\nThe metadata looks like this:\\n\\n```console\\n$ cat repo/metadata/timestamp.json\\n{\\n  \\"signed\\": {\\n    \\"_type\\": \\"timestamp\\",\\n    \\"spec_version\\": \\"1.0.0\\",\\n    \\"version\\": 1,\\n    \\"expires\\": \\"2023-01-24T11:50:00.608598985Z\\",\\n    \\"meta\\": {\\n      \\"snapshot.json\\": {\\n        \\"length\\": 1004,\\n        \\"hashes\\": {\\n          \\"sha256\\": \\"b92c443c21b6bc15d4f3991491e8bcb201f66a26ab289fb8cc9af7f851530872\\"\\n        },\\n        \\"version\\": 1\\n      }\\n    }\\n  },\\n  \\"signatures\\": [\\n    {\\n      \\"keyid\\": \\"6e99ec437323f2c7334c8b16fd7a7a197829ba89ff50d07aa4b50fc9634dad9f\\",\\n      \\"sig\\": \\"3708f055fe58b2c70e92cbc46bd9cc0f3149900bf25b3e924ff666eb8b45187df8d1f064b249cd790c170b5e97b322866d298d527ff950d2fbcbf508097868afca34ebaa159890799155c6bb615bab9a8bcfc34a39574584716d9a89b531fce97d876884fad2db69dc8f3569870dee280e87c9d506b5b08698e7c23e7dbbf4a3209fbc91ec764b54bf87367145cbb7bc9d7edaf47f709355284315fac9167312833d990e9e064852bb4fa905ec4edb5fe051480e70d505694528c5e9b47fefc3b78f6e54623f93344511326bdeec392a5eac31e7299bf9f602036d9f9524810eb03c4720370250f3e9f503e8d5bee94a6e6539ca9f988a27272d47612fd03436\\"\\n    }\\n  ]\\n}\\n```\\n\\nNotice that `snapshot.json` is refering to the file `1.timestamp.json` in the\\nTUF repository.\\n\\nFinally, we have repo/metadata/1.root.json which is identical to root/root.json\\nwhich we saw previously as this is the only version we have in our repository.\\n\\nWhat we have been doing is setting up a repository which would be most often\\nexist on on a server somewhere.\\n\\n### TUF client/consumer\\n\\nA consumer of the artifact would use a TUF client library to download the\\nartifacts, and would specify the metadata and targets to download.\\n\\nTo make this more concrete we have created a very basic [example](https://github.com/danbev/tuf-client#readme)\\nwhich is intended to show how one such client library, in this case\\n[tough](https://crates.io/crates/tough), might be used. Please refer to the\\nexample\'s README.md for details.\\n\\n### TUF usage in Sigstore\\n\\nArmed with the above knowledge, lets take a look at how Sigstore uses TUF.\\n\\nThe [root-signing](https://github.com/sigstore/root-signing/tree/main/repository)\\nrepository has a directory which contains the TUF repository metadata.\\n\\nSigstore uses TUF to protect their public keys and certificates, which was one\\nreason for trying to say \\"things\\" instead of software updates in the text above.\\nSo the artifacts that are updated are public keys and certificates. As of this\\nwriting these are the current [targets](https://github.com/sigstore/root-signing/blob/0ce4aa6c45c3ee709766d90e34c6b1372ad4b29a/repository/repository/targets.json):\\n\\n```\\nfulcio.crt.pem  Fulico (CA) certificate.\\nrekor.pub       Rekor public key.\\nctfe.pub        Certificate Transparency log public key used to verify\\n                signed certificate timestamps.\\nartifact.pub    Public key which is used to verify Sigstore releases, like\\n                Cosign, Rekor, and Fulcio releases.\\n```\\n\\nThere are more than four in the actual file but they are different versions. The\\nfour listed here just explain the usage of these keys. In the case of\\nsigstore-rs only the Fulio certificate, and Rekor\'s public key are used. We will\\nfocus on these in this post.\\n\\nThe [root.json](https://github.com/sigstore/root-signing/blob/main/repository/repository/root.json)\\ncan also be found in the same directory and hopefully this should look familar\\nafter seeing the root.json earlier.\\n\\nWe also learned earlier that the initial trusted `root.json` is shipped with the\\nsoftware in some manner. In sigstore-rs, `root.json` is a constant named\\n[SIGSTORE_ROOT](https://github.com/sigstore/sigstore-rs/blob/8d22a6d23a6771688c8206850524a2b1076bbdb0/src/tuf/constants.rs#L30-L173) in the crate itself.\\n\\nsigstore-rs uses the `tough` crate just like the example we saw earlier, and\\nsimilar to the [example](https://github.com/danbev/tuf-client/blob/3adca52130c69f242ef24c5845c91ee1612fc64c/src/main.rs#L61), creates a tough [RepositoryLoader](https://github.com/sigstore/sigstore-rs/blob/cef673776548c9b268e0ce8ecc3a4fe2da504658/src/tuf/repository_helper.rs#L43) in a RepositoryHelper\\nstruct. A RepositoryHelper is created in SigstoreRepository\'s\\n[fetch](https://github.com/sigstore/sigstore-rs/blob/cef673776548c9b268e0ce8ecc3a4fe2da504658/src/tuf/mod.rs#L123) method, and the `Fulcio certificate`, and `Rekor\'s public key` are read from\\nthe repository, similar to how `artifact.txt` was read in the example:\\n\\n```rust\\n    let repository_helper = RepositoryHelper::new(\\n        SIGSTORE_ROOT.as_bytes(),\\n        metadata_base,\\n        target_base,\\n        checkout_dir,\\n    )?;\\n\\n    let fulcio_certs = repository_helper.fulcio_certs()?;\\n\\n    let rekor_pub_key = repository_helper.rekor_pub_key().map(|data| {\\n        String::from_utf8(data).map_err(|e| {\\n            SigstoreError::UnexpectedError(format!(\\n                \\"Cannot parse Rekor\'s public key obtained from TUF repository: {}\\",\\n                e\\n            ))\\n        })\\n    })??;\\n```\\n\\nThere is a caching layer in-between the call to `fulcio_certs` but after\\nthat, and if there is a cache miss, the actual call the TUF repository can\\nbe found in [fetch_target](https://github.com/sigstore/sigstore-rs/blob/cef673776548c9b268e0ce8ecc3a4fe2da504658/src/tuf/repository_helper.rs#L158):\\n\\n```rust\\n/// Download a file from a TUF repository\\nfn fetch_target(repository: &tough::Repository, target_name: &TargetName) -> Result<Vec<u8>> {\\n    let data: Vec<u8>;\\n    match repository.read_target(target_name)? {\\n        None => Err(SigstoreError::TufTargetNotFoundError(\\n            target_name.raw().to_string(),\\n        )),\\n        Some(reader) => {\\n            data = read_to_end(reader)?;\\n            Ok(data)\\n        }\\n    }\\n}\\n```\\n\\nAnd this is simliar to how in the example above we [downloaded](https://github.com/danbev/tuf-client/blob/3adca52130c69f242ef24c5845c91ee1612fc64c/src/main.rs#L69-L71) `artifact.txt` like this:\\n\\n```rust\\n    let artifact = repository\\n        .read_target(&TargetName::from_str(\\"artifact.txt\\").unwrap())\\n        .unwrap();\\n```\\n\\nBy using the TUF framework for Fulcio\'s certificate, and Rekor\'s public keys\\nthese can be updated in a secure manner like described previously in this post."},{"id":"/2023/01/25/keys","metadata":{"permalink":"/blog/2023/01/25/keys","editUrl":"https://github.com/trustification/trustification.github.io/tree/main/blog/2023-01-25-keys.md","source":"@site/blog/2023-01-25-keys.md","title":"Is this a cryptographic key which I see before me?","description":"Yes, it is. Really? Then what format is it in and how can I tell?","date":"2023-01-25T00:00:00.000Z","formattedDate":"January 25, 2023","tags":[],"readingTime":3.565,"hasTruncateMarker":true,"authors":[{"name":"Daniel Bevenius","title":"Maintainer","url":"https://github.com/danbev","imageURL":"https://github.com/danbev.png","key":"danbev"}],"frontMatter":{"title":"Is this a cryptographic key which I see before me?","authors":"danbev","tags":[]},"prevItem":{"title":"The Update Framework (TUF)","permalink":"/blog/2023/01/31/tuf"},"nextItem":{"title":"Sigstore bundle format","permalink":"/blog/2023/01/13/sigstore-bundle-format"}},"content":"Yes, it is. Really? Then what format is it in and how can I tell?\\n\\nI\'ve found myself in this situation a number of times and this post tries to\\nprovide some guidelines for figuring out the type and format of keys without\\nhaving to go off and read some project\'s documentation.\\n\\n\x3c!--truncate--\x3e\\n\\nTo start off we can try to determine if the key is in a PEM format, or in\\nDER format.\\n\\nKeys in PEM format are in ascii and can be inspected from the\\ncommand line using `cat`, or opened in any text editor. For example:\\n\\n```console\\n$ cat pubkey.pem\\n-----BEGIN PUBLIC KEY-----\\nMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEDTvL0PRsxoxMXfSaXu+7w0ovVNzZ\\nk/BAIoz2GL2cPY3qZENU/+YrR92AuZFXn0jSmmvOktpAzGhnDhtidonkyA==\\n-----END PUBLIC KEY-----\\n```\\n\\nIf we try the same with DER format then we will get a bunch of strange\\ncharacters printed. For example:\\n\\n```console\\n$ cat pubkey.der\\n;\ufffd\ufffd\ufffdl\u018cL]\ufffd\ufffd^\ufffd\ufffdJ/T\ufffd\u0653\ufffd@\\"\ufffd\ufffd\ufffd\ufffd=\ufffd\ufffddCT\ufffd\ufffd+G\u0740\ufffd\ufffdW\ufffdH\u049ak\u0392\ufffd@\ufffdhgv\ufffd\ufffd\ufffd$\\n```\\n\\n## PEM formatted keys\\n\\nSo we have determined that the key we have in front of us is in PEM format.\\nNow, if we take a look at the PEM output above again:\\n\\n```console\\n-----BEGIN PUBLIC KEY-----\\nMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEDTvL0PRsxoxMXfSaXu+7w0ovVNzZ\\nk/BAIoz2GL2cPY3qZENU/+YrR92AuZFXn0jSmmvOktpAzGhnDhtidonkyA==\\n-----END PUBLIC KEY-----\\n```\\n\\nWe can see that it has a header and the footer. Notice that there is no\\ninformation about the type of public key that this file contains. This means\\nthat the information about the type of key in baked in there somewhere. So how\\ncan we find out what the type of the key?  \\nOne option is to use the `openssl asn1parse` command:\\n\\n```\\n$ openssl asn1parse -i  -in pubkey.pem\\n    0:d=0  hl=2 l=  89 cons: SEQUENCE\\n    2:d=1  hl=2 l=  19 cons:  SEQUENCE\\n    4:d=2  hl=2 l=   7 prim:   OBJECT            :id-ecPublicKey\\n   13:d=2  hl=2 l=   8 prim:   OBJECT            :prime256v1\\n   23:d=1  hl=2 l=  66 prim:  BIT STRING\\n```\\n\\nAnd we can see, that there is an id here which is `ecPublicKey`.\\n\\nAs a rule of thumb, if there is no key type in the PEM header, then\\nthe format of the key is most probably in Subject Public Key Info (SPKI) if it\\nis a public key, and in Public-Key Cryptography Standard 8 (pkcs8) format if it\\nis a private key.\\n\\nWith the knowledge that the key is an Elliptic Curve (EC) public key we can use\\nthe following openssl command to inspect it:\\n\\n```console\\n$ openssl ec -pubin -in pubkey.pem --text --noout\\nread EC key\\nPublic-Key: (256 bit)\\npub:\\n    04:0d:3b:cb:d0:f4:6c:c6:8c:4c:5d:f4:9a:5e:ef:\\n    bb:c3:4a:2f:54:dc:d9:93:f0:40:22:8c:f6:18:bd:\\n    9c:3d:8d:ea:64:43:54:ff:e6:2b:47:dd:80:b9:91:\\n    57:9f:48:d2:9a:6b:ce:92:da:40:cc:68:67:0e:1b:\\n    62:76:89:e4:c8\\nASN1 OID: prime256v1\\nNIST CURVE: P-256\\n```\\n\\nSome PEM keys can also be in a specific key format, in which case the type is\\nin the header of the pem, for example:\\n\\n```\\n-----BEGIN RSA PUBLIC KEY-----\\n...\\n-----END RSA PUBLIC KEY-----\\n```\\n\\nAnd if needed we can use the `openssl rsa` command to inspect them further.\\n\\nThe same reasoning can be applied to private keys as well with regards to the\\nPEM header/footer information, and in the case of private keys the `-pubin`\\nargument to the openssl commands should left out.\\n\\n## DER formatted keys\\n\\nAs mentioned before we can\'t just print DER files as they are in binary format,\\nbut we can still use `openssl asn1parse`:\\n\\n```console\\n$ openssl asn1parse -i -inform der  -in pubkey.der\\n    0:d=0  hl=2 l=  89 cons: SEQUENCE\\n    2:d=1  hl=2 l=  19 cons:  SEQUENCE\\n    4:d=2  hl=2 l=   7 prim:   OBJECT            :id-ecPublicKey\\n   13:d=2  hl=2 l=   8 prim:   OBJECT            :prime256v1\\n   23:d=1  hl=2 l=  66 prim:  BIT STRING\\n```\\n\\nAnd just like with the PEM example we can use other openssl tools to inspect the\\nkey.\\n\\n## When the guidelines fail\\n\\nThe above seems to work for most situations, but it can fail.\\n\\nOne example of this is when openssl cannot parse the key at all. I ran into this\\nrecently with [in-toto-rs](https://github.com/in-toto/in-toto-rs), which uses\\nthe Rust [ring](https://crates.io/crates/ring) crate to handle Ed25519 keys.\\n\\nThe issue here is that `ring` supports pkcs8 version 2\\n([RFC-5958](https://www.rfc-editor.org/rfc/rfc5958)), and OpenSSL currently only\\nsupports pkcs8 version 1 ([RFC-5208](https://www.rfc-editor.org/rfc/rfc5208)),\\nso the openssl tools will not be able to parse keys in the version 2 format.\\n\\nBelow is an example of trying to use a version 2 formatted Ed25519 key with\\nopenssl:\\n\\n```console\\n$ openssl pkey -inform der -in ed25519-1 -pubout\\nCould not read key from ed25519-1\\n```\\n\\nThere is an open [openssl issue](https://github.com/openssl/openssl/issues/10468)\\nfor this.\\n\\nHopefully there will not be many cases like this, and we hope that the\\nguidelines provided in this post are helpful."},{"id":"/2023/01/13/sigstore-bundle-format","metadata":{"permalink":"/blog/2023/01/13/sigstore-bundle-format","editUrl":"https://github.com/trustification/trustification.github.io/tree/main/blog/2023-01-13-sigstore-bundle-format.md","source":"@site/blog/2023-01-13-sigstore-bundle-format.md","title":"Sigstore bundle format","description":"This post takes a look at Sigstore\'s bundle format which is the format of","date":"2023-01-13T00:00:00.000Z","formattedDate":"January 13, 2023","tags":[{"label":"sigstore","permalink":"/blog/tags/sigstore"}],"readingTime":6.58,"hasTruncateMarker":true,"authors":[{"name":"Daniel Bevenius","title":"Maintainer","url":"https://github.com/danbev","imageURL":"https://github.com/danbev.png","key":"danbev"}],"frontMatter":{"title":"Sigstore bundle format","authors":"danbev","tags":["sigstore"]},"prevItem":{"title":"Is this a cryptographic key which I see before me?","permalink":"/blog/2023/01/25/keys"},"nextItem":{"title":"Sigstore, in-toto, OPA, orientation","permalink":"/blog/2023/01/11/sigstore-in-toto-opa"}},"content":"This post takes a look at Sigstore\'s bundle format which is the format of\\nSigstore\'s offline verification data.\\n\\nOffline verification is described like this in\\n[busting-5-sigstore-myths](https://www.chainguard.dev/unchained/busting-5-sigstore-myths):\\n\\n\x3c!--truncate--\x3e\\n\\n```\\nAnother common use case is that organizations need to run systems in air-gapped\\nenvironments with no outside network access. That means it\u2019s not possible to\\nlook up a signature in the transparency log, Rekor, right? Wrong! We use what\u2019s\\ncalled \\"stapled inclusion proofs\\" by default, meaning you can verify an object\\nis present in the transparency log without needing to contact the transparency\\nlog! The signer is responsible for gathering this evidence from the log and\\npresenting it alongside the artifact and signature. We store this in an OCI\\nimage automatically, but you can treat it like a normal file and copy it around\\nfor verification as well.\\n```\\n\\nSo, lets create a bundle and inspect the contents. First, we need to sign an\\nartifact and in this case we are going to use a simple text file:\\n\\n```console\\n$ echo \\"some data\\" > artifact.txt\\n```\\n\\nAs mentioned, Sigstore can create a `bundle`, which contains all the information\\nrequired for \\"stapled inclusion proofs\\". A bundle can be generated using the\\nfollowing command:\\n\\n```console\\n$ COSIGN_EXPERIMENTAL=1 cosign sign-blob --bundle=artifact.bundle artifact.txt\\nUsing payload from: artifact.txt\\nGenerating ephemeral keys...\\nRetrieving signed certificate...\\n\\n        Note that there may be personally identifiable information associated with this signed artifact.\\n        This may include the email address associated with the account with which you authenticate.\\n        This information will be used for signing this artifact and will be stored in public transparency logs and cannot be removed later.\\n        By typing \'y\', you attest that you grant (or have permission to grant) and agree to have this information stored permanently in transparency logs.\\n\\nAre you sure you want to continue? (y/[N]): y\\nYour browser will now be opened to:\\nhttps://oauth2.sigstore.dev/auth/auth?access_type=online&client_id=sigstore&code_challenge=gGdRPWHb4ZNnBjRIEs9wbBhI3bqVriOCyq2W98YuqQ0&code_challenge_method=S256&nonce=2KGHDNf4CZ4gXINF9A12quVVxHl&redirect_uri=http%3A%2F%2Flocalhost%3A41711%2Fauth%2Fcallback&response_type=code&scope=openid+email&state=2KGHDOhtlyDhCegsNy1qPuKAWbd\\nSuccessfully verified SCT...\\nusing ephemeral certificate:\\n-----BEGIN CERTIFICATE-----\\nMIICpzCCAi6gAwIBAgIUb6LDCNlvHnUGD55dbYuRq9BEB7gwCgYIKoZIzj0EAwMw\\nNzEVMBMGA1UEChMMc2lnc3RvcmUuZGV2MR4wHAYDVQQDExVzaWdzdG9yZS1pbnRl\\ncm1lZGlhdGUwHhcNMjMwMTEzMDcxMTIyWhcNMjMwMTEzMDcyMTIyWjAAMFkwEwYH\\nKoZIzj0CAQYIKoZIzj0DAQcDQgAEDTvL0PRsxoxMXfSaXu+7w0ovVNzZk/BAIoz2\\nGL2cPY3qZENU/+YrR92AuZFXn0jSmmvOktpAzGhnDhtidonkyKOCAU0wggFJMA4G\\nA1UdDwEB/wQEAwIHgDATBgNVHSUEDDAKBggrBgEFBQcDAzAdBgNVHQ4EFgQUdZPv\\nPd5abMkW8mcBgb3umAmHTcUwHwYDVR0jBBgwFoAU39Ppz1YkEZb5qNjpKFWixi4Y\\nZD8wJwYDVR0RAQH/BB0wG4EZZGFuaWVsLmJldmVuaXVzQGdtYWlsLmNvbTAsBgor\\nBgEEAYO/MAEBBB5odHRwczovL2dpdGh1Yi5jb20vbG9naW4vb2F1dGgwgYoGCisG\\nAQQB1nkCBAIEfAR6AHgAdgDdPTBqxscRMmMZHhyZZzcCokpeuN48rf+HinKALynu\\njgAAAYWp+AiYAAAEAwBHMEUCIAlfL870WJta7pD97Yiw0JbvY7YGg604cGxXEXtQ\\ntzoaAiEA+VWQiz+JPEsLBLbtclfhXFhn/C4kTyaS2Fj12+voTt4wCgYIKoZIzj0E\\nAwMDZwAwZAIwUtBB+1H6177KW3nfTpK9unSGgwIPEuNqQviJyeZRjkK85pnfk0p5\\nlwQVbfekXYq+AjBgJA/xjX5+UqRh+O1LqxBIun1gYhIwK+UUZq49SH0uP2sQL5un\\nILHOPrBw0f00Q68=\\n-----END CERTIFICATE-----\\n\\ntlog entry created with index: 11074687\\nBundle wrote in the file artifact.bundle\\nMEUCIBbfVr0rREgk2yXfENMzTduXnSRc2GkJEUOb5tBncFgSAiEAtC4f1CA4Yio9N3wjdMAbY6hCerCKwyM+hn8L1kn33GE=\\n```\\n\\nAfter this there will be an file named `artifact.bundle` in the directory where\\nthe above command was executed.\\n\\nSo lets take a look at the bundle:\\n\\n```console\\n$ cat artifact.bundle | jq\\n{\\n  \\"base64Signature\\": \\"MEUCIBbfVr0rREgk2yXfENMzTduXnSRc2GkJEUOb5tBncFgSAiEAtC4f1CA4Yio9N3wjdMAbY6hCerCKwyM+hn8L1kn33GE=\\",\\n  \\"cert\\": \\"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNwekNDQWk2Z0F3SUJBZ0lVYjZMRENObHZIblVHRDU1ZGJZdVJxOUJFQjdnd0NnWUlLb1pJemowRUF3TXcKTnpFVk1CTUdBMVVFQ2hNTWMybG5jM1J2Y21VdVpHVjJNUjR3SEFZRFZRUURFeFZ6YVdkemRHOXlaUzFwYm5SbApjbTFsWkdsaGRHVXdIaGNOTWpNd01URXpNRGN4TVRJeVdoY05Nak13TVRFek1EY3lNVEl5V2pBQU1Ga3dFd1lICktvWkl6ajBDQVFZSUtvWkl6ajBEQVFjRFFnQUVEVHZMMFBSc3hveE1YZlNhWHUrN3cwb3ZWTnpaay9CQUlvejIKR0wyY1BZM3FaRU5VLytZclI5MkF1WkZYbjBqU21tdk9rdHBBekdobkRodGlkb25reUtPQ0FVMHdnZ0ZKTUE0RwpBMVVkRHdFQi93UUVBd0lIZ0RBVEJnTlZIU1VFRERBS0JnZ3JCZ0VGQlFjREF6QWRCZ05WSFE0RUZnUVVkWlB2ClBkNWFiTWtXOG1jQmdiM3VtQW1IVGNVd0h3WURWUjBqQkJnd0ZvQVUzOVBwejFZa0VaYjVxTmpwS0ZXaXhpNFkKWkQ4d0p3WURWUjBSQVFIL0JCMHdHNEVaWkdGdWFXVnNMbUpsZG1WdWFYVnpRR2R0WVdsc0xtTnZiVEFzQmdvcgpCZ0VFQVlPL01BRUJCQjVvZEhSd2N6b3ZMMmRwZEdoMVlpNWpiMjB2Ykc5bmFXNHZiMkYxZEdnd2dZb0dDaXNHCkFRUUIxbmtDQkFJRWZBUjZBSGdBZGdEZFBUQnF4c2NSTW1NWkhoeVpaemNDb2twZXVONDhyZitIaW5LQUx5bnUKamdBQUFZV3ArQWlZQUFBRUF3QkhNRVVDSUFsZkw4NzBXSnRhN3BEOTdZaXcwSmJ2WTdZR2c2MDRjR3hYRVh0UQp0em9hQWlFQStWV1FpeitKUEVzTEJMYnRjbGZoWEZobi9DNGtUeWFTMkZqMTIrdm9UdDR3Q2dZSUtvWkl6ajBFCkF3TURad0F3WkFJd1V0QkIrMUg2MTc3S1czbmZUcEs5dW5TR2d3SVBFdU5xUXZpSnllWlJqa0s4NXBuZmswcDUKbHdRVmJmZWtYWXErQWpCZ0pBL3hqWDUrVXFSaCtPMUxxeEJJdW4xZ1loSXdLK1VVWnE0OVNIMHVQMnNRTDV1bgpJTEhPUHJCdzBmMDBRNjg9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\\",\\n  \\"rekorBundle\\": {\\n    \\"SignedEntryTimestamp\\": \\"MEUCIQDYiu9WHR4eCJ2JGPCfwWYg/lILIM+9IvDEb3Nq2MYIUAIgK2tRLSYDLuU0uaywKy8C+3ETUBKfw1lds4Q4Bw4l8jQ=\\",\\n    \\"Payload\\": {\\n      \\"body\\": \\"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\\",\\n      \\"integratedTime\\": 1673593883,\\n      \\"logIndex\\": 11074687,\\n      \\"logID\\": \\"c0d23d6ad406973f9559f3ba2d1ca01f84147d8ffc5b8445c224f98b9591801d\\"\\n    }\\n  }\\n}\\n```\\n\\nSo we have a json object with three fields, a `base64Signature`, a `cert`, and\\na `rekorBundle` field.\\n\\nLets start with `base64Signature` field:\\n\\n```console\\n$ cat artifact.bundle | jq \'.base64Signature\'\\n\\"MEUCIBbfVr0rREgk2yXfENMzTduXnSRc2GkJEUOb5tBncFgSAiEAtC4f1CA4Yio9N3wjdMAbY6hCerCKwyM+hn8L1kn33GE=\\"\\n```\\n\\nAs the name of this field implies it contains a base64 encoded signature.\\n\\nLets be decode the signature and store it in a file:\\n\\n```\\n$ cat artifact.bundle | jq -r \'.base64Signature\' | base64 -d - > signature\\n```\\n\\nWe will use this file shortly.\\n\\nThe `cert` field contains a base64 encoded certificate in pem format:\\n\\n```\\n$ cat artifact.bundle | jq -r \'.rekorBundle.Payload.body\' | base64 -d - | jq -r \'.spec.signature.publicKey.content\' | base64 -d -\\n-----BEGIN CERTIFICATE-----\\nMIICpzCCAi6gAwIBAgIUb6LDCNlvHnUGD55dbYuRq9BEB7gwCgYIKoZIzj0EAwMw\\nNzEVMBMGA1UEChMMc2lnc3RvcmUuZGV2MR4wHAYDVQQDExVzaWdzdG9yZS1pbnRl\\ncm1lZGlhdGUwHhcNMjMwMTEzMDcxMTIyWhcNMjMwMTEzMDcyMTIyWjAAMFkwEwYH\\nKoZIzj0CAQYIKoZIzj0DAQcDQgAEDTvL0PRsxoxMXfSaXu+7w0ovVNzZk/BAIoz2\\nGL2cPY3qZENU/+YrR92AuZFXn0jSmmvOktpAzGhnDhtidonkyKOCAU0wggFJMA4G\\nA1UdDwEB/wQEAwIHgDATBgNVHSUEDDAKBggrBgEFBQcDAzAdBgNVHQ4EFgQUdZPv\\nPd5abMkW8mcBgb3umAmHTcUwHwYDVR0jBBgwFoAU39Ppz1YkEZb5qNjpKFWixi4Y\\nZD8wJwYDVR0RAQH/BB0wG4EZZGFuaWVsLmJldmVuaXVzQGdtYWlsLmNvbTAsBgor\\nBgEEAYO/MAEBBB5odHRwczovL2dpdGh1Yi5jb20vbG9naW4vb2F1dGgwgYoGCisG\\nAQQB1nkCBAIEfAR6AHgAdgDdPTBqxscRMmMZHhyZZzcCokpeuN48rf+HinKALynu\\njgAAAYWp+AiYAAAEAwBHMEUCIAlfL870WJta7pD97Yiw0JbvY7YGg604cGxXEXtQ\\ntzoaAiEA+VWQiz+JPEsLBLbtclfhXFhn/C4kTyaS2Fj12+voTt4wCgYIKoZIzj0E\\nAwMDZwAwZAIwUtBB+1H6177KW3nfTpK9unSGgwIPEuNqQviJyeZRjkK85pnfk0p5\\nlwQVbfekXYq+AjBgJA/xjX5+UqRh+O1LqxBIun1gYhIwK+UUZq49SH0uP2sQL5un\\nILHOPrBw0f00Q68=\\n-----END CERTIFICATE-----\\n```\\n\\nWe can inspect this certificate using openssl:\\n\\n```\\n$ cat artifact.bundle | jq -r \'.rekorBundle.Payload.body\' | base64 -d - | jq -r \'.spec.signature.publicKey.content\' | base64 -d - | openssl x509 -text\\nCertificate:\\n    Data:\\n        Version: 3 (0x2)\\n        Serial Number:\\n            6f:a2:c3:08:d9:6f:1e:75:06:0f:9e:5d:6d:8b:91:ab:d0:44:07:b8\\n        Signature Algorithm: ecdsa-with-SHA384\\n        Issuer: O = sigstore.dev, CN = sigstore-intermediate\\n        Validity\\n            Not Before: Jan 13 07:11:22 2023 GMT\\n            Not After : Jan 13 07:21:22 2023 GMT\\n        Subject:\\n        Subject Public Key Info:\\n            Public Key Algorithm: id-ecPublicKey\\n                Public-Key: (256 bit)\\n                pub:\\n                    04:0d:3b:cb:d0:f4:6c:c6:8c:4c:5d:f4:9a:5e:ef:\\n                    bb:c3:4a:2f:54:dc:d9:93:f0:40:22:8c:f6:18:bd:\\n                    9c:3d:8d:ea:64:43:54:ff:e6:2b:47:dd:80:b9:91:\\n                    57:9f:48:d2:9a:6b:ce:92:da:40:cc:68:67:0e:1b:\\n                    62:76:89:e4:c8\\n                ASN1 OID: prime256v1\\n                NIST CURVE: P-256\\n        X509v3 extensions:\\n            X509v3 Key Usage: critical\\n                Digital Signature\\n            X509v3 Extended Key Usage:\\n                Code Signing\\n            X509v3 Subject Key Identifier:\\n                75:93:EF:3D:DE:5A:6C:C9:16:F2:67:01:81:BD:EE:98:09:87:4D:C5\\n            X509v3 Authority Key Identifier:\\n                keyid:DF:D3:E9:CF:56:24:11:96:F9:A8:D8:E9:28:55:A2:C6:2E:18:64:3F\\n\\n            X509v3 Subject Alternative Name: critical\\n                email:daniel.bevenius@gmail.com\\n            1.3.6.1.4.1.57264.1.1:\\n                https://github.com/login/oauth\\n            CT Precertificate SCTs:\\n                Signed Certificate Timestamp:\\n                    Version   : v1 (0x0)\\n                    Log ID    : DD:3D:30:6A:C6:C7:11:32:63:19:1E:1C:99:67:37:02:\\n                                A2:4A:5E:B8:DE:3C:AD:FF:87:8A:72:80:2F:29:EE:8E\\n                    Timestamp : Jan 13 07:11:22.776 2023 GMT\\n                    Extensions: none\\n                    Signature : ecdsa-with-SHA256\\n                                30:45:02:20:09:5F:2F:CE:F4:58:9B:5A:EE:90:FD:ED:\\n                                88:B0:D0:96:EF:63:B6:06:83:AD:38:70:6C:57:11:7B:\\n                                50:B7:3A:1A:02:21:00:F9:55:90:8B:3F:89:3C:4B:0B:\\n                                04:B6:ED:72:57:E1:5C:58:67:FC:2E:24:4F:26:92:D8:\\n                                58:F5:DB:EB:E8:4E:DE\\n    Signature Algorithm: ecdsa-with-SHA384\\n         30:64:02:30:52:d0:41:fb:51:fa:d7:be:ca:5b:79:df:4e:92:\\n         bd:ba:74:86:83:02:0f:12:e3:6a:42:f8:89:c9:e6:51:8e:42:\\n         bc:e6:99:df:93:4a:79:97:04:15:6d:f7:a4:5d:8a:be:02:30:\\n         60:24:0f:f1:8d:7e:7e:52:a4:61:f8:ed:4b:ab:10:48:ba:7d:\\n         60:62:12:30:2b:e5:14:66:ae:3d:48:7d:2e:3f:6b:10:2f:9b:\\n         a7:20:b1:ce:3e:b0:70:d1:fd:34:43:af\\n-----BEGIN CERTIFICATE-----\\nMIICpzCCAi6gAwIBAgIUb6LDCNlvHnUGD55dbYuRq9BEB7gwCgYIKoZIzj0EAwMw\\nNzEVMBMGA1UEChMMc2lnc3RvcmUuZGV2MR4wHAYDVQQDExVzaWdzdG9yZS1pbnRl\\ncm1lZGlhdGUwHhcNMjMwMTEzMDcxMTIyWhcNMjMwMTEzMDcyMTIyWjAAMFkwEwYH\\nKoZIzj0CAQYIKoZIzj0DAQcDQgAEDTvL0PRsxoxMXfSaXu+7w0ovVNzZk/BAIoz2\\nGL2cPY3qZENU/+YrR92AuZFXn0jSmmvOktpAzGhnDhtidonkyKOCAU0wggFJMA4G\\nA1UdDwEB/wQEAwIHgDATBgNVHSUEDDAKBggrBgEFBQcDAzAdBgNVHQ4EFgQUdZPv\\nPd5abMkW8mcBgb3umAmHTcUwHwYDVR0jBBgwFoAU39Ppz1YkEZb5qNjpKFWixi4Y\\nZD8wJwYDVR0RAQH/BB0wG4EZZGFuaWVsLmJldmVuaXVzQGdtYWlsLmNvbTAsBgor\\nBgEEAYO/MAEBBB5odHRwczovL2dpdGh1Yi5jb20vbG9naW4vb2F1dGgwgYoGCisG\\nAQQB1nkCBAIEfAR6AHgAdgDdPTBqxscRMmMZHhyZZzcCokpeuN48rf+HinKALynu\\njgAAAYWp+AiYAAAEAwBHMEUCIAlfL870WJta7pD97Yiw0JbvY7YGg604cGxXEXtQ\\ntzoaAiEA+VWQiz+JPEsLBLbtclfhXFhn/C4kTyaS2Fj12+voTt4wCgYIKoZIzj0E\\nAwMDZwAwZAIwUtBB+1H6177KW3nfTpK9unSGgwIPEuNqQviJyeZRjkK85pnfk0p5\\nlwQVbfekXYq+AjBgJA/xjX5+UqRh+O1LqxBIun1gYhIwK+UUZq49SH0uP2sQL5un\\nILHOPrBw0f00Q68=\\n-----END CERTIFICATE-----\\n```\\n\\nLets store the certificate in a file, and extract the public key:\\n\\n```console\\n$ cat artifact.bundle | jq -r \'.rekorBundle.Payload.body\' | base64 -d - | jq -r \'.spec.signature.publicKey.content\' | base64 -d -  > cert.pem\\n$ openssl x509 -pubkey -noout -in cert.pem  > pub.pem\\n```\\n\\nWith those files, the `signature`, the `public key`, and the `blob` we should be\\nable to verify the signature from the `base64Signature` field using the\\nfollowing command:\\n\\n```console\\n$ openssl dgst -verify pub.pem -keyform PEM -sha256 -signature signature -binary artifact.txt\\nVerified OK\\n```\\n\\nThe motivation of doing that was to show that the `base64Signature` is just the\\nsignature of the blob.\\n\\nNext, lets take a look at the `rekorBundle` field:\\n\\n```console\\n$ cat artifact.bundle | jq -r \'.rekorBundle\' |  jq \'.\'\\n{\\n  \\"SignedEntryTimestamp\\": \\"MEUCIQDYiu9WHR4eCJ2JGPCfwWYg/lILIM+9IvDEb3Nq2MYIUAIgK2tRLSYDLuU0uaywKy8C+3ETUBKfw1lds4Q4Bw4l8jQ=\\",\\n  \\"Payload\\": {\\n    \\"body\\": \\"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\\",\\n    \\"integratedTime\\": 1673593883,\\n    \\"logIndex\\": 11074687,\\n    \\"logID\\": \\"c0d23d6ad406973f9559f3ba2d1ca01f84147d8ffc5b8445c224f98b9591801d\\"\\n  }\\n}\\n```\\n\\n`SignedEntryTimestamp` is a signature of the `logIndex`, the `body`, and the\\n`integratedTime` time fields created by Rekor. We can inspect the Rekor log\\nentry to verify:\\n\\n```console\\n$ curl --silent https://rekor.sigstore.dev/api/v1/log/entries?logIndex=11074687 | jq -r \'.[]\'\\n{\\n  \\"body\\": \\"eyJhcGlWZXJzaW9uIjoiMC4wLjEiLCJraW5kIjoiaGFzaGVkcmVrb3JkIiwic3BlYyI6eyJkYXRhIjp7Imhhc2giOnsiYWxnb3JpdGhtIjoic2hhMjU2IiwidmFsdWUiOiI1YWEwM2Y5NmM3NzUzNjU3OTE2NmZiYTE0NzkyOTYyNmNjM2E5Nzk2MGU5OTQwNTdhOWQ4MDI3MWE3MzZkMTBmIn19LCJzaWduYXR1cmUiOnsiY29udGVudCI6Ik1FVUNJQmJmVnIwclJFZ2syeVhmRU5NelRkdVhuU1JjMkdrSkVVT2I1dEJuY0ZnU0FpRUF0QzRmMUNBNFlpbzlOM3dqZE1BYlk2aENlckNLd3lNK2huOEwxa24zM0dFPSIsInB1YmxpY0tleSI6eyJjb250ZW50IjoiTFMwdExTMUNSVWRKVGlCRFJWSlVTVVpKUTBGVVJTMHRMUzB0Q2sxSlNVTndla05EUVdrMlowRjNTVUpCWjBsVllqWk1SRU5PYkhaSWJsVkhSRFUxWkdKWmRWSnhPVUpGUWpkbmQwTm5XVWxMYjFwSmVtb3dSVUYzVFhjS1RucEZWazFDVFVkQk1WVkZRMmhOVFdNeWJHNWpNMUoyWTIxVmRWcEhWakpOVWpSM1NFRlpSRlpSVVVSRmVGWjZZVmRrZW1SSE9YbGFVekZ3WW01U2JBcGpiVEZzV2tkc2FHUkhWWGRJYUdOT1RXcE5kMDFVUlhwTlJHTjRUVlJKZVZkb1kwNU5hazEzVFZSRmVrMUVZM2xOVkVsNVYycEJRVTFHYTNkRmQxbElDa3R2V2tsNmFqQkRRVkZaU1V0dldrbDZhakJFUVZGalJGRm5RVVZFVkhaTU1GQlNjM2h2ZUUxWVpsTmhXSFVyTjNjd2IzWldUbnBhYXk5Q1FVbHZlaklLUjB3eVkxQlpNM0ZhUlU1Vkx5dFpjbEk1TWtGMVdrWlliakJxVTIxdGRrOXJkSEJCZWtkb2JrUm9kR2xrYjI1cmVVdFBRMEZWTUhkblowWktUVUUwUndwQk1WVmtSSGRGUWk5M1VVVkJkMGxJWjBSQlZFSm5UbFpJVTFWRlJFUkJTMEpuWjNKQ1owVkdRbEZqUkVGNlFXUkNaMDVXU0ZFMFJVWm5VVlZrV2xCMkNsQmtOV0ZpVFd0WE9HMWpRbWRpTTNWdFFXMUlWR05WZDBoM1dVUldVakJxUWtKbmQwWnZRVlV6T1ZCd2VqRlphMFZhWWpWeFRtcHdTMFpYYVhocE5Ga0tXa1E0ZDBwM1dVUldVakJTUVZGSUwwSkNNSGRITkVWYVdrZEdkV0ZYVm5OTWJVcHNaRzFXZFdGWVZucFJSMlIwV1Zkc2MweHRUblppVkVGelFtZHZjZ3BDWjBWRlFWbFBMMDFCUlVKQ1FqVnZaRWhTZDJONmIzWk1NbVJ3WkVkb01WbHBOV3BpTWpCMllrYzVibUZYTkhaaU1rWXhaRWRuZDJkWmIwZERhWE5IQ2tGUlVVSXhibXREUWtGSlJXWkJValpCU0dkQlpHZEVaRkJVUW5GNGMyTlNUVzFOV2tob2VWcGFlbU5EYjJ0d1pYVk9ORGh5Wml0SWFXNUxRVXg1Ym5VS2FtZEJRVUZaVjNBclFXbFpRVUZCUlVGM1FraE5SVlZEU1VGc1prdzROekJYU25SaE4zQkVPVGRaYVhjd1NtSjJXVGRaUjJjMk1EUmpSM2hZUlZoMFVRcDBlbTloUVdsRlFTdFdWMUZwZWl0S1VFVnpURUpNWW5SamJHWm9XRVpvYmk5RE5HdFVlV0ZUTWtacU1USXJkbTlVZERSM1EyZFpTVXR2V2tsNmFqQkZDa0YzVFVSYWQwRjNXa0ZKZDFWMFFrSXJNVWcyTVRjM1MxY3pibVpVY0VzNWRXNVRSMmQzU1ZCRmRVNXhVWFpwU25sbFdsSnFhMHM0TlhCdVptc3djRFVLYkhkUlZtSm1aV3RZV1hFclFXcENaMHBCTDNocVdEVXJWWEZTYUN0UE1VeHhlRUpKZFc0eFoxbG9TWGRMSzFWVlduRTBPVk5JTUhWUU1uTlJURFYxYmdwSlRFaFBVSEpDZHpCbU1EQlJOamc5Q2kwdExTMHRSVTVFSUVORlVsUkpSa2xEUVZSRkxTMHRMUzBLIn19fX0=\\",\\n  \\"integratedTime\\": 1673593883,\\n  \\"logID\\": \\"c0d23d6ad406973f9559f3ba2d1ca01f84147d8ffc5b8445c224f98b9591801d\\",\\n  \\"logIndex\\": 11074687,\\n  \\"verification\\": {\\n    \\"inclusionProof\\": {\\n      \\"checkpoint\\": \\"rekor.sigstore.dev - 2605736670972794746\\\\n6915767\\\\nIDKYzW3/yaZoFrPpz2HKEReyArFz47FmWC3Z9REfsCY=\\\\nTimestamp: 1673599771028600100\\\\n\\\\n\u2014 rekor.sigstore.dev wNI9ajBFAiBcmpywRj3UZCOMIzwlHzd5eNYEG1rQgX5VKhHAqM49iQIhAPxul/hYfn7wHRCh8/LTXFpLbB3vieU4mqLEPZSUdX4L\\\\n\\",\\n      \\"hashes\\": [\\n        \\"e5871beb5d6ffc577b31f4b0f14763adb1a231d52f2f15dd8c44f4925e402d1d\\",\\n        \\"1d2962738aaebe76a8497de8615fadb0b8a52db957ccfb37b87719131abb53bd\\",\\n        \\"62c6f1d6610f123395faffd632dc853f682a7f1bd93e36c08e53f8591b2b50d7\\",\\n        \\"c1bb29369643451e47467ce1293a981ee4bd00a019a0a5bf77dacfd0aa15eff7\\",\\n        \\"fec4c3b5bfb7783f8eee0e83af6e781f49825efb515bd70b2745b4d15b0b56f5\\",\\n        \\"aaee4f535cb2dca6853ab13d2f2eda182dd72aa708824d6281182009763e753e\\",\\n        \\"22430b589e10029a924c4ec50a96b51fa0aff8b461b205dc9e02d3eb588bcd98\\",\\n        \\"067da42bda7c3c476cbd160a7df567266e3c4788d38d214b44774907a1e1bd27\\",\\n        \\"2269e80ae081e893a5e7a6350826b29bdf890afa397110c632910acf895e7a26\\",\\n        \\"7c4a791951a23906049a3060ac3f29e571df225f0d7eadeb5417dff7631c8cec\\",\\n        \\"3648d28ef4842a998b3f22755750e99a81a74d6567166831e325f044cde6162b\\",\\n        \\"0cd1da2cb04f7956568f32c500bad03be8a022faa50f393c185ac5ac201f3339\\",\\n        \\"a14a3e23a363f496dc96a3061d454e1f0629ea94c0664991d4e5eab4d29306cb\\",\\n        \\"4c00279b889607cf1f98294f05dc3f10ffecd2a87df4af3e4360a039ff3421c9\\",\\n        \\"35da85b3d8a823b9040af497f298dc7c517236e78a98b1b426251b9ecde33628\\",\\n        \\"9d57b977c2a8ebeb68d127df7be605c849c732275a0b05db00264a59f4ca0834\\",\\n        \\"9eb0417210c64dcc971f58268b5aaa968ce2c2d200c41b346f50a100728ebc72\\",\\n        \\"e7d67f5102ddeda58eda651dcba76876d01955a4eca9fce4caaf9e0ba7521cdd\\",\\n        \\"616429db6c7d20c5b0eff1a6e512ea57a0734b94ae0bc7c914679463e01a7fba\\",\\n        \\"5a4ad1534b1e770f02bfde0de15008a6971cf1ffbfa963fc9c2a644973a8d2d1\\"\\n      ],\\n      \\"logIndex\\": 6911256,\\n      \\"rootHash\\": \\"203298cd6dffc9a66816b3e9cf61ca1117b202b173e3b166582dd9f5111fb026\\",\\n      \\"treeSize\\": 6915767\\n    },\\n    \\"signedEntryTimestamp\\": \\"MEUCIGicHWGa0XI3perd9LM64+tdneXvvVsOrWxn7pCoUbuNAiEAjmgWIxOH8itbqYjAgkiilYmNVR/hewmfatviQZf3Wr8=\\"\\n  }\\n}\\n```\\n\\nThe Rekor log can also be accessed using https:\\nhttps://rekor.tlog.dev/?logIndex=11074687\\n\\nNow, lets inspect the `body` field of the bundle:\\n\\n```console\\n$ cat artifact.bundle | jq -r \'.rekorBundle.Payload.body\' | base64 -d - | jq\\n{\\n  \\"apiVersion\\": \\"0.0.1\\",\\n  \\"kind\\": \\"hashedrekord\\",\\n  \\"spec\\": {\\n    \\"data\\": {\\n      \\"hash\\": {\\n        \\"algorithm\\": \\"sha256\\",\\n        \\"value\\": \\"5aa03f96c77536579166fba147929626cc3a97960e994057a9d80271a736d10f\\"\\n      }\\n    },\\n    \\"signature\\": {\\n      \\"content\\": \\"MEUCIBbfVr0rREgk2yXfENMzTduXnSRc2GkJEUOb5tBncFgSAiEAtC4f1CA4Yio9N3wjdMAbY6hCerCKwyM+hn8L1kn33GE=\\",\\n      \\"publicKey\\": {\\n        \\"content\\": \\"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNwekNDQWk2Z0F3SUJBZ0lVYjZMRENObHZIblVHRDU1ZGJZdVJxOUJFQjdnd0NnWUlLb1pJemowRUF3TXcKTnpFVk1CTUdBMVVFQ2hNTWMybG5jM1J2Y21VdVpHVjJNUjR3SEFZRFZRUURFeFZ6YVdkemRHOXlaUzFwYm5SbApjbTFsWkdsaGRHVXdIaGNOTWpNd01URXpNRGN4TVRJeVdoY05Nak13TVRFek1EY3lNVEl5V2pBQU1Ga3dFd1lICktvWkl6ajBDQVFZSUtvWkl6ajBEQVFjRFFnQUVEVHZMMFBSc3hveE1YZlNhWHUrN3cwb3ZWTnpaay9CQUlvejIKR0wyY1BZM3FaRU5VLytZclI5MkF1WkZYbjBqU21tdk9rdHBBekdobkRodGlkb25reUtPQ0FVMHdnZ0ZKTUE0RwpBMVVkRHdFQi93UUVBd0lIZ0RBVEJnTlZIU1VFRERBS0JnZ3JCZ0VGQlFjREF6QWRCZ05WSFE0RUZnUVVkWlB2ClBkNWFiTWtXOG1jQmdiM3VtQW1IVGNVd0h3WURWUjBqQkJnd0ZvQVUzOVBwejFZa0VaYjVxTmpwS0ZXaXhpNFkKWkQ4d0p3WURWUjBSQVFIL0JCMHdHNEVaWkdGdWFXVnNMbUpsZG1WdWFYVnpRR2R0WVdsc0xtTnZiVEFzQmdvcgpCZ0VFQVlPL01BRUJCQjVvZEhSd2N6b3ZMMmRwZEdoMVlpNWpiMjB2Ykc5bmFXNHZiMkYxZEdnd2dZb0dDaXNHCkFRUUIxbmtDQkFJRWZBUjZBSGdBZGdEZFBUQnF4c2NSTW1NWkhoeVpaemNDb2twZXVONDhyZitIaW5LQUx5bnUKamdBQUFZV3ArQWlZQUFBRUF3QkhNRVVDSUFsZkw4NzBXSnRhN3BEOTdZaXcwSmJ2WTdZR2c2MDRjR3hYRVh0UQp0em9hQWlFQStWV1FpeitKUEVzTEJMYnRjbGZoWEZobi9DNGtUeWFTMkZqMTIrdm9UdDR3Q2dZSUtvWkl6ajBFCkF3TURad0F3WkFJd1V0QkIrMUg2MTc3S1czbmZUcEs5dW5TR2d3SVBFdU5xUXZpSnllWlJqa0s4NXBuZmswcDUKbHdRVmJmZWtYWXErQWpCZ0pBL3hqWDUrVXFSaCtPMUxxeEJJdW4xZ1loSXdLK1VVWnE0OVNIMHVQMnNRTDV1bgpJTEhPUHJCdzBmMDBRNjg9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\\"\\n      }\\n    }\\n  }\\n}\\n```\\n\\nNow, when an online verification is done, that is when the bundle is not\\nspecified on the command line, the Rekor bundle is looked up in the transparency\\nlog. Notice for example that the same information is also available in the\\nRekor log:\\n\\n```console\\n$ curl --silent https://rekor.sigstore.dev/api/v1/log/entries?logIndex=11074687 | jq -r \'.[].body\' | base64 -d | jq\\n{\\n  \\"apiVersion\\": \\"0.0.1\\",\\n  \\"kind\\": \\"hashedrekord\\",\\n  \\"spec\\": {\\n    \\"data\\": {\\n      \\"hash\\": {\\n        \\"algorithm\\": \\"sha256\\",\\n        \\"value\\": \\"5aa03f96c77536579166fba147929626cc3a97960e994057a9d80271a736d10f\\"\\n      }\\n    },\\n    \\"signature\\": {\\n      \\"content\\": \\"MEUCIBbfVr0rREgk2yXfENMzTduXnSRc2GkJEUOb5tBncFgSAiEAtC4f1CA4Yio9N3wjdMAbY6hCerCKwyM+hn8L1kn33GE=\\",\\n      \\"publicKey\\": {\\n        \\"content\\": \\"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNwekNDQWk2Z0F3SUJBZ0lVYjZMRENObHZIblVHRDU1ZGJZdVJxOUJFQjdnd0NnWUlLb1pJemowRUF3TXcKTnpFVk1CTUdBMVVFQ2hNTWMybG5jM1J2Y21VdVpHVjJNUjR3SEFZRFZRUURFeFZ6YVdkemRHOXlaUzFwYm5SbApjbTFsWkdsaGRHVXdIaGNOTWpNd01URXpNRGN4TVRJeVdoY05Nak13TVRFek1EY3lNVEl5V2pBQU1Ga3dFd1lICktvWkl6ajBDQVFZSUtvWkl6ajBEQVFjRFFnQUVEVHZMMFBSc3hveE1YZlNhWHUrN3cwb3ZWTnpaay9CQUlvejIKR0wyY1BZM3FaRU5VLytZclI5MkF1WkZYbjBqU21tdk9rdHBBekdobkRodGlkb25reUtPQ0FVMHdnZ0ZKTUE0RwpBMVVkRHdFQi93UUVBd0lIZ0RBVEJnTlZIU1VFRERBS0JnZ3JCZ0VGQlFjREF6QWRCZ05WSFE0RUZnUVVkWlB2ClBkNWFiTWtXOG1jQmdiM3VtQW1IVGNVd0h3WURWUjBqQkJnd0ZvQVUzOVBwejFZa0VaYjVxTmpwS0ZXaXhpNFkKWkQ4d0p3WURWUjBSQVFIL0JCMHdHNEVaWkdGdWFXVnNMbUpsZG1WdWFYVnpRR2R0WVdsc0xtTnZiVEFzQmdvcgpCZ0VFQVlPL01BRUJCQjVvZEhSd2N6b3ZMMmRwZEdoMVlpNWpiMjB2Ykc5bmFXNHZiMkYxZEdnd2dZb0dDaXNHCkFRUUIxbmtDQkFJRWZBUjZBSGdBZGdEZFBUQnF4c2NSTW1NWkhoeVpaemNDb2twZXVONDhyZitIaW5LQUx5bnUKamdBQUFZV3ArQWlZQUFBRUF3QkhNRVVDSUFsZkw4NzBXSnRhN3BEOTdZaXcwSmJ2WTdZR2c2MDRjR3hYRVh0UQp0em9hQWlFQStWV1FpeitKUEVzTEJMYnRjbGZoWEZobi9DNGtUeWFTMkZqMTIrdm9UdDR3Q2dZSUtvWkl6ajBFCkF3TURad0F3WkFJd1V0QkIrMUg2MTc3S1czbmZUcEs5dW5TR2d3SVBFdU5xUXZpSnllWlJqa0s4NXBuZmswcDUKbHdRVmJmZWtYWXErQWpCZ0pBL3hqWDUrVXFSaCtPMUxxeEJJdW4xZ1loSXdLK1VVWnE0OVNIMHVQMnNRTDV1bgpJTEhPUHJCdzBmMDBRNjg9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\\"\\n      }\\n    }\\n  }\\n}\\n```\\n\\nNow, let first look at the `spec.data.hash` field which contains a hash\\nalgorithm that was used, and a value. The value is the hash of our blob, the\\n`artifact.txt`file:\\n\\n```console\\n$ sha256sum artifact.txt\\n5aa03f96c77536579166fba147929626cc3a97960e994057a9d80271a736d10f  artifact.txt\\n```\\n\\nAnd we can compare this with the value in the `spec.data.hash.value` field and\\ncheck that they indeed are the same:\\n\\n```console\\n$ cat artifact.bundle | jq -r \'.rekorBundle.Payload.body\' | base64 -d - | jq -r \'.spec.data.hash.value\'\\n5aa03f96c77536579166fba147929626cc3a97960e994057a9d80271a736d10f\\n```\\n\\nNext we have signature field:\\n\\n```console\\n$ cat artifact.bundle | jq -r \'.rekorBundle.Payload.body\' | base64 -d - | jq -r \'.spec.signature.content\'\\nMEUCIBbfVr0rREgk2yXfENMzTduXnSRc2GkJEUOb5tBncFgSAiEAtC4f1CA4Yio9N3wjdMAbY6hCerCKwyM+hn8L1kn33GE=\\n```\\n\\nNotice that this is the same value as the toplevel `base64Signature` field:\\n\\n```console\\n$ cat artifact.bundle | jq -r \'.base64Signature\'\\nMEUCIBbfVr0rREgk2yXfENMzTduXnSRc2GkJEUOb5tBncFgSAiEAtC4f1CA4Yio9N3wjdMAbY6hCerCKwyM+hn8L1kn33GE=\\n```\\n\\nAnd there is also a `publicKey` field in the body which contains:\\n\\n```console\\n$ cat artifact.bundle | jq -r \'.rekorBundle.Payload.body\' | base64 -d - | jq -r \'.spec.signature.publicKey.content\' | base64 -d\\n-----BEGIN CERTIFICATE-----\\nMIICpzCCAi6gAwIBAgIUb6LDCNlvHnUGD55dbYuRq9BEB7gwCgYIKoZIzj0EAwMw\\nNzEVMBMGA1UEChMMc2lnc3RvcmUuZGV2MR4wHAYDVQQDExVzaWdzdG9yZS1pbnRl\\ncm1lZGlhdGUwHhcNMjMwMTEzMDcxMTIyWhcNMjMwMTEzMDcyMTIyWjAAMFkwEwYH\\nKoZIzj0CAQYIKoZIzj0DAQcDQgAEDTvL0PRsxoxMXfSaXu+7w0ovVNzZk/BAIoz2\\nGL2cPY3qZENU/+YrR92AuZFXn0jSmmvOktpAzGhnDhtidonkyKOCAU0wggFJMA4G\\nA1UdDwEB/wQEAwIHgDATBgNVHSUEDDAKBggrBgEFBQcDAzAdBgNVHQ4EFgQUdZPv\\nPd5abMkW8mcBgb3umAmHTcUwHwYDVR0jBBgwFoAU39Ppz1YkEZb5qNjpKFWixi4Y\\nZD8wJwYDVR0RAQH/BB0wG4EZZGFuaWVsLmJldmVuaXVzQGdtYWlsLmNvbTAsBgor\\nBgEEAYO/MAEBBB5odHRwczovL2dpdGh1Yi5jb20vbG9naW4vb2F1dGgwgYoGCisG\\nAQQB1nkCBAIEfAR6AHgAdgDdPTBqxscRMmMZHhyZZzcCokpeuN48rf+HinKALynu\\njgAAAYWp+AiYAAAEAwBHMEUCIAlfL870WJta7pD97Yiw0JbvY7YGg604cGxXEXtQ\\ntzoaAiEA+VWQiz+JPEsLBLbtclfhXFhn/C4kTyaS2Fj12+voTt4wCgYIKoZIzj0E\\nAwMDZwAwZAIwUtBB+1H6177KW3nfTpK9unSGgwIPEuNqQviJyeZRjkK85pnfk0p5\\nlwQVbfekXYq+AjBgJA/xjX5+UqRh+O1LqxBIun1gYhIwK+UUZq49SH0uP2sQL5un\\nILHOPrBw0f00Q68=\\n-----END CERTIFICATE-----\\n```\\n\\nNotice that this is the same certificate as the toplevel `cert` field. Using\\nthe `base64Signature` field, and the `cert` field we are able to verify a blob\\nwhich we saw an example of previously.\\n\\nHopefully this has given some insight into the bundle format and given some\\nexample of how one can inspect the fields."},{"id":"/2023/01/11/sigstore-in-toto-opa","metadata":{"permalink":"/blog/2023/01/11/sigstore-in-toto-opa","editUrl":"https://github.com/trustification/trustification.github.io/tree/main/blog/2023-01-11-sigstore-in-toto-opa.md","source":"@site/blog/2023-01-11-sigstore-in-toto-opa.md","title":"Sigstore, in-toto, OPA, orientation","description":"As someone who was completly new to secure supply chain security (sscs) there","date":"2023-01-11T00:00:00.000Z","formattedDate":"January 11, 2023","tags":[{"label":"sigstore","permalink":"/blog/tags/sigstore"}],"readingTime":3.21,"hasTruncateMarker":true,"authors":[{"name":"Daniel Bevenius","title":"Maintainer","url":"https://github.com/danbev","imageURL":"https://github.com/danbev.png","key":"danbev"}],"frontMatter":{"title":"Sigstore, in-toto, OPA, orientation","authors":"danbev","tags":["sigstore"]},"prevItem":{"title":"Sigstore bundle format","permalink":"/blog/2023/01/13/sigstore-bundle-format"},"nextItem":{"title":"An Adventure with the CycloneDX Maven Plugin","permalink":"/blog/2022/12/09/cyclonedx-maven-plugin-adventure"}},"content":"As someone who was completly new to secure supply chain security (sscs) there\\nwere a lot of new projects that I learned the names of but did not really\\nunderstand exactly what they did or how they complement each other. This post\\nhopes to clarify a few of these projects, and others will be addressed in future\\nposts.\\n\\n\x3c!--truncate--\x3e\\n\\nLets say we have a software project that we want to distribute. We want to sign\\nthe artifact that we produce, and lets say it\'s distributed as a tar file. It\\nis possible to do this signing manually, but it involves some work like managing\\nkeys and using tools to perform the signing tasks. Using\\n[sigstore](https://www.sigstore.dev/) simplfies this process, similar to how\\nLet\'s Encrypt made it simpler to get certificates to be used with web sites.\\nSigstore also provides tools to verify signatures and a transparency log to\\nstore signatures. So that allows us to sign our end product, and publish the\\nsignatures to the transparency log, and consumers/clients can verify our\\nartifact.\\n\\nBut how can we trust what was built? For example, if I built this tar on my\\nlocal laptop I could replace a source code file with a backdoor and still be\\nable to produce a valid signature, and it could still be verified. This is also\\nthe case if a build server is used and it gets compromised, so we need something\\nmore.\\n\\nThis is where another project named [in-toto](https://in-toto.io/) comes into\\nplay. It contains tools to define the steps of a build process, and assign\\nsomeone that is responsible for each step. This person also signs the artifact\\nproduced by that step. So each step is signed by the person responsible for that\\nstep, called the funtionary, and then all the steps are signed by a product\\nowner. This will produce a document which lists the steps that were followed to\\nproduce the software, with signatures for each step.\\n\\nFor example, one step might have been checking out a specific version from git,\\nand this could be verified that it was indeed that version that was used, and\\nthe source files that were used. This gives the end user insight into the\\nproduct that they are about to install and the ability to verify it.\\n\\nSo we now have our built artifact, signed it, and we have attestations, in\\nthis case json files that contain metadata about how it was built. And we can\\nuse in-toto-verify to verify that all that information is correct.\\n\\nNow, lets say that another company, or another project, wants to include our\\nsoftware in their project, as a thirdparty dependency. Ours might be one of many\\ndependencies that they include in their product and they might have\\nrequirements/restrictions on what they are allowed to use. For example, they\\nmight require that only certain licences are used. The license information is\\nhopefully available in the project, like a license file or field in Cargo.toml,\\nbut there is nothing available to say that only certain licenses are allowed.\\nThis is where a policy engine like\\n[Open Policy Agent (OPA)](https://www.openpolicyagent.org/) comes into play. OPA\\ngives us the ability to write policy rules that take in-toto json files as\\ninput, and verify that there are licences for all thirdparty dependencies and\\nthat they are of the type(s) that are allowed. Rules can be written to handle\\nother types of restrictions/requirements as well, which are the policies that\\nthe company has.\\n\\nSo they could include a step in their build process that execute enforces the\\npolicy rules they have defined. Policy rules can also be useful when deploying\\napplications in container images where one might want to make sure that only\\nsupported base images are used etc.\\n\\nHopefully this post gives some insight into how Sigstore, in-toto, and OPA may\\nbe used, and how they complement each other."},{"id":"/2022/12/09/cyclonedx-maven-plugin-adventure","metadata":{"permalink":"/blog/2022/12/09/cyclonedx-maven-plugin-adventure","editUrl":"https://github.com/trustification/trustification.github.io/tree/main/blog/2022-12-09-cyclonedx-maven-plugin-adventure.md","source":"@site/blog/2022-12-09-cyclonedx-maven-plugin-adventure.md","title":"An Adventure with the CycloneDX Maven Plugin","description":"The CycloneDX Maven Plugin can be used to generate CycloneDX Software Bill of Materials (SBOM) for your maven projects as part of your build process. The plugin is easy to integrate, however does have some issues due mostly to idiosyncrasies and shortcomings with the maven resolution mechanism. In this post I attempt to provide some background, examples and explanations for the issues I\'ve discovered as well as context for the solutions I\'m proposing.","date":"2022-12-09T00:00:00.000Z","formattedDate":"December 9, 2022","tags":[{"label":"cyclonedx","permalink":"/blog/tags/cyclonedx"}],"readingTime":14.28,"hasTruncateMarker":true,"authors":[{"name":"Kevin Conner","title":"Maintainer","url":"https://github.com/kevinconner","imageURL":"https://github.com/kevinconner.png","key":"kevinconner"}],"frontMatter":{"title":"An Adventure with the CycloneDX Maven Plugin","authors":"kevinconner","tags":["cyclonedx"]},"prevItem":{"title":"Sigstore, in-toto, OPA, orientation","permalink":"/blog/2023/01/11/sigstore-in-toto-opa"},"nextItem":{"title":"How to Configure a Gitsign Cache","permalink":"/blog/2022/12/05/gitsign-cache"}},"content":"The CycloneDX Maven Plugin can be used to generate CycloneDX Software Bill of Materials (SBOM) for your maven projects as part of your build process. The plugin is easy to integrate, however does have some issues due mostly to idiosyncrasies and shortcomings with the maven resolution mechanism. In this post I attempt to provide some background, examples and explanations for the issues I\'ve discovered as well as context for the solutions I\'m proposing.\\n\\n\x3c!--truncate--\x3e\\n\\n# Part One - In the Beginning\\n\\nThree weeks ago I started an adventure with the [CycloneDX Maven Plugin](https://github.com/CycloneDX/cyclonedx-maven-plugin \\"The CycloneDX Maven Plugin GitHub repository\\"), investigating how we could make use of this plugin to generate Software Bill of Materials (SBOMs) for the [Quarkus](https://github.com/quarkusio/quarkus \\"The Quarkus GitHub repository\\") project. At first this goal appeared easy to achieve, simply enable the plugin for all projects within the **Quarkus** build (hello parent pom.xml) and verify the generated _bom_ contents were accurate.\\n\\nAs I had expected, enabling the plugin was very straight forward. I created a profile in the top level _pom.xml_ to capture the information required at compile time, incorporating all artifacts using _compile_, _provided_ and _system_ maven scopes. I chose not to include any _test_ or _runtime_ artifacts.\\n\\nOnce the BOM files were created I looked for a way to verify the output was reasonable. I turned to the [maven dependency plugin](https://github.com/apache/maven-dependency-plugin \\"The Maven Dependency Plugin GitHub repository\\") and its _tree_ goal, along with a fairly straight forward script to compare the information in the generated dependency tree with that in the _bom_ file and highlight any discrepancies. This is where the fun began \ud83d\ude01\\n\\nThe remainder of those three weeks involved many hours with a debugger, walking through the internals of maven as well as the CycloneDX plugin, identifying the causes of these discrepancies and working through fixes to generate the output I believed should have been included in the _bom_ files.\\n\\n---\\n\\n**Note:** At this time these changes have neither been discussed nor reviewed by the CycloneDX community, I\'ve reached out to them via slack and am hoping we can find time to go through this in detail over the next few weeks or so. In the meantime treat this article as my opinions of what I believe should be done.\\n\\n---\\n\\n# Part Two - The Case of the Missing Dependency\\n\\nBefore we talk about missing dependencies we first need to take a quick refresher for how maven resolves artifacts within a project.\\n\\nOne of the benefits of maven is its ability to automatically derive the dependency tree for your project using the information in your _pom.xml_ combined with the information in the _pom.xml_ for each of your transitive dependencies. Maven will take this information and create a dependency tree where each artifact exists once (not always the case, but we will come on to that), will favour artifacts which are closer to the root of the tree than those farther away, reconcile their versions based on the defined constraints and derive an appropriate scope.\\n\\nThe example used in the maven documentation is as follows\\n\\n```\\nA\\n  \u251c\u2500\u2500 B\\n  \u2502   \u2514\u2500\u2500 C\\n  \u2502       \u2514\u2500\u2500 D 2.0\\n  \u2514\u2500\u2500 E\\n      \u2514\u2500\u2500 D 1.0\\n```\\n\\nAs the shortest path to **D** is via E, the generated dependency tree will look like\\n\\n```\\nA\\n  \u251c\u2500\u2500 B\\n  \u2502   \u2514\u2500\u2500 C\\n  \u2514\u2500\u2500 E\\n      \u2514\u2500\u2500 D 1.0\\n```\\n\\nWith the above refresher out of the way let\'s move on to the issue of missing dependencies and consider the following example\\n\\n```\\ncom.example:trustification:jar:1.0.0\\n+- com.example:dependency1:jar:1.0.0:compile\\n|  \\\\- com.example:dependency2:jar:1.0.0:compile\\n|     \\\\- com.example:shared_dependency1:jar:1.0.0:compile\\n|       \\\\- com.example:shared_dependency2:jar:1.0.0:compile\\n\\\\- com.example:test_dependency:jar:1.0.0:test\\n   \\\\- com.example:shared_dependency1:jar:1.0.0:compile\\n      \\\\- com.example:shared_dependency2:jar:1.0.0:compile\\n```\\n\\nIn this graph we can see that **com.example:dependency2** and **com.example:test_dependency** both depend on **com.example:shared_dependency1**, with **dependency2** having a scope of _compile_ and **test_dependency** having a scope of _test_. When this graph is processed by maven we end up with a dependency tree which is close to the example given in the maven documentation.\\n\\n```\\ncom.example:trustification:jar:1.0.0\\n+- com.example:dependency1:jar:1.0.0:compile\\n|  \\\\- com.example:dependency2:jar:1.0.0:compile\\n\\\\- com.example:test_dependency:jar:1.0.0:test\\n   \\\\- com.example:shared_dependency1:jar:1.0.0:compile\\n      \\\\- com.example:shared_dependency2:jar:1.0.0:compile\\n```\\n\\nEach artifact exists once and the **shared_dependency1** artifact is now seen only under the **test_dependency** artifact.\\n\\nOnly this isn\'t the full picture.\\n\\nWhat is actually taking place under the covers is that each set of conflicts within the graph is being evaluated in order to decide which one is chosen (the winning artifact), with all losing artifacts then being updated and turned into a marker artifact without child dependencies. We can see a visualisation of this by enabling the _verbose_ flag on the _dependency:tree_ goal (use version 3.4.0), which displays the underlying dependency tree and not the clean version.\\n\\n```\\ncom.example:trustification:jar:1.0.0\\n+- com.example:dependency1:jar:1.0.0:compile\\n|  \\\\- com.example:dependency2:jar:1.0.0:compile\\n|     \\\\- (com.example:shared_dependency1:jar:1.0.0:compile - omitted for duplicate)\\n\\\\- com.example:test_dependency:jar:1.0.0:test\\n   \\\\- com.example:shared_dependency1:jar:1.0.0:compile (scope not updated to compile)\\n      \\\\- com.example:shared_dependency2:jar:1.0.0:compile\\n```\\n\\nWe can now see that the **shared_dependency1** artifact exists multiple times and that the dependency under **dependency2** lost the conflict resolution to the dependency under **test_dependency**.\\n\\nHow do we end up with missing dependencies? This happens when we filter the graph using scopes, which in my case includes _compile_, _provided_ and _system_. Let\'s take a look at what happens under those circumstances.\\n\\nMaven\'s DependencyCollectorBuilder works by first generating the dependency tree, with conflicts resolved as above, then pruning the result to remove subtrees which are not accepted by the filter. In my case this means any artifacts which do not have the right scope, along with their dependent children, will be removed. The tree returned to the CycloneDX plugin will be as follows\\n\\n```\\ncom.example:trustification:jar:1.0.0\\n\\\\- com.example:dependency1:jar:1.0.0:compile\\n   \\\\- com.example:dependency2:jar:1.0.0:compile\\n      \\\\- (com.example:shared_dependency1:jar:1.0.0:compile - omitted for duplicate)\\n```\\n\\nThe CycloneDX plugin will process this tree and create the following dependency graph within the _bom_ file\\n\\n```\\n<dependencies>\\n  <dependency ref=\\"pkg:maven/com.example/trustification@1.0.0?type=jar\\">\\n    <dependency ref=\\"pkg:maven/com.example/dependency1@1.0.0?type=jar\\"/>\\n  </dependency>\\n  <dependency ref=\\"pkg:maven/com.example/dependency1@1.0.0?type=jar\\">\\n    <dependency ref=\\"pkg:maven/com.example/dependency2@1.0.0?type=jar\\"/>\\n  </dependency>\\n  <dependency ref=\\"pkg:maven/com.example/dependency2@1.0.0?type=jar\\">\\n    <dependency ref=\\"pkg:maven/com.example/shared_dependency1@1.0.0?type=jar\\"/>\\n  </dependency>\\n  <dependency ref=\\"pkg:maven/com.example/shared_dependency1@1.0.0?type=jar\\"/>\\n</dependencies>\\n```\\n\\nThis is notable for two reasons\\n\\n1. The bom contains the _shared_dependency1_ artifact even though this entry came from the marker entry (we can make use of this)\\n2. We have lost the _shared_dependency2_ artifact since it is only present under the pruned _test_ scoped subtree.\\n\\nIn order to see a representation of the graph I would like in the _bom_ we simply need to comment out the _test_ dependency in the top level _pom.xml_, which would result in the following dependency tree\\n\\n```\\ncom.example:trustification:jar:1.0.0\\n\\\\- com.example:dependency1:jar:1.0.0:compile\\n   \\\\- com.example:dependency2:jar:1.0.0:compile\\n      \\\\- com.example:shared_dependency1:jar:1.0.0:compile\\n         \\\\- com.example:shared_dependency2:jar:1.0.0:compile\\n```\\n\\nand the following dependency graph in the _bom_\\n\\n```\\n<dependencies>\\n  <dependency ref=\\"pkg:maven/com.example/trustification@1.0.0?type=jar\\">\\n    <dependency ref=\\"pkg:maven/com.example/dependency1@1.0.0?type=jar\\"/>\\n  </dependency>\\n  <dependency ref=\\"pkg:maven/com.example/dependency1@1.0.0?type=jar\\">\\n    <dependency ref=\\"pkg:maven/com.example/dependency2@1.0.0?type=jar\\"/>\\n  </dependency>\\n  <dependency ref=\\"pkg:maven/com.example/dependency2@1.0.0?type=jar\\">\\n    <dependency ref=\\"pkg:maven/com.example/shared_dependency1@1.0.0?type=jar\\"/>\\n  </dependency>\\n  <dependency ref=\\"pkg:maven/com.example/shared_dependency1@1.0.0?type=jar\\">\\n    <dependency ref=\\"pkg:maven/com.example/shared_dependency2@1.0.0?type=jar\\"/>\\n  </dependency>\\n  <dependency ref=\\"pkg:maven/com.example/shared_dependency2@1.0.0?type=jar\\"/>\\n</dependencies>\\n```\\n\\nHow can we fix this while including the test artifact as a dependency? We can rely on the following information\\n\\n- the marker artifact has been included in the filtered result tree, however it is without children\\n- the full dependency tree contains a _winner_ artifact which contains any dependent children\\n\\nWe can then follow this process\\n\\n- retrieve the full dependency tree\\n- collect all children from top level _test_ scoped dependencies\\n- search the original tree looking for potential marker artifacts, i.e. those without children, then\\n  - check the set of hidden artifacts to see if the potential marker artifact is also hidden\\n  - if the hidden artifact does exist then transplant its dependencies to the marker artifact\\n\\nThe above will allow us to reconstruct the missing parts of the SBOM dependency graph and create my desired _bom_ content.\\n\\n---\\n\\n**Note:** The same issue will occur with _runtime_ scoped artifacts within the graph, these can also conceal _compile_ scoped artifacts if closer to the root. The difference between _test_ scoped artifacts and those with _runtime_ scope is that the _runtime_ scoped artifacts can exist anywhere in the dependency tree whereas the _test_ artifacts will only be found at the top level. The _runtime_ artifacts can be handled by following a similar process to the one above, extended to the full dependency tree.\\n\\n---\\n\\n# Part Three - Should Consistency Matter?\\n\\nIn going through the _bom_ file we can see that the information is split into two major types, **Components** and **Dependencies**. My expectation was that this information would be consistent, with these elements being related as follows\\n\\n- each **Dependency** being associated with a **Component**\\n- each nested **Dependency** referencing an existing top level **Dependency** element\\n- each **Component** being associated with a top level **Dependency**\\n\\nOne of the verification tests I ran on the _bom_ files was to test these expectations, and while I did not discover any issues with the **Dependencies** I did discover numerous examples of **Components** existing without any associated dependency information.\\n\\nWe can revisit the example from earlier to explain why this happens. The CycloneDX code decides which components to include based on the set of resolved artifacts derived from the full dependency tree, and it does this separately from determining the filtered dependency graph. The only connection between the two is that, when generating the dependency graph, the existing process will check whether the dependency has an associated _Component_ before adding it to the graph. If the _Component_ has not been created then the dependency will be ignored.\\n\\nAs a reminder, here\'s the example we covered earlier as seen by CycloneDX when processing the dependency graph.\\n\\n```\\ncom.example:trustification:jar:1.0.0\\n+- com.example:dependency1:jar:1.0.0:compile\\n|  \\\\- com.example:dependency2:jar:1.0.0:compile\\n|     \\\\- (com.example:shared_dependency1:jar:1.0.0:compile - omitted for duplicate)\\n\\\\- com.example:test_dependency:jar:1.0.0:test\\n   \\\\- com.example:shared_dependency1:jar:1.0.0:compile (scope not updated to compile)\\n      \\\\- com.example:shared_dependency2:jar:1.0.0:compile\\n```\\n\\nAnd the following represents the set of resolved artifacts used to determine which _Components_ and _Dependencies_ are included in the SBOM dependency graph, assuming we are generating the _bom_ based on the _compile_ scope.\\n\\n```\\ncom.example:trustification:jar:1.0.0\\n+- com.example:dependency1:jar:1.0.0:compile\\n|  \\\\- com.example:dependency2:jar:1.0.0:compile\\n\\\\- <test artifact ignored>\\n   \\\\- com.example:shared_dependency1:jar:1.0.0:compile (scope not updated to compile)\\n      \\\\- com.example:shared_dependency2:jar:1.0.0:compile\\n```\\n\\nThere are two issues with this that I can see\\n\\n- this will generate _Components_ for all _compile_ scope artifacts in the tree, including **shared_dependency2** from the test subtree\\n- this ignores the cumulative scopes used by maven when filtering the artifacts to create the dependency tree, relying instead on an explicit test of equality\\n\\nAll of the instances I have discovered so far have been related to the missing dependency issue from [Part Two](#part-two---the-case-of-the-missing-dependency), and are addressed by ensuring concealed artifacts are included in the SBOM dependency graph, however I am not yet convinced this is the only circumstance under which this would occur with the current codebase.\\n\\n---\\n\\n**Note:** The Maven cumulative scopes are as follows\\n\\n- compile -> system, provided and compile\\n- runtime -> compile and runtime\\n- test -> system, provided, compile, runtime and test\\n\\n---\\n\\nAnother related issue, coupled with the processing of excluded types, is the possibility of creating split dependency graphs within the same SBOM.\\n\\nConsider the following dependency tree\\n\\n```\\ncom.example:trustification:jar:1.0.0\\n+- com.example:dependency1:jar:1.0.0:compile\\n|  \\\\- com.example:dependency2:jar:1.0.0:compile\\n\\\\- com.example:type_dependency:test-jar:tests:1.0.0:compile\\n   \\\\- com.example:shared_type_dependency1:jar:1.0.0:compile\\n      \\\\- com.example:shared_type_dependency2:jar:1.0.0:compile\\n```\\n\\nIf we create the SBOM for the above tree, and choose to exclude artifacts of type _test-jar_ from the graph, the current approach will result in two dependency graphs being generated. The first would be rooted at **com.example:trustification** and the second would be rooted at **com.example:shared_type_dependency1**. This is another consequence of the _Component_ creation process, since only the specific **com.example:type_dependency:test-jar** artifact will be removed from the graph and not its dependencies. This results in the following dependency section within the _bom_.\\n\\n```\\n<dependencies>\\n  \x3c!-- the first dependency graph is rooted here --\x3e\\n  <dependency ref=\\"pkg:maven/com.example/trustification@1.0.0?type=jar\\">\\n    <dependency ref=\\"pkg:maven/com.example/dependency1@1.0.0?type=jar\\"/>\\n  </dependency>\\n  <dependency ref=\\"pkg:maven/com.example/dependency1@1.0.0?type=jar\\">\\n    <dependency ref=\\"pkg:maven/com.example/dependency2@1.0.0?type=jar\\"/>\\n  </dependency>\\n  <dependency ref=\\"pkg:maven/com.example/dependency2@1.0.0?type=jar\\"/>\\n\\n  \x3c!-- the second dependency graph is rooted here --\x3e\\n  <dependency ref=\\"pkg:maven/com.example/shared_type_dependency1@1.0.0?type=jar\\">\\n    <dependency ref=\\"pkg:maven/com.example/shared_type_dependency2@1.0.0?type=jar\\"/>\\n  </dependency>\\n  <dependency ref=\\"pkg:maven/com.example/shared_type_dependency2@1.0.0?type=jar\\"/>\\n</dependencies>\\n```\\n\\nIf we move the processing of the excluded types to the creation of the SBOM dependency graph we could address this issue and create only a single dependency graph for the root artifact, however without also addressing how we determine which _Components_ are included in the _bom_ we would be creating another source for inconsistencies and those now excluded artifacts would still exist in the _Components_ section.\\n\\nI believe a better approach would be to start with the assumption that all artifacts are included as _Components_, generate the dependency graph with type exclusion, and then prune all unreferenced components from the generated _bom_. Please remember this for now, it will come up again in the next part.\\n\\n# Part Four - The Return of the Missing Dependency\\n\\nIn [Part Two](#part-two---the-case-of-the-missing-dependency) we discussed what happened when the dependency resolution led to parts of the dependency tree being concealed by artifacts with _test_ or _runtime_ scopes, however one edge case we did not cover is when the dependency is itself the _test_ scoped artifact. In the current CycloneDX implementation this will cause the _test_ artifact, and its children, to be ignored because of the mechanism used to determine which _Components_ are included in the _bom_. Recall from [Part Three](#part-three---should-consistency-matter) that dependencies will not be included in the dependency graph unless they already have an associated _Component_.\\n\\nWe can show this in action by looking at a modified version of the earlier example, depicted by _dependency:tree_ as follows\\n\\n```\\ncom.example:trustification:jar:1.0.0\\n+- com.example:dependency1:jar:1.0.0:compile\\n|  \\\\- (com.example:test_dependency:jar:1.0.0:compile - omitted for duplicate)\\n\\\\- com.example:test_dependency:jar:1.0.0:test (scope not updated to compile)\\n   \\\\- com.example:shared_dependency1:jar:1.0.0:test\\n      \\\\- com.example:shared_dependency2:jar:1.0.0:test\\n```\\n\\nOne fact we should be aware of is that the only real _test_ scoped artifacts in the tree will be at the top level, Maven will ignore lower level _test_ scoped artifacts when determining the transitive graph. In this case **test_dependency** is the only true _test_ scoped artifact, and as neither **shared_dependency1** not **shared_dependency2** are referenced from a _compile_ scope they only have a scope of _test_ by inheriting it from the parent. Compare this with the example we used previously, when **shared_dependency1** was referenced from a _compile_ scope.\\n\\n```\\ncom.example:trustification:jar:1.0.0\\n+- com.example:dependency1:jar:1.0.0:compile\\n|  \\\\- com.example:dependency2:jar:1.0.0:compile\\n|     \\\\- (com.example:shared_dependency1:jar:1.0.0:compile - omitted for duplicate)\\n\\\\- com.example:test_dependency:jar:1.0.0:test\\n   \\\\- com.example:shared_dependency1:jar:1.0.0:compile (scope not updated to compile)\\n      \\\\- com.example:shared_dependency2:jar:1.0.0:compile\\n```\\n\\nWhat I would like to see is the SBOM dependency graph represented as follows\\n\\n```\\ncom.example:trustification:jar:1.0.0\\n+- com.example:dependency1:jar:1.0.0\\n   \\\\- com.example:test_dependency:jar:1.0.0\\n      \\\\- com.example:shared_dependency1:jar:1.0.0\\n         \\\\- com.example:shared_dependency2:jar:1.0.0\\n```\\n\\nThis can be fixed by following a similar process to that discussed in [Part Two](#part-two---the-case-of-the-missing-dependency), however it will also require us to move away from the current mechanism of identifying _Components_ and relying on those references to restrict which _Artifacts_ can be included in the dependency graph. Instead of the existing mechanism we would start from the assumption that all Artifacts will be included, determine the actual dependency graph, and then use this to prune the set of _Components_ down to the required set.\\n\\nOnce this switch is made we can finally include those remaining missing dependencies in the graph and address the issues from [Part Three](#part-three---should-consistency-matter), we certainly have a number of reasons for doing so.\\n\\nThere are a few implementation details we do need to be aware of when switching approaches\\n\\n- we need to consider excluded types, as moving their processing to the creation of the dependency graph will allow us to correctly handle their subtrees, however they can now also conceal parts of the dependency graph\\n- we need to ensure we do not inadvertently pull in _runtime_ scoped artifacts when reconstructing the graph from concealed dependencies\\n\\n# Conclusions\\n\\nThis has been an interesting journey, and through it I\'ve learned a lot about CycloneDX and more than I had likely wanted to know about the implementation of Maven. I have addressed all of the issues I believe to be present with the current approach, with the exception of one. While it is true that we can reconstruct the dependency tree to include concealed artifacts, the information provided directly by maven is insufficient to resolve all cases. The edge case identified in [Part Four](#part-four---the-return-of-the-missing-dependency) covers the inclusion of _compile_ scoped artifacts which are referenced through a top level _test_ artifact, however there is also the case of _runtime_ artifacts being included which, unfortunately, also have their scopes modified to a _test_ scope.\\n\\nBy way of example, if we were to change the scope of the **shared_dependency1** dependency within the **test_dependency** pom.xml to a scope of **runtime** we would still see the following returned by maven\\n\\n```\\ncom.example:trustification:jar:1.0.0\\n+- com.example:dependency1:jar:1.0.0:compile\\n|  \\\\- (com.example:test_dependency:jar:1.0.0:compile - omitted for duplicate)\\n\\\\- com.example:test_dependency:jar:1.0.0:test (scope not updated to compile)\\n   \\\\- com.example:shared_dependency1:jar:1.0.0:test\\n      \\\\- com.example:shared_dependency2:jar:1.0.0:test\\n```\\n\\nAs things currently stand we now have a _bom_ file which is no longer missing dependencies, although it may include the occasional _runtime_ scoped artifact when it really shouldn\'t. In order to fix this edge case we will need to go deeper into the maven level and look at the underlying graph generated by [Eclipse Aether](https://wiki.eclipse.org/Aether \\"Eclipse Aether website\\") as this contains more useful information.\\n\\n```\\nNode: com.example:trustification:1.0.0    {}\\n  Node: com.example:dependency1:1.0.0    {conflict.originalScope=compile, conflict.originalOptionality=false}\\n    Node: com.example:test_dependency:1.0.0    {conflict.winner=com.example:test_dependency:jar:1.0.0 (test), conflict.originalScope=compile, conflict.originalOptionality=false}\\n  Node: com.example:test_dependency:1.0.0    {REDUCED_SCOPE=compile, conflict.originalScope=test, conflict.originalOptionality=false}\\n    Node: com.example:shared_dependency1:1.0.0    {conflict.originalScope=runtime, conflict.originalOptionality=false}\\n      Node: com.example:shared_dependency2:1.0.0    {conflict.originalScope=compile, conflict.originalOptionality=false}\\n```\\n\\nThe above information contains the original scope of the dependencies, the optionality and also a reference to the winning dependency in the case of conflict.\\n\\nSwitching over to using this tree would likely be intrusive, and not something I would like to do without building up the existing test cases within the project. This is definitely a task for another day \ud83d\ude01"},{"id":"/2022/12/05/gitsign-cache","metadata":{"permalink":"/blog/2022/12/05/gitsign-cache","editUrl":"https://github.com/trustification/trustification.github.io/tree/main/blog/2022-12-05-gitsign-cache.md","source":"@site/blog/2022-12-05-gitsign-cache.md","title":"How to Configure a Gitsign Cache","description":"This post contains the steps for setting up","date":"2022-12-05T00:00:00.000Z","formattedDate":"December 5, 2022","tags":[{"label":"sigstore","permalink":"/blog/tags/sigstore"}],"readingTime":1.15,"hasTruncateMarker":true,"authors":[{"name":"Daniel Bevenius","title":"Maintainer","url":"https://github.com/danbev","imageURL":"https://github.com/danbev.png","key":"danbev"}],"frontMatter":{"title":"How to Configure a Gitsign Cache","authors":"danbev","tags":["sigstore"]},"prevItem":{"title":"An Adventure with the CycloneDX Maven Plugin","permalink":"/blog/2022/12/09/cyclonedx-maven-plugin-adventure"},"nextItem":{"title":"Keyless Git Signing with Sigstore","permalink":"/blog/2022/12/02/sign-commits-with-sigstore"}},"content":"This post contains the steps for setting up\\n[gitsign-credential-cache](https://github.com/sigstore/gitsign/tree/main/cmd/gitsign-credential-cache).\\nwhich is useful if one has to perform multiple commits in short succession, or\\nwhen doing a rebase.\\n\\n\x3c!--truncate--\x3e\\n\\nIt can be somewhat frustrating to have the browser open for every single\\ncommit. For these situations a cache can be enabled using the instructions in\\nthis post.\\n\\nFirst install `gitsign-credential-cache` if it is not already installed:\\n\\n```console\\n$ go install github.com/sigstore/gitsign/cmd/gitsign-credential-cache@latest\\n```\\n\\nCreate a file named `~/.config/systemd/user/gitsign.service`:\\n\\n```console\\n[Unit]\\nDescription=Gitsign Credentials Cache\\nDocumentation=https://github.com/sigstore/gitsign\\n\\n[Service]\\nType=simple\\nExecStart=%h/go/bin/gitsign-credential-cache\\n\\nRestart=on-failure\\n\\n[Install]\\nWantedBy=default.target\\n```\\n\\nThis service can then be enabled using:\\n\\n```console\\n$ systemctl --user daemon-reload\\n$ systemctl --user enable gitsign.service\\nCreated symlink /home/danielbevenius/.config/systemd/user/default.target.wants/gitsign.service \u2192 /home/danielbevenius/.config/systemd/user/gitsign.service.\\n```\\n\\nAnd we can start it manually using:\\n\\n```console\\n$ systemctl --user start gitsign.service\\n```\\n\\nCheck that it has started successfully:\\n\\n```console\\n$ systemctl --user status gitsign.service\\n\u25cf gitsign.service - Gitsign Credentials Cache\\n     Loaded: loaded (/home/danielbevenius/.config/systemd/user/gitsign.service; enabled; vendor preset: disabled)\\n     Active: active (running) since Mon 2022-11-28 11:27:47 CET; 2min 35s ago\\n       Docs: https://github.com/sigstore/gitsign\\n   Main PID: 177444 (gitsign-credent)\\n     CGroup: /user.slice/user-1000.slice/user@1000.service/app.slice/gitsign.service\\n             \u2514\u2500 177444 /home/danielbevenius/go/bin/gitsign-credential-cache\\n\\nNov 28 11:27:47 localhost.localdomain systemd[1295]: Started Gitsign Credentials Cache.\\nNov 28 11:27:47 localhost.localdomain gitsign-credential-cache[177444]: /home/danielbevenius/.cache/.sigstore/gitsig>\\n```\\n\\nAnd we then need to add the following environment variable:\\n\\n```console\\n$ export GITSIGN_CREDENTIAL_CACHE=~/.cache/.sigstore/gitsign/cache.sock\\n```\\n\\nAfter this we should be able to commit a first time and have our credentials\\nstored. Subsequent commits will then be made without a browser \\"popup\\"."},{"id":"/2022/12/02/sign-commits-with-sigstore","metadata":{"permalink":"/blog/2022/12/02/sign-commits-with-sigstore","editUrl":"https://github.com/trustification/trustification.github.io/tree/main/blog/2022-12-02-sign-commits-with-sigstore.md","source":"@site/blog/2022-12-02-sign-commits-with-sigstore.md","title":"Keyless Git Signing with Sigstore","description":"This post contains the steps for setting up","date":"2022-12-02T00:00:00.000Z","formattedDate":"December 2, 2022","tags":[{"label":"sigstore","permalink":"/blog/tags/sigstore"}],"readingTime":1.715,"hasTruncateMarker":true,"authors":[{"name":"Daniel Bevenius","title":"Maintainer","url":"https://github.com/danbev","imageURL":"https://github.com/danbev.png","key":"danbev"}],"frontMatter":{"title":"Keyless Git Signing with Sigstore","authors":"danbev","tags":["sigstore"]},"prevItem":{"title":"How to Configure a Gitsign Cache","permalink":"/blog/2022/12/05/gitsign-cache"},"nextItem":{"title":"Welcome","permalink":"/blog/welcome"}},"content":"This post contains the steps for setting up\\n[gitsign](https://github.com/sigstore/gitsign) to sign your git\\ncommits using [sigstore](https://www.sigstore.dev/).\\n\\n\x3c!--truncate--\x3e\\n\\n### Install gitsign\\n\\n```console\\n$ go install github.com/sigstore/gitsign@latest\\n```\\n\\nOr using brew:\\n\\n```console\\n$ brew install sigstore/tap/gitsign\\n```\\n\\n### Configure git\\n\\nThe collowing will configure signing for the current project:\\n\\n```console\\n#!/bin/bash\\n\\n# Sign all commits\\ngit config --local commit.gpgsign true\\n\\n# Sign all tags\\ngit config --local tag.gpgsign true\\n\\n# Use gitsign for signing\\ngit config --local gpg.x509.program gitsign\\n\\n# gitsign expects x509 args\\ngit config --local gpg.format x509\\n```\\n\\nTo configure for all projects, use:\\n\\n```console\\n#!/bin/bash\\n\\n# Sign all commits\\ngit config --global commit.gpgsign true\\n\\n# Sign all tags\\ngit config --global tag.gpgsign true\\n\\n# Use gitsign for signing\\ngit config --global gpg.x509.program gitsign\\n\\n# gitsign expects x509 args\\ngit config --global gpg.format x509\\n```\\n\\n### Commit\\n\\nNow when you commit, `gitsign` will be used to start an Open ID\\nConnect (OIDC) flow. This will allow you to choose an OIDC provider:\\n\\n```console\\n$ git commit -v\\nYour browser will now be opened to:\\nhttps://oauth2.sigstore.dev/auth/auth?access_type=online&client_id=sigstore&code_challenge=eQvdw56pTgXnkj76Cab-4ZWaKk8XFM6UFFBdayKQX1Y&code_challenge_method=S256&nonce=2GmBDq86TMNuz8VhMUixMxiPSe2&redirect_uri=http%3A%2F%2Flocalhost%3A39617%2Fauth%2Fcallback&response_type=code&scope=openid+email&state=2GmBDlYDps5Ywd8dX4Ebwo4VnQL\\n[master 4292869] Add initial Oniro notes\\n 1 file changed, 10 insertions(+)\\n create mode 100644 notes/oniro.md\\n```\\n\\nNote that on github this commit will be marked as `Unverified` because\\nthe sigstore Certificate Authority root is not part of Github\'s trust\\nroot. Further, validation needs to be done using Rekor to verify that\\nthe certificate was valid at the time this commit was signed.\\n\\nTo avoid having to choose an auth provider each time, set the following environment variable. For example:\\n\\n```console\\n$ export GITSIGN_CONNECTOR_ID=https://github.com/login/oauth\\n```\\n\\n### Verifying a commit\\n\\n```console\\n$ git verify-commit HEAD\\n```\\n\\nIf verified, you\'ll see output similar to this:\\n\\n```console\\ntlog index: 6058402\\ngitsign: Signature made using certificate ID 0xb073e00bfabd4fb9988b9e1e0896dcfc1527fcdb | CN=sigstore-intermediate,O=sigstore.dev\\ngitsign: Good signature from [daniel.bevenius@gmail.com]\\nValidated Git signature: true\\nValidated Rekor entry: true\\n```\\n\\n### Inspect commit signature\\n\\n```console\\n$ git cat-file commit HEAD \\\\\\n  | sed -n \'/BEGIN/, /END/p\' \\\\\\n  | sed \'s/^ //g\' \\\\\\n  | sed \'s/gpgsig //g\' \\\\\\n  | sed \'s/SIGNED MESSAGE/PKCS7/g\' \\\\\\n  | openssl pkcs7 -print -print_certs -text\\n```"},{"id":"welcome","metadata":{"permalink":"/blog/welcome","editUrl":"https://github.com/trustification/trustification.github.io/tree/main/blog/2022-11-30-welcome/index.md","source":"@site/blog/2022-11-30-welcome/index.md","title":"Welcome","description":"Today, we\'re excited to announce the launch of Trustification, a new","date":"2022-11-30T00:00:00.000Z","formattedDate":"November 30, 2022","tags":[],"readingTime":0.105,"hasTruncateMarker":true,"authors":[{"name":"Jim Crossley","title":"Maintainer","url":"https://github.com/jcrossley3","imageURL":"https://github.com/jcrossley3.png","key":"jcrossley3"}],"frontMatter":{"slug":"welcome","title":"Welcome","authors":["jcrossley3"],"tags":[]},"prevItem":{"title":"Keyless Git Signing with Sigstore","permalink":"/blog/2022/12/02/sign-commits-with-sigstore"}},"content":"Today, we\'re excited to announce the launch of Trustification, a new\\ncommunity dedicated to improving software supply-chain security.\\n\\n\x3c!--truncate--\x3e\\n\\n![ChatGPT\'s welcome](./ai-intro.png)"}]}')}}]);